{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "355LmyCRd_sZ"
   },
   "source": [
    "# Assignment 1: Supervised Learning Tasks\n",
    "\n",
    "In the lecture, we briefly discussed supervised and unsupervised learning in the context of machine learning. In this assignment, we will go through binary classification tasks on one of the most popular datasets in the fairness in machine learning literature.\n",
    "\n",
    "We will:\n",
    "1. Import the data, implement a few pre-processing steps, and inspect the data\n",
    "2. Run a short exploratory analysis of the target variable\n",
    "3. Train a binary classifier and evaluate the classifier using the predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdeHvX_6woJg"
   },
   "source": [
    "# Packages and Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lI_-xUcTnhv3"
   },
   "source": [
    "You may need to install the following packages:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas numpy statsmodels matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmvEsKCOTIV-"
   },
   "source": [
    "Import the packages and modules:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "from IPython.display import Markdown, display, Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from IPython.core.display import HTML \n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJDFA9XSwdMi"
   },
   "source": [
    "### Step 1: Load Data\n",
    "\n",
    "For this assignment, we will work with the German Credit dataset which has been provided with this notebook (the dataset can also be downloaded from the UCI ML repository). The dataset has demographic and financial information for about 1,000 individuals, and the task for this dataset is to predict whether an individual is a good credit risk or a bad credit risk. More information about the dataset can be found here: https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data.\n",
    "\n",
    "Load the dataset and check the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['status', 'duration', 'credit_hist', 'purpose', 'credit_amt', 'savings', 'employment',\\\n",
    "            'install_rate', 'personal_status', 'debtors', 'residence', 'property', 'age', 'install_plans',\\\n",
    "            'housing', 'num_credits', 'job', 'num_liable', 'telephone', 'foreign_worker', 'credit']\n",
    "data_df = pd.read_table('german.data', names=cols, sep=\" \", index_col=False)\n",
    "y = data_df['credit']\n",
    "\n",
    "print(\"Shape: \", data_df.shape)\n",
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data preprocessing\n",
    "\n",
    "For our analysis, we will preprocess the dataset to get additional columns and bucketize some columns with numeric data. In particular, we will do the following:\n",
    "1. Create a new column \"sex\" that is derived from the column \"personal_status\". From the \"personal_status\" column, two values of sex can be derived: male and female (refer the data definition in the UCI ML repository to enumerate the column values of this column). Append the new column to the dataframe created above.\n",
    "2. Bucketize (or, discretize) the \"age\" column according to a chosen threshold (say, 45). For individuals with age greater than or equal to the threshold, replace the value with \"old\", and those with age less than the threshold will have the value \"young\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to create the \"sex\" column and append to the dataframe\n",
    "\n",
    "\n",
    "# print the first few rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to replace the \"age\" column with discrete values\n",
    "\n",
    "# print the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported the data and prepared it for analysis, we will inspect it for further analysis. Let's start by observing the distribution of the target variable and the columns we created just now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to plot the distribution of age, sex, and credit\n",
    "# in this code cell\n",
    "\n",
    "\n",
    "\n",
    "# You may also plot the distribution of all the columns to better understand the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write down your observations from the plots above**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the data distribution in terms of age and sex together: generate a crosstab summarizing the number of observations by age and sex:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to generate a crosstab summarizing the number of observations by\n",
    "# age and sex (i.e. a pivot table) in this code cell\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write down your observations from the results you obtained**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn our focus to the primary variable of interest: credit risk. In this exploratory analysis, we are interested in the variable named \"credit\".\n",
    "\n",
    "Plot the distribution of credit risk for males and for females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot credit risk by sex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **TODO: To what extent do these distributions differ? Summarize the difference between the distribution of credit risks for male and female applicants in this text cell (three sentences maximum):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about age? Repeat the above plots for young and old applicants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to plot credit risk for young and old applicants in this code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Summarize the difference between the distribution of credit risks for young and old applicants in this text cell (three sentences maximum):**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train a classifier to predict credit using the original data\n",
    "\n",
    "We will be training a logistic regression model to predict good/bad credit, then fine-tuning the model over a set of hyperparameters. For this purpose, we will use sklearn's logistic regression model. However, the model expects the data to be in numerical format. \n",
    "\n",
    "#### 4.1. Data preparation \n",
    "We have two options here:\n",
    "1. Use sklearn's encoders to transform the categorical values into numerical values.\n",
    "2. Write your own mapping functions (example for two of the columns has been provided below) with the best of your knowledge about the domain. Pay attention to columns with numerical data-- those could be bucketized or used as is. Similarly, column values in the categorical columns \"purpose\" and \"housing\" may not have a clear ordering-- in that case, you may consider creating dummy variables (be sure to drop the original columns).\n",
    "\n",
    "Using either of the two techniques, prepare the data for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code to preprocess the data\n",
    "\n",
    "# example mappings\n",
    "# data_df['status'] = data_df['status'].map({'A11': 0, 'A12': 1, 'A13': 2, 'A14': 3}).astype(int)\n",
    "# data_df['credit_hist'] = data_df['credit_hist'].map({'A34': 0, 'A33': 1, 'A32': 2, 'A31': 3, 'A30': 4}).astype(int)\n",
    "\n",
    "\n",
    "# print the first few rows of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Split the data into train/validation/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split original data into train and test data\n",
    "train, test = train_test_split(data_df, train_size=0.80, random_state=10)\n",
    "\n",
    "# Split training data in to training and validation data for hyperparameter tuning\n",
    "train, val = train_test_split(train, train_size=0.75, shuffle=True)\n",
    "\n",
    "# Use the shape function to output the size of each of the dataframes below\n",
    "print(\"Train set: \", )\n",
    "print(\"Val set: \", )\n",
    "print(\"Test set: \", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Split the data into training features and target variable\n",
    "\n",
    "First, we need to split our data up into the independent variables (x) and the outcome variable (y). We will recode the outcome so that the values are 0 (= bad credit) and 1 (= good credit). This is the format that the sklearn logistic regression function expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the training data\n",
    "x_train = train.drop(\"credit\", axis=1)\n",
    "y_train = train.credit.replace({2:0}) #1 = Good, 2= Bad credit risk\n",
    "print(\"Training Outcomes: \\n\", y_train.value_counts())\n",
    "\n",
    "# Similary, set up the test and validation data, and print their counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. Model training\n",
    "\n",
    "Now let's fit our model on the training data. Remember, we do not use test and validation sets at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the logistic regression model with the given hyperparameters\n",
    "initial_lr = LogisticRegression(C=0.5, penalty=\"l1\", solver='liblinear')\n",
    "    \n",
    "# Fit the model using the training data\n",
    "initial_lr = initial_lr.fit(x_train, y_train, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5. Model evaluation\n",
    "Now that we have a trained model, we should evaluate it on our validation set. For now, we will compute the area under the curve (AUC) as well as accuracy when we use a cutoff of 0.5 (that is, predicted values over 0.5 are interpreted as good credit, and less than 0.5 are interpreted as bad credit).\n",
    "\n",
    "**TODO: Write the function below to evaluate accuracy and AUC of a trained model.**\n",
    "You can use the `accuracy_score` and `roc_auc_score` sklearn functions to compute the accuracy and the AUC of the model, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y_true):\n",
    "    '''Calculates the AUC and accuracy for a trained logistic regression model'''\n",
    "    \n",
    "    # Calculate predicted values\n",
    "    y_pred = model.predict_proba(X)\n",
    "    \n",
    "    # This returns a tuple for each observation containing the probability of being in each class.\n",
    "    # We are doing binary classification and hence, will need to know P(Y_hat=1) the probability that the outcome = 1 (good credit)\n",
    "    y_pred = [row[1] for row in y_pred] # This pulls the predicted probability that y = 1 for each observation\n",
    "    \n",
    "    \n",
    "    # Calculate accuracy using the accuracy_score function\n",
    "    accuracy = \n",
    "    \n",
    "    # Calculate AUC using the roc_auc function\n",
    "    auc = \n",
    "    \n",
    "    return accuracy, auc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, auc = evaluate(initial_lr, x_val, y_val)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"AUC: \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.6. Hyperparameter tuning the logistic regression model \n",
    "\n",
    "Notice that for fitting the logistic regression model, we used the parameters C=0.5, penalty=\"l1\". There might be models (trained with different values of C and penalty) that yield a higher accuracy on the validataion data. We want to be able to easily train models with a variety of hyperparameters and determine which one performs the best on the validation data. We can use the evaluate function above to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_logistic_regression(x_train, y_train, x_val, y_val, penalty_types, C_values, weights=None, verbose=True):\n",
    "    '''Tunes logistic regression models over the hyperparameters penalty type and C\n",
    "       to maximize the AUC'''\n",
    "    \n",
    "    # Create empty lists where we will store the results of hyperparameter tuning \n",
    "    parameters = []\n",
    "    models = []\n",
    "    val_aucs = []\n",
    "    \n",
    "    # Loop through the hyperparameters of interest\n",
    "    for penalty in penalty_types:\n",
    "        for C in C_values:\n",
    "            \n",
    "            # Train the logistic regression model with the given hyperparameters\n",
    "            lr = LogisticRegression(C=C, penalty=penalty, solver='liblinear')\n",
    "    \n",
    "            # Fit the model using the training data\n",
    "            lr = lr.fit(x_train, y_train, sample_weight=weights)\n",
    "            \n",
    "            # Get the evalution metrics on the validation set \n",
    "            accuracy, auc  = evaluate(lr, x_val, y_val)\n",
    "            \n",
    "            # Store the results\n",
    "            parameters.append({'penalty': penalty, 'C': C})\n",
    "            models.append(lr)\n",
    "            val_aucs.append(auc)\n",
    "            \n",
    "            # Print the results\n",
    "            if verbose:\n",
    "                print(\"\\nParmeters: \\tpenalty={} \\tC={}\".format(penalty, C))\n",
    "                print(\"Validtion AUC: {}\".format(auc))\n",
    "            \n",
    "    \n",
    "    # Determine the best model -- that is, the one with the AUC\n",
    "    best_model_index = np.argmax(val_aucs)\n",
    "    best_model = models[best_model_index]\n",
    "    \n",
    "    print(\"\\nBest model parameters: \", parameters[best_model_index])\n",
    "    print(\"Best model AUC: \", val_aucs[best_model_index])\n",
    "    \n",
    "    # Return best model\n",
    "    return best_model, parameters, models, val_aucs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty_types=[\"l1\", \"l2\"]\n",
    "C_values=[0.001, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "best_lr, parameters, models, val_aucs = tune_logistic_regression(x_train, y_train, x_val, y_val, penalty_types, C_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the results so that we understand what hyperparameter tuning actually did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_aucs_l1 = [val_aucs[i] for i in range(len(val_aucs)) if parameters[i]['penalty']==\"l1\"]\n",
    "val_aucs_l2 = [val_aucs[i] for i in range(len(val_aucs)) if parameters[i]['penalty']==\"l2\"]\n",
    "C_values = [parameters[i]['C'] for i in range(len(parameters)) if parameters[i]['penalty']==\"l2\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx( C_values, val_aucs_l1, marker='.', markerfacecolor='blue', markersize=12, color='blue', linewidth=4, label='L1 Penalty')\n",
    "ax.semilogx( C_values, val_aucs_l2, marker='.', markerfacecolor='red', markersize=12, color='red', linewidth=4, label='L2 Penalty')\n",
    "ax.set_xlabel(\"C\")\n",
    "ax.set_ylabel(\"AUC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: From the plot above, interpret why the hyperparameters were chosen as such.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvGGSeCqY-eN"
   },
   "source": [
    "#### 4.7. Interpreting the predictive accuracy\n",
    "\n",
    "Until now, we used the predictive accuracy of the logistic regression model, which refers to the concordance between an individual's credit risk and the label assigned to that individual by the model. For instance, how often did the model predict that a person was a \"bad credit risk\" and that person in fact defaulted on a loan? \n",
    "\n",
    "We can think of this in terms of a 2x2 table (also called a *confusion matrix*):\n",
    "\n",
    "|      | Did not default | Defaulted   |\n",
    "| :---        |    :----:   |          ---: |\n",
    "| **Labeled bad credit risk**  | A       | B   |\n",
    "| **Labeled good credit risk**   | C       | D      |\n",
    "\n",
    "**TODO: What are the generic terms for A and D and what do they represent?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kojOZPLybCSv"
   },
   "source": [
    "**TODO: Based on this confusion matrix, input the number of true positives, false positives, true negatives, and false negatives:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3Av4Eu0ZjFN"
   },
   "outputs": [],
   "source": [
    "true_positive  = 0#@param {type:\"number\"}\n",
    "false_positive = 0#@param {type:\"number\"}\n",
    "true_negative  = 0#@param {type:\"number\"}\n",
    "false_negative = 0#@param {type:\"number\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOKUOXBh6rh0"
   },
   "source": [
    "Calculate the false positive rate:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VrkCFBzAZa2R"
   },
   "outputs": [],
   "source": [
    "# write code to calculate the false positive rate for all applicants\n",
    "# in this code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgmNak0ob2C8"
   },
   "source": [
    "Calculate the false *negative* rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46bwGfLba3ij"
   },
   "outputs": [],
   "source": [
    "# write code to calculate the false negative rate for all applicants\n",
    "# in this code cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6jHN6nWcP3X"
   },
   "source": [
    "**TODO: Take a moment to review the false positive rate and false negative rate above. For this application, comment on whether one metric is more desired over another?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8. Now, let's try another popular machine learning model, a Random Forest Classifier. You only need to change step 4.4 above, evaluate the trained random forest and compute the statistics as in step 4.7. You may also use hyperparameter tuning and compare the best random forest classifier with the best logistic regression classifier.\n",
    "\n",
    "**TODO: Compare your results (accuracy and AUC) for the two models and comment on the similarities and differences in their performances.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06n4G0ShMTWY"
   },
   "source": [
    "# Submitting this Assignment Notebook\n",
    "\n",
    "Once complete, please submit your assignment notebook as an attachment under \"Assignments > Assignment 1\" on Brightspace. You can download a copy of your notebook using ```File > Download .ipynb```. Please ensure you submit the `.ipynb` file (and not a `.py` file)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "355LmyCRd_sZ",
    "JI0slRqXFyXZ",
    "06n4G0ShMTWY"
   ],
   "name": "lab_1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
