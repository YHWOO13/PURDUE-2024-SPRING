{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoSkdhhuQs0l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b2906dc-dadc-4df5-9c94-eee14c374947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update && apt install cuda-11-8"
      ],
      "metadata": {
        "id": "I9RgeWwaqqny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#coding:utf-8\n",
        "import numpy as np\n",
        "import json\n",
        "# from model import ConceptFlow, use_cuda\n",
        "# from preprocession import prepare_data, build_vocab, gen_batched_data\n",
        "import torch\n",
        "import warnings\n",
        "import yaml\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.nn import utils as nn_utils\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "mWpgUunzQ8kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2iPZNray51T",
        "outputId": "3ac78cec-e7ad-4ae8-916f-1d6e47fc6d22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = True\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')"
      ],
      "metadata": {
        "id": "YcWJtgwsro77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxE8flivlnvN",
        "outputId": "30033239-b152-49c1-d9a9-939c41497f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/local/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DsePM-OmBzL",
        "outputId": "d8cafa37-621e-49eb-e910-d290c1efb9b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin    cuda\tcuda-12.2  games\t       include\tlib64\t   man\t share\n",
            "colab  cuda-12\tetc\t   _gcs_config_ops.so  lib\tlicensing  sbin  src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "9Js5c-8fd2Wa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "649df87c-56ba-4c41-cdc3-62c5db4b3fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Apr 12 09:32:10 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              43W / 400W |      5MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKyfyGmhfP0A",
        "outputId": "50f56665-015c-4369-d768-7b3ec771bb3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.cuda.get_device_name(0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsG289j0fRqj",
        "outputId": "721dece3-70d9-4479-f1ae-296421417475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fK7grJgiRAu",
        "outputId": "f3d1b7a0-e700-4515-958c-8715f896700e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding"
      ],
      "metadata": {
        "id": "-k6Y5kv2bVTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #coding:utf-8\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# from torch.autograd import Variable\n",
        "# import torch.nn as nn\n",
        "# from torch.nn import utils as nn_utils\n",
        "\n",
        "VERY_SMALL_NUMBER = 1e-10\n",
        "VERY_NEG_NUMBER = -100000000000\n",
        "\n",
        "def use_cuda(var):\n",
        "    if torch.cuda.is_available():\n",
        "        var = var.to(device=device)\n",
        "        return var\n",
        "    else:\n",
        "        return var\n",
        "'''\n",
        "def use_cuda(var):\n",
        "    if torch.cuda.is_available():\n",
        "        return var.cuda()\n",
        "    else:\n",
        "        return var\n",
        "'''\n",
        "\n",
        "\n",
        "class EntityEmbedding(nn.Module):\n",
        "  def __init__(self, entity_embed, trans_units):\n",
        "    super(EntityEmbedding, self).__init__()\n",
        "    self.trans_units = trans_units\n",
        "    self.entity_embedding = nn.Embedding(num_embeddings = entity_embed.shape[0] + 7, embedding_dim = self.trans_units, padding_idx = 0)\n",
        "    entity_embed = torch.tensor(entity_embed, device=device)\n",
        "\n",
        "    entity_embed = torch.cat((torch.zeros(7, self.trans_units, device=device), entity_embed), 0)\n",
        "    self.entity_embedding.weight = nn.Parameter(torch.tensor(entity_embed, device=device))\n",
        "    self.entity_embedding.weight.requires_grad = True\n",
        "    self.entity_linear = nn.Linear(in_features = self.trans_units, out_features = self.trans_units)\n",
        "\n",
        "  def forward(self, entity):\n",
        "    entity_emb = self.entity_embedding(entity)\n",
        "    entity_emb = self.entity_linear(entity_emb)\n",
        "    return entity_emb\n",
        "\n",
        "\n",
        "\n",
        "class WordEmbedding(nn.Module):\n",
        "    def __init__(self, word_embed, embed_units):\n",
        "      super(WordEmbedding, self).__init__()\n",
        "\n",
        "      self.embed_units = embed_units\n",
        "      self.word_embedding = nn.Embedding(num_embeddings = word_embed.shape[0], embedding_dim = self.embed_units, padding_idx = 0)\n",
        "      self.word_embedding.weight = nn.Parameter(torch.tensor(word_embed, device=device))\n",
        "      self.word_embedding.weight.requires_grad = True\n",
        "\n",
        "    def forward(self, query_text):\n",
        "      return self.word_embedding(query_text)"
      ],
      "metadata": {
        "id": "Fnhv6EKXbXuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utils"
      ],
      "metadata": {
        "id": "2Bycxa2Vfvec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def padding(sent, l):\n",
        "    return sent + ['_EOS'] + ['_PAD'] * (l-len(sent)-1)\n",
        "\n",
        "def padding_triple_id(entity2id, triple, num, l):\n",
        "    newtriple = []\n",
        "    for i in range(len(triple)):\n",
        "        for j in range(len(triple[i])):\n",
        "            for k in range(len(triple[i][j])):\n",
        "                if triple[i][j][k] in entity2id:\n",
        "                    triple[i][j][k] = entity2id[triple[i][j][k]]\n",
        "                else:\n",
        "                    triple[i][j][k] = entity2id['_NONE']\n",
        "\n",
        "    for tri in triple:\n",
        "        newtriple.append(tri + [[entity2id['_PAD_H'], entity2id['_PAD_R'], entity2id['_PAD_T']]] * (l - len(tri)))\n",
        "    pad_triple = [[entity2id['_PAD_H'], entity2id['_PAD_R'], entity2id['_PAD_T']]] * l\n",
        "    return newtriple + [pad_triple] * (num - len(newtriple))\n",
        "\n",
        "def build_kb_adj_mat(kb_adj_mats, fact_dropout):\n",
        "    \"\"\"Create sparse matrix representation for batched data\"\"\"\n",
        "    mats0_batch = np.array([], dtype=int)\n",
        "    mats0_0 = np.array([], dtype=int)\n",
        "    mats0_1 = np.array([], dtype=int)\n",
        "    vals0 = np.array([], dtype=float)\n",
        "\n",
        "    mats1_batch = np.array([], dtype=int)\n",
        "    mats1_0 = np.array([], dtype=int)\n",
        "    mats1_1 = np.array([], dtype=int)\n",
        "    vals1 = np.array([], dtype=float)\n",
        "\n",
        "    for i in range(kb_adj_mats.shape[0]):\n",
        "        (mat0_0, mat0_1, val0), (mat1_0, mat1_1, val1) = kb_adj_mats[i]\n",
        "        assert len(val0) == len(val1)\n",
        "        num_fact = len(val0)\n",
        "        num_keep_fact = int(np.floor(num_fact * (1 - fact_dropout)))\n",
        "        mask_index = np.random.permutation(num_fact)[ : num_keep_fact]\n",
        "        # mat0\n",
        "        mats0_batch = np.append(mats0_batch, np.full(len(mask_index), i, dtype=int))\n",
        "        mats0_0 = np.append(mats0_0, mat0_0[mask_index])\n",
        "        mats0_1 = np.append(mats0_1, mat0_1[mask_index])\n",
        "        vals0 = np.append(vals0, val0[mask_index])\n",
        "        # mat1\n",
        "        mats1_batch = np.append(mats1_batch, np.full(len(mask_index), i, dtype=int))\n",
        "        mats1_0 = np.append(mats1_0, mat1_0[mask_index])\n",
        "        mats1_1 = np.append(mats1_1, mat1_1[mask_index])\n",
        "        vals1 = np.append(vals1, val1[mask_index])\n",
        "\n",
        "    return (mats0_batch, mats0_0, mats0_1, vals0), (mats1_batch, mats1_0, mats1_1, vals1)"
      ],
      "metadata": {
        "id": "bvppaBmpfw8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# central"
      ],
      "metadata": {
        "id": "iJEEMVK4bkrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CentralEncoder(nn.Module):\n",
        "  def __init__(self, config, gnn_layers, embed_units, trans_units, word_embedding, entity_embedding):\n",
        "    super(CentralEncoder, self).__init__()\n",
        "    self.k = 2 + 1\n",
        "    self.gnn_layers = gnn_layers # 3\n",
        "    self.WordEmbedding = word_embedding\n",
        "    self.EntityEmbedding = entity_embedding\n",
        "    self.embed_units = embed_units # 300\n",
        "    self.trans_units = trans_units # 100\n",
        "    self.pagerank_lambda = config.pagerank_lambda # 0.8\n",
        "    self.fact_scale = config.fact_scale # 1\n",
        "\n",
        "    self.node_encoder = nn.LSTM(input_size = self.embed_units, hidden_size = self.trans_units, batch_first=True, bidirectional=False)\n",
        "    self.lstm_drop = nn.Dropout(p = config.lstm_dropout)\n",
        "    self.softmax_d1 = nn.Softmax(dim = 1)\n",
        "    self.linear_drop = nn.Dropout(p = config.linear_dropout)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    for i in range(self.gnn_layers):\n",
        "        self.add_module('q2e_linear' + str(i), nn.Linear(in_features=self.trans_units, out_features=self.trans_units))\n",
        "        self.add_module('d2e_linear' + str(i), nn.Linear(in_features=self.trans_units, out_features=self.trans_units))\n",
        "        self.add_module('e2q_linear' + str(i), nn.Linear(in_features=self.k * self.trans_units, out_features=self.trans_units))\n",
        "        self.add_module('e2d_linear' + str(i), nn.Linear(in_features=self.k * self.trans_units, out_features=self.trans_units))\n",
        "        self.add_module('e2e_linear' + str(i), nn.Linear(in_features=self.k * self.trans_units, out_features=self.trans_units))\n",
        "\n",
        "        #use kb\n",
        "        self.add_module('kb_head_linear' + str(i), nn.Linear(in_features=self.trans_units, out_features=self.trans_units))\n",
        "        self.add_module('kb_tail_linear' + str(i), nn.Linear(in_features=self.trans_units, out_features=self.trans_units))\n",
        "        self.add_module('kb_self_linear' + str(i), nn.Linear(in_features=self.trans_units, out_features=self.trans_units))\n",
        "\n",
        "  def forward(self, batch_size, max_local_entity, max_fact, query_text, local_entity, q2e_adj_mat, kb_adj_mat, kb_fact_rel, query_mask):\n",
        "    # normalized adj matrix\n",
        "    pagerank_f = Variable(torch.from_numpy(q2e_adj_mat).type('torch.FloatTensor'), requires_grad=True).to(device) # use_cuda(Variable(torch.from_numpy(q2e_adj_mat).type('torch.FloatTensor'), requires_grad=True))\n",
        "    q2e_adj_mat = Variable(torch.from_numpy(q2e_adj_mat).type('torch.FloatTensor'), requires_grad=False).to(device) # use_cuda\n",
        "    assert pagerank_f.requires_grad == True\n",
        "\n",
        "    # encode query\n",
        "    query_word_emb = self.WordEmbedding(query_text)\n",
        "    query_hidden_emb, (query_node_emb, _) = self.node_encoder(self.lstm_drop(query_word_emb), self.init_hidden(1, batch_size, self.trans_units))\n",
        "    query_node_emb = query_node_emb.squeeze(dim=0).unsqueeze(dim=1)\n",
        "    query_rel_emb = query_node_emb\n",
        "\n",
        "    # build kb_adj_matrix from sparse matrix\n",
        "    (e2f_batch, e2f_f, e2f_e, e2f_val), (f2e_batch, f2e_e, f2e_f, f2e_val) = kb_adj_mat\n",
        "    entity2fact_index = torch.LongTensor([e2f_batch, e2f_f, e2f_e]).to(device)\n",
        "    entity2fact_val = torch.FloatTensor(e2f_val).to(device)\n",
        "    entity2fact_mat = use_cuda(torch.sparse.FloatTensor(entity2fact_index, entity2fact_val, torch.Size([batch_size, max_fact, max_local_entity])))#       entity2fact_mat = use_cuda(torch.sparse.FloatTensor(entity2fact_index, entity2fact_val, torch.Size([batch_size, max_fact, max_local_entity])))\n",
        "\n",
        "    fact2entity_index = torch.LongTensor([f2e_batch, f2e_e, f2e_f])\n",
        "    fact2entity_val = torch.FloatTensor(f2e_val)\n",
        "    fact2entity_mat = use_cuda(torch.sparse.FloatTensor(fact2entity_index, fact2entity_val, torch.Size([batch_size, max_local_entity, max_fact])))\n",
        "\n",
        "    local_fact_emb = self.EntityEmbedding(kb_fact_rel)\n",
        "    local_fact_emb= local_fact_emb.to(device=device)\n",
        "    # attention fact2question\n",
        "    div = float(np.sqrt(self.trans_units))\n",
        "    fact2query_sim = torch.bmm(query_hidden_emb, local_fact_emb.transpose(1, 2)) / div\n",
        "    fact2query_sim = self.softmax_d1(fact2query_sim + (1 - query_mask.unsqueeze(dim=2)) * VERY_NEG_NUMBER)\n",
        "\n",
        "    fact2query_att = torch.sum(fact2query_sim.unsqueeze(dim=3) * query_hidden_emb.unsqueeze(dim=2), dim=1)\n",
        "\n",
        "    W = torch.sum(fact2query_att * local_fact_emb, dim=2) / div\n",
        "    W_max = torch.max(W, dim=1, keepdim=True)[0]\n",
        "    W_tilde = torch.exp(W - W_max)\n",
        "    e2f_softmax = self.sparse_bmm(entity2fact_mat.transpose(1, 2), W_tilde.unsqueeze(dim=2)).squeeze(dim=2)\n",
        "    e2f_softmax = torch.clamp(e2f_softmax, min=VERY_SMALL_NUMBER)\n",
        "    e2f_out_dim = use_cuda(Variable(torch.sum(entity2fact_mat.to_dense(), dim=1), requires_grad=False))\n",
        "\n",
        "    # load entity embedding\n",
        "    local_entity_emb = self.EntityEmbedding(local_entity)\n",
        "\n",
        "    # label propagation on entities\n",
        "    for i in range(self.gnn_layers):\n",
        "        # get linear transformation functions for each layer\n",
        "        q2e_linear = getattr(self, 'q2e_linear' + str(i))\n",
        "        d2e_linear = getattr(self, 'd2e_linear' + str(i))\n",
        "        e2q_linear = getattr(self, 'e2q_linear' + str(i))\n",
        "        e2d_linear = getattr(self, 'e2d_linear' + str(i))\n",
        "        e2e_linear = getattr(self, 'e2e_linear' + str(i))\n",
        "\n",
        "        kb_self_linear = getattr(self, 'kb_self_linear' + str(i))\n",
        "        kb_head_linear = getattr(self, 'kb_head_linear' + str(i))\n",
        "        kb_tail_linear = getattr(self, 'kb_tail_linear' + str(i))\n",
        "\n",
        "        # start propagation\n",
        "        next_local_entity_emb = local_entity_emb\n",
        "\n",
        "        # STEP 1: propagate from question, documents, and facts to entities\n",
        "        # question -> entity\n",
        "        q2e_emb = q2e_linear(self.linear_drop(query_node_emb)).expand(batch_size, max_local_entity, self.trans_units)\n",
        "        next_local_entity_emb = torch.cat((next_local_entity_emb, q2e_emb), dim=2)\n",
        "\n",
        "        # fact -> entity\n",
        "        e2f_emb = self.relu(kb_self_linear(local_fact_emb) + self.sparse_bmm(entity2fact_mat, kb_head_linear(self.linear_drop(local_entity_emb))))\n",
        "        e2f_softmax_normalized = W_tilde.unsqueeze(dim=2) * self.sparse_bmm(entity2fact_mat, (pagerank_f / e2f_softmax).unsqueeze(dim=2))\n",
        "        e2f_emb = e2f_emb * e2f_softmax_normalized\n",
        "        f2e_emb = self.relu(kb_self_linear(local_entity_emb) + self.sparse_bmm(fact2entity_mat, kb_tail_linear(self.linear_drop(e2f_emb))))\n",
        "\n",
        "        pagerank_f = self.pagerank_lambda * self.sparse_bmm(fact2entity_mat, e2f_softmax_normalized).squeeze(dim=2) + (1 - self.pagerank_lambda) * pagerank_f\n",
        "\n",
        "        # STEP 2: combine embeddings from fact\n",
        "        next_local_entity_emb = torch.cat((next_local_entity_emb, self.fact_scale * f2e_emb), dim=2)\n",
        "\n",
        "        # STEP 3: propagate from entities to update question, documents, and facts\n",
        "        # entity -> query\n",
        "        query_node_emb = torch.bmm(pagerank_f.unsqueeze(dim=1), e2q_linear(self.linear_drop(next_local_entity_emb)))\n",
        "        # update entity\n",
        "        local_entity_emb = self.relu(e2e_linear(self.linear_drop(next_local_entity_emb)))\n",
        "\n",
        "    return local_entity_emb\n",
        "\n",
        "  def init_hidden(self, num_layer, batch_size, hidden_size):\n",
        "    return (use_cuda(Variable(torch.zeros(num_layer, batch_size, hidden_size))),\n",
        "              use_cuda(Variable(torch.zeros(num_layer, batch_size, hidden_size))))\n",
        "\n",
        "  def sparse_bmm(self, X, Y):\n",
        "    \"\"\"Batch multiply X and Y where X is sparse, Y is dense.\n",
        "    Args:\n",
        "        X: Sparse tensor of size BxMxN. Consists of two tensors,\n",
        "            I:3xZ indices, and V:1xZ values.\n",
        "        Y: Dense tensor of size BxNxK.\n",
        "    Returns:\n",
        "        batched-matmul(X, Y): BxMxK\n",
        "    \"\"\"\n",
        "\n",
        "    class LeftMMFixed(torch.autograd.Function):\n",
        "        \"\"\"\n",
        "        Implementation of matrix multiplication of a Sparse Variable with a Dense Variable, returning a Dense one.\n",
        "        This is added because there's no autograd for sparse yet. No gradient computed on the sparse weights.\n",
        "        \"\"\"\n",
        "\n",
        "        @staticmethod\n",
        "        def forward(ctx, sparse_weights, x):\n",
        "            ctx.save_for_backward(sparse_weights)\n",
        "            return torch.mm(sparse_weights, x)\n",
        "\n",
        "        @staticmethod\n",
        "        def backward(ctx, grad_output):\n",
        "            sparse_weights, = ctx.saved_tensors\n",
        "            return None, torch.mm(sparse_weights.t(), grad_output)\n",
        "\n",
        "    def sparse_mm_fixed(sparse_weights, x):\n",
        "        return LeftMMFixed.apply(sparse_weights, x)\n",
        "\n",
        "    I = X._indices()\n",
        "    V = X._values()\n",
        "    B, M, N = X.size()\n",
        "    _, _, K = Y.size()\n",
        "    Z = I.size()[1]\n",
        "    lookup = Y[I[0, :], I[2, :], :]\n",
        "    X_I = torch.stack((I[0, :] * M + I[1, :], torch.arange(Z, device=X.device, dtype=torch.long)), 0)\n",
        "    S = torch.sparse.FloatTensor(X_I, V, torch.Size([B * M, Z])).to(device=X.device)\n",
        "    prod = sparse_mm_fixed(S, lookup)\n",
        "    return prod.view(B, M, K)"
      ],
      "metadata": {
        "id": "ebW2DxzhascZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Outer"
      ],
      "metadata": {
        "id": "D5Ofae6JbziO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OuterEncoder(nn.Module):\n",
        "  def __init__(self, trans_units, entity_embedding):\n",
        "    super(OuterEncoder, self).__init__()\n",
        "    self.EntityEmbedding = entity_embedding\n",
        "    self.trans_units = trans_units\n",
        "\n",
        "    self.head_tail_linear = nn.Linear(in_features = self.trans_units * 2, out_features = self.trans_units)\n",
        "    self.one_two_entity_linear = nn.Linear(in_features = self.trans_units, out_features = self.trans_units)\n",
        "    self.softmax_d2 = nn.Softmax(dim = 2)\n",
        "\n",
        "  def forward(self, batch_size, one_two_triples_id, one_two_triple_num):\n",
        "    one_two_triples_embedding = self.EntityEmbedding(one_two_triples_id).reshape([batch_size, one_two_triple_num, -1, 3 * self.trans_units])\n",
        "\n",
        "    head, relation, tail = torch.split(one_two_triples_embedding, [self.trans_units] * 3, 3)\n",
        "    head_tail = torch.cat((head, tail), 3)\n",
        "    head_tail_transformed = torch.tanh(self.head_tail_linear(head_tail))\n",
        "\n",
        "    relation_transformed = self.one_two_entity_linear(relation)\n",
        "\n",
        "    e_weight = torch.sum(relation_transformed * head_tail_transformed, 3)\n",
        "    alpha_weight = self.softmax_d2(e_weight)\n",
        "\n",
        "    one_two_embed = torch.sum(alpha_weight.unsqueeze(3) * head_tail, 2)\n",
        "\n",
        "    return one_two_embed"
      ],
      "metadata": {
        "id": "0BPZAHISb2Li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conceptflow"
      ],
      "metadata": {
        "id": "B75221lSatFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConceptFlow(nn.Module):\n",
        "\n",
        "  def __init__(self, config, word_embed, entity_embed, is_select=False):\n",
        "    super(ConceptFlow, self).__init__()\n",
        "    self.is_select = is_select\n",
        "    self.is_inference = False\n",
        "\n",
        "    self.trans_units = config.trans_units\n",
        "    self.embed_units = config.embed_units\n",
        "    self.units = config.units\n",
        "    self.layers = config.layers\n",
        "    self.gnn_layers = config.gnn_layers\n",
        "    self.symbols = config.symbols\n",
        "\n",
        "    self.WordEmbedding = WordEmbedding(word_embed, self.embed_units)\n",
        "    self.EntityEmbedding = EntityEmbedding(entity_embed, self.trans_units)\n",
        "    self.CentralEncoder = CentralEncoder(config, self.gnn_layers, self.embed_units, self.trans_units, self.WordEmbedding, self.EntityEmbedding)\n",
        "    self.OuterEncoder = OuterEncoder(self.trans_units, self.EntityEmbedding)\n",
        "\n",
        "    self.softmax_d1 = nn.Softmax(dim = 1)\n",
        "    self.softmax_d2 = nn.Softmax(dim = 2)\n",
        "\n",
        "    self.text_encoder = nn.GRU(input_size = self.embed_units, hidden_size = self.units, num_layers = self.layers, batch_first = True)\n",
        "    self.decoder = nn.GRU(input_size = self.units + self.embed_units, hidden_size = self.units, num_layers = self.layers, batch_first = True)\n",
        "\n",
        "    self.attn_c_linear = nn.Linear(in_features = self.units, out_features = self.units, bias = False)\n",
        "    self.attn_ce_linear = nn.Linear(in_features = self.trans_units, out_features = 2 * self.units, bias = False)\n",
        "    self.attn_co_linear = nn.Linear(in_features = 2 * self.trans_units, out_features = 2 * self.units, bias = False)\n",
        "    self.attn_ct_linear = nn.Linear(in_features = self.trans_units, out_features = 2 * self.units, bias = False)\n",
        "\n",
        "    self.context_linear = nn.Linear(in_features = 4 * self.units, out_features = self.units, bias = False)\n",
        "\n",
        "\n",
        "    self.logits_linear = nn.Linear(in_features = self.units, out_features = self.symbols)\n",
        "    self.selector_linear = nn.Linear(in_features = self.units, out_features = 3)\n",
        "\n",
        "  def forward(self, batch_data):\n",
        "    query_text = batch_data['query_text']\n",
        "    answer_text = batch_data['answer_text']\n",
        "    local_entity = batch_data['local_entity']\n",
        "    responses_length = batch_data['responses_length']\n",
        "    q2e_adj_mat = batch_data['q2e_adj_mat']\n",
        "    kb_adj_mat = batch_data['kb_adj_mat']\n",
        "    kb_fact_rel = batch_data['kb_fact_rel']\n",
        "    match_entity_one_hop = batch_data['match_entity_one_hop']\n",
        "    only_two_entity = batch_data['only_two_entity']\n",
        "    match_entity_only_two = batch_data['match_entity_only_two']\n",
        "    one_two_triples_id = batch_data['one_two_triples_id']\n",
        "    local_entity_length = batch_data['local_entity_length']\n",
        "    only_two_entity_length = batch_data['only_two_entity_length']\n",
        "\n",
        "    if self.is_inference == True:\n",
        "        word2id = batch_data['word2id']\n",
        "        entity2id = batch_data['entity2id']\n",
        "        id2entity = dict()\n",
        "        for key in entity2id.keys():\n",
        "            id2entity[entity2id[key]] = key\n",
        "    else:\n",
        "        id2entity = None\n",
        "\n",
        "    batch_size, max_local_entity = local_entity.shape\n",
        "    _, max_only_two_entity = only_two_entity.shape\n",
        "    _, one_two_triple_num, one_two_triple_len, _ = one_two_triples_id.shape\n",
        "    _, max_fact = kb_fact_rel.shape\n",
        "\n",
        "    # numpy to tensor\n",
        "    local_entity = use_cuda(Variable(torch.from_numpy(local_entity).type('torch.LongTensor'), requires_grad=False))\n",
        "    local_entity_mask = use_cuda((local_entity != 0).type('torch.FloatTensor'))\n",
        "    kb_fact_rel = use_cuda(Variable(torch.from_numpy(kb_fact_rel).type('torch.LongTensor'), requires_grad=False))\n",
        "    query_text = use_cuda(Variable(torch.from_numpy(query_text).type('torch.LongTensor'), requires_grad=False))\n",
        "    answer_text = use_cuda(Variable(torch.from_numpy(answer_text).type('torch.LongTensor'), requires_grad=False))\n",
        "    responses_length = use_cuda(Variable(torch.Tensor(responses_length).type('torch.LongTensor'), requires_grad=False))\n",
        "    query_mask = use_cuda((query_text != 0).type('torch.FloatTensor'))\n",
        "    match_entity_one_hop = use_cuda(Variable(torch.from_numpy(match_entity_one_hop).type('torch.LongTensor'), requires_grad=False))\n",
        "    only_two_entity = use_cuda(Variable(torch.from_numpy(only_two_entity).type('torch.LongTensor'), requires_grad=False))\n",
        "    match_entity_only_two = use_cuda(Variable(torch.from_numpy(match_entity_only_two).type('torch.LongTensor'), requires_grad=False))\n",
        "    one_two_triples_id = use_cuda(Variable(torch.from_numpy(one_two_triples_id).type('torch.LongTensor'), requires_grad=False))\n",
        "\n",
        "\n",
        "    decoder_len = answer_text.shape[1]\n",
        "    encoder_len = query_text.shape[1]\n",
        "    responses_target = answer_text\n",
        "    responses_id = torch.cat((use_cuda(torch.ones([batch_size, 1], device=device).type('torch.LongTensor')),torch.split(answer_text, [decoder_len - 1, 1], 1)[0]), 1)\n",
        "\n",
        "    # ★☆★☆★☆★☆★☆★☆★☆★☆★☆★☆★☆★☆★☆★☆\n",
        "    # encode central graph\n",
        "    # print('batch_size', batch_size)\n",
        "    # print('max_local_entity', max_local_entity)\n",
        "    # print('max_fact',max_fact)\n",
        "    # print('query_text',query_text)\n",
        "    # print('local_entity',local_entity)\n",
        "    # print('q2e_adj_mat',q2e_adj_mat)\n",
        "    # print('kb_adj_mat', kb_adj_mat)\n",
        "    # print('kb_fact_rel',kb_fact_rel)\n",
        "    # print('query_mask', query_mask)\n",
        "\n",
        "    local_entity_emb = self.CentralEncoder(batch_size, max_local_entity, max_fact, query_text, local_entity, q2e_adj_mat, kb_adj_mat, kb_fact_rel, query_mask)\n",
        "    local_entity_emb = local_entity_emb.to(device=device)\n",
        "    # print('local_entity_emb',local_entity_emb)\n",
        "\n",
        "    # encode text\n",
        "    text_encoder_input = self.WordEmbedding(query_text)\n",
        "    text_encoder_output, text_encoder_state = self.text_encoder(text_encoder_input, use_cuda(Variable(torch.zeros(self.layers, batch_size, self.units))))\n",
        "\n",
        "    # encode outer graph\n",
        "    one_two_embed = self.OuterEncoder(batch_size, one_two_triples_id, one_two_triple_num)\n",
        "    one_two_embed = one_two_embed.to(device=device)\n",
        "    # prepare decoder input for training\n",
        "    decoder_input = self.WordEmbedding(responses_id)\n",
        "\n",
        "    # attention key and values\n",
        "    c_attention_keys = self.attn_c_linear(text_encoder_output)\n",
        "    c_attention_values = text_encoder_output\n",
        "    ce_attention_keys, ce_attention_values = torch.split(self.attn_ce_linear(local_entity_emb), [self.units, self.units], 2)\n",
        "    co_attention_keys, co_attention_values = torch.split(self.attn_co_linear(one_two_embed), [self.units, self.units], 2)\n",
        "    only_two_entity_embed = self.EntityEmbedding(only_two_entity)\n",
        "    ct_attention_keys, ct_attention_values = torch.split(self.attn_ct_linear(only_two_entity_embed), [self.units, self.units], 2)\n",
        "\n",
        "\n",
        "    decoder_state = text_encoder_state\n",
        "    decoder_output = use_cuda(torch.empty(0))\n",
        "    ce_alignments = use_cuda(torch.empty(0))\n",
        "    co_alignments = use_cuda(torch.empty(0))\n",
        "    ct_alignments = use_cuda(torch.empty(0))\n",
        "\n",
        "    # central entity mask\n",
        "    local_entity_mask = np.zeros([batch_size, local_entity.shape[1]])\n",
        "    for i in range(batch_size):\n",
        "        local_entity_mask[i][0:local_entity_length[i]] = 1\n",
        "    local_entity_mask = use_cuda(torch.from_numpy(local_entity_mask).type('torch.LongTensor'))\n",
        "\n",
        "    # two-hop entity mask\n",
        "    only_two_entity_mask = np.zeros([batch_size, only_two_entity.shape[1]])\n",
        "    for i in range(batch_size):\n",
        "        only_two_entity_mask[i][0:only_two_entity_length[i]] = 1\n",
        "    only_two_entity_mask = use_cuda(torch.from_numpy(only_two_entity_mask).type('torch.LongTensor'))\n",
        "\n",
        "    context = use_cuda(torch.zeros([batch_size, self.units]))\n",
        "\n",
        "    if not self.is_inference:\n",
        "        for t in range(decoder_len):\n",
        "            decoder_input_t = torch.cat((decoder_input[:,t,:], context), 1).unsqueeze(1)\n",
        "\n",
        "            decoder_output_t, decoder_state = self.decoder(decoder_input_t, decoder_state)\n",
        "            context, ce_alignments_t, co_alignments_t, ct_alignments_t = self.attention(c_attention_keys, c_attention_values, \\\n",
        "                ce_attention_keys, ce_attention_values, co_attention_keys, co_attention_values, ct_attention_keys, \\\n",
        "                decoder_output_t.squeeze(1), local_entity_mask, only_two_entity_mask)\n",
        "            decoder_output_t = context.unsqueeze(1)\n",
        "            ce_alignments = torch.cat((ce_alignments, ce_alignments_t.unsqueeze(1)), 1)\n",
        "\n",
        "            co_alignments = torch.cat((co_alignments, co_alignments_t.unsqueeze(1)), 1)\n",
        "            decoder_output = torch.cat((decoder_output, decoder_output_t), 1)\n",
        "            ct_alignments = torch.cat((ct_alignments, ct_alignments_t.unsqueeze(1)), 1)\n",
        "\n",
        "    else:\n",
        "        word_index = use_cuda(torch.empty(0).type('torch.LongTensor'))\n",
        "        decoder_input_t = self.WordEmbedding(use_cuda(torch.ones([batch_size]).type('torch.LongTensor')))\n",
        "        context = use_cuda(torch.zeros([batch_size, self.units]))\n",
        "        decoder_state = text_encoder_state\n",
        "        selector = use_cuda(torch.empty(0).type('torch.LongTensor'))\n",
        "\n",
        "        for t in range(decoder_len):\n",
        "            decoder_input_t = torch.cat((decoder_input_t, context), 1).unsqueeze(1)\n",
        "            decoder_output_t, decoder_state = self.decoder(decoder_input_t, decoder_state)\n",
        "            context, ce_alignments_t, co_alignments_t, ct_alignments_t = self.attention(c_attention_keys, c_attention_values, \\\n",
        "                ce_attention_keys, ce_attention_values, co_attention_keys, co_attention_values, ct_attention_keys, \\\n",
        "                decoder_output_t.squeeze(1), local_entity_mask, only_two_entity_mask)\n",
        "            ct_alignments = torch.cat((ct_alignments, ct_alignments_t.unsqueeze(1)), 1)\n",
        "            decoder_output_t = context.unsqueeze(1)\n",
        "\n",
        "            decoder_input_t, word_index_t, selector_t = self.inference(decoder_output_t, ce_alignments_t, ct_alignments_t, word2id, \\\n",
        "                local_entity, only_two_entity, id2entity)\n",
        "            word_index = torch.cat((word_index, word_index_t.unsqueeze(1)), 1)\n",
        "            selector = torch.cat((selector, selector_t.unsqueeze(1)), 1)\n",
        "\n",
        "    decoder_mask = np.zeros([batch_size, decoder_len])\n",
        "    for i in range(batch_size):\n",
        "        decoder_mask[i][0:responses_length[i]] = 1\n",
        "    decoder_mask = use_cuda(torch.from_numpy(decoder_mask).type('torch.LongTensor'))\n",
        "\n",
        "    one_hot_entities_local = use_cuda(torch.zeros(batch_size, decoder_len, max_local_entity))\n",
        "    for b in range(batch_size):\n",
        "        for d in range(decoder_len):\n",
        "            if match_entity_one_hop[b][d] == -1:\n",
        "                continue\n",
        "            else:\n",
        "                one_hot_entities_local[b][d][match_entity_one_hop[b][d]] = 1\n",
        "\n",
        "    use_entities_local = torch.sum(one_hot_entities_local, [2])\n",
        "\n",
        "    one_hot_entities_only_two = use_cuda(torch.zeros(batch_size, decoder_len, max_only_two_entity))\n",
        "    for b in range(batch_size):\n",
        "        for d in range(decoder_len):\n",
        "            if match_entity_only_two[b][d] == -1:\n",
        "                continue\n",
        "            else:\n",
        "                one_hot_entities_only_two[b][d][match_entity_only_two[b][d]] = 1\n",
        "\n",
        "    use_entities_only_two = torch.sum(one_hot_entities_only_two, [2])\n",
        "\n",
        "    if not self.is_inference:\n",
        "        decoder_loss, ppx_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, \\\n",
        "            word_neg_num, local_neg_num, only_two_neg_num = self.total_loss(decoder_output, responses_target, decoder_mask, \\\n",
        "            ce_alignments, ct_alignments, use_entities_local, one_hot_entities_local, use_entities_only_two, one_hot_entities_only_two)\n",
        "\n",
        "    if self.is_select:\n",
        "        self.sort(id2entity, ct_alignments, only_two_entity)\n",
        "\n",
        "    if self.is_inference == True:\n",
        "        return word_index.cpu().numpy().tolist(), selector.cpu().numpy().tolist()\n",
        "    return decoder_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, word_neg_num, local_neg_num, only_two_neg_num\n",
        "\n",
        "  def sort(self, id2entity, ct_alignments, only_two_entity):\n",
        "    only_two_score = torch.sum(ct_alignments, 1)\n",
        "    _, sort_local_index = only_two_score.sort(1)\n",
        "    sort_global_index = torch.gather(only_two_entity, 1, sort_local_index)\n",
        "    sort_global_index = sort_global_index.cpu().numpy().tolist()\n",
        "\n",
        "    sort_str = []\n",
        "    for i in range(len(sort_global_index)):\n",
        "      tmp = []\n",
        "      for j in range(len(sort_global_index[i])):\n",
        "        if sort_global_index[i][j] == 1:\n",
        "          continue\n",
        "        tmp.append(id2entity[sort_global_index[i][j]])\n",
        "      sort_str.append(tmp)\n",
        "\n",
        "    sort_f = open('selected_concept.txt','a')\n",
        "    for line in sort_str:\n",
        "      sort_f.write(str(line) + '\\n')\n",
        "    sort_f.close()\n",
        "\n",
        "\n",
        "  def inference(self, decoder_output_t, ce_alignments_t, ct_alignments_t, word2id, local_entity, only_two_entity, id2entity):\n",
        "    batch_size = decoder_output_t.shape[0]\n",
        "\n",
        "    logits = self.logits_linear(decoder_output_t.squeeze(1)) # batch * num_symbols\n",
        "\n",
        "    selector = self.softmax_d1(self.selector_linear(decoder_output_t.squeeze(1)))\n",
        "\n",
        "    (word_prob, word_t) = torch.max(selector[:,0].unsqueeze(1) * self.softmax_d1(logits), dim = 1)\n",
        "    (local_entity_prob, local_entity_l_index_t) = torch.max(selector[:,1].unsqueeze(1) * ce_alignments_t, dim = 1)\n",
        "    (only_two_entity_prob, only_two_entity_l_index_t) = torch.max(selector[:,2].unsqueeze(1) * ct_alignments_t, dim = 1)\n",
        "\n",
        "    selector[:,0] = selector[:,0] * word_prob\n",
        "    selector[:,1] = selector[:,1] * local_entity_prob\n",
        "    selector[:,2] = selector[:,2] * only_two_entity_prob\n",
        "    selector = torch.argmax(selector, dim = 1)\n",
        "\n",
        "    local_entity_l_index_t = local_entity_l_index_t.cpu().numpy().tolist()\n",
        "    only_two_entity_l_index_t = only_two_entity_l_index_t.cpu().numpy().tolist()\n",
        "    word_t = word_t.cpu().numpy().tolist()\n",
        "\n",
        "    word_local_entity_t = []\n",
        "    word_only_two_entity_t = []\n",
        "    word_index_final_t = []\n",
        "    for i in range(batch_size):\n",
        "        if selector[i] == 0:\n",
        "            word_index_final_t.append(word_t[i])\n",
        "            continue\n",
        "        if selector[i] == 1:\n",
        "            local_entity_index_t = int(local_entity[i][local_entity_l_index_t[i]])\n",
        "            local_entity_text = id2entity[local_entity_index_t]\n",
        "            if local_entity_text not in word2id:\n",
        "                local_entity_text = '_UNK'\n",
        "            word_index_final_t.append(word2id[local_entity_text])\n",
        "            continue\n",
        "        if selector[i] == 2:\n",
        "            only_two_entity_index_t = int(only_two_entity[i][only_two_entity_l_index_t[i]])\n",
        "            only_two_entity_text = id2entity[only_two_entity_index_t]\n",
        "            if only_two_entity_text not in word2id:\n",
        "                only_two_entity_text = '_UNK'\n",
        "            word_index_final_t.append(word2id[only_two_entity_text])\n",
        "            continue\n",
        "\n",
        "    word_index_final_t = use_cuda(torch.LongTensor(word_index_final_t))\n",
        "    decoder_input_t = self.WordEmbedding(word_index_final_t)\n",
        "\n",
        "    return decoder_input_t, word_index_final_t, selector\n",
        "\n",
        "  def total_loss(self, decoder_output, responses_target, decoder_mask, ce_alignments, ct_alignments, use_entities_local, \\\n",
        "        entity_targets_local, use_entities_only_two, entity_targets_only_two):\n",
        "    batch_size = decoder_output.shape[0]\n",
        "    decoder_len = responses_target.shape[1]\n",
        "\n",
        "    local_masks = use_cuda(decoder_mask.reshape([-1]).type(\"torch.FloatTensor\"))\n",
        "    local_masks_word = use_cuda((1 - use_entities_local - use_entities_only_two).reshape([-1]).type(\"torch.FloatTensor\")) * local_masks\n",
        "    local_masks_local = use_cuda(use_entities_local.reshape([-1]).type(\"torch.FloatTensor\"))\n",
        "    local_masks_only_two = use_cuda(use_entities_only_two.reshape([-1]).type(\"torch.FloatTensor\"))\n",
        "    logits = self.logits_linear(decoder_output) #batch * decoder_len * num_symbols\n",
        "\n",
        "    word_prob = torch.gather(self.softmax_d2(logits), 2, responses_target.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "    selector_word, selector_local, selector_only_two = torch.split(self.softmax_d2(self.selector_linear(decoder_output)), [1, 1, 1], 2) #batch_size * decoder_len * 1\n",
        "    selector_word = selector_word.squeeze(2)\n",
        "    selector_local = selector_local.squeeze(2)\n",
        "    selector_only_two = selector_only_two.squeeze(2)\n",
        "\n",
        "    entity_prob_local = torch.sum(ce_alignments * entity_targets_local, [2])\n",
        "    entity_prob_only_two = torch.sum(ct_alignments * entity_targets_only_two, [2])\n",
        "\n",
        "    ppx_prob = word_prob * (1 - use_entities_local - use_entities_only_two) + entity_prob_local * use_entities_local + entity_prob_only_two * use_entities_only_two\n",
        "    ppx_word = word_prob * (1 - use_entities_local - use_entities_only_two)\n",
        "    ppx_local = entity_prob_local * use_entities_local\n",
        "    ppx_only_two = entity_prob_only_two * use_entities_only_two\n",
        "\n",
        "    final_prob = word_prob * selector_word * (1 - use_entities_local - use_entities_only_two) + entity_prob_local * selector_local * \\\n",
        "        use_entities_local + entity_prob_only_two * selector_only_two * use_entities_only_two\n",
        "\n",
        "    final_loss = torch.sum(- torch.log(1e-12 + final_prob).reshape([-1]) * local_masks)\n",
        "\n",
        "    sentence_ppx = torch.sum((- torch.log(1e-12 + ppx_prob).reshape([-1]) * local_masks).reshape([batch_size, -1]), 1)\n",
        "    sentence_ppx_word = torch.sum((- torch.log(1e-12 + ppx_word).reshape([-1]) * local_masks_word).reshape([batch_size, -1]), 1)\n",
        "    sentence_ppx_local = torch.sum((- torch.log(1e-12 + ppx_local).reshape([-1]) * local_masks_local).reshape([batch_size, -1]), 1)\n",
        "    sentence_ppx_only_two = torch.sum((- torch.log(1e-12 + ppx_only_two).reshape([-1]) * local_masks_only_two).reshape([batch_size, -1]), 1)\n",
        "\n",
        "    selector_loss = torch.sum(- torch.log(1e-12 + selector_local * use_entities_local + selector_only_two * use_entities_only_two + \\\n",
        "        selector_word * (1 - use_entities_local - use_entities_only_two)).reshape([-1]) * local_masks)\n",
        "\n",
        "    loss = final_loss + selector_loss\n",
        "    total_size = torch.sum(local_masks)\n",
        "    total_size += 1e-12\n",
        "\n",
        "    sum_word = torch.sum(use_cuda(((1 - use_entities_local - use_entities_only_two) * use_cuda(decoder_mask.type(\"torch.FloatTensor\"))).type(\"torch.FloatTensor\")), 1)\n",
        "    sum_local = torch.sum(use_cuda(use_entities_local.type(\"torch.FloatTensor\")), 1)\n",
        "    sum_only_two= torch.sum(use_cuda(use_entities_only_two.type(\"torch.FloatTensor\")), 1)\n",
        "\n",
        "    word_neg_mask = use_cuda((sum_word == 0).type(\"torch.FloatTensor\"))\n",
        "    local_neg_mask = use_cuda((sum_local == 0).type(\"torch.FloatTensor\"))\n",
        "    only_two_neg_mask = use_cuda((sum_only_two == 0).type(\"torch.FloatTensor\"))\n",
        "\n",
        "    word_neg_num = torch.sum(word_neg_mask)\n",
        "    local_neg_num = torch.sum(local_neg_mask)\n",
        "    only_two_neg_num = torch.sum(only_two_neg_mask)\n",
        "\n",
        "    sum_word = sum_word + word_neg_mask\n",
        "    sum_local = sum_local + local_neg_mask\n",
        "    sum_only_two = sum_only_two + only_two_neg_mask\n",
        "\n",
        "    return loss / total_size, 0, sentence_ppx / torch.sum(use_cuda(decoder_mask.type(\"torch.FloatTensor\")), 1), \\\n",
        "        sentence_ppx_word / sum_word, sentence_ppx_local / sum_local, sentence_ppx_only_two / sum_only_two, word_neg_num, \\\n",
        "        local_neg_num, only_two_neg_num\n",
        "\n",
        "\n",
        "\n",
        "  def attention(self, c_attention_keys, c_attention_values, ce_attention_keys, ce_attention_values, co_attention_keys, \\\n",
        "        co_attention_values, ct_attention_keys, decoder_state, local_entity_mask, only_two_entity_mask):\n",
        "    batch_size = ct_attention_keys.shape[0]\n",
        "    only_two_len = ct_attention_keys.shape[1]\n",
        "\n",
        "    c_query = decoder_state.reshape([-1, 1, self.units])\n",
        "    ce_query = decoder_state.reshape([-1, 1, self.units])\n",
        "    co_query = decoder_state.reshape([-1, 1, self.units])\n",
        "    ct_query = decoder_state.reshape([-1, 1, self.units])\n",
        "\n",
        "    c_scores = torch.sum(c_attention_keys * c_query, 2)\n",
        "    ce_scores = torch.sum(ce_attention_keys * ce_query, 2)\n",
        "    co_scores = torch.sum(co_attention_keys * co_query, 2)\n",
        "    ct_scores = torch.sum(ct_attention_keys * ct_query, 2)\n",
        "\n",
        "    c_alignments = self.softmax_d1(c_scores)\n",
        "    ce_alignments = self.softmax_d1(ce_scores)\n",
        "    co_alignments = self.softmax_d1(co_scores)\n",
        "    ct_alignments = self.softmax_d1(ct_scores)\n",
        "\n",
        "    ce_alignments = ce_alignments * use_cuda(local_entity_mask.type(\"torch.FloatTensor\"))\n",
        "    ct_alignments = ct_alignments * use_cuda(only_two_entity_mask.type(\"torch.FloatTensor\"))\n",
        "\n",
        "    c_context = torch.sum(c_alignments.unsqueeze(2) * c_attention_values, 1)\n",
        "    ce_context = torch.sum(ce_alignments.unsqueeze(2) * ce_attention_values, 1)\n",
        "    co_context = torch.sum(co_alignments.unsqueeze(2) * co_attention_values, 1)\n",
        "\n",
        "    context = self.context_linear(torch.cat((decoder_state, c_context, ce_context, co_context), 1))\n",
        "\n",
        "    return context, ce_alignments, co_alignments, ct_alignments\n"
      ],
      "metadata": {
        "id": "Ij5DceOZwbWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prepare_data"
      ],
      "metadata": {
        "id": "6aooXeq2c9Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(config):\n",
        "    global csk_entities, csk_triples, kb_dict, dict_csk_entities, dict_csk_triples\n",
        "\n",
        "    with open('%s/resource.txt' % config.data_dir) as f:\n",
        "        d = json.loads(f.readline())\n",
        "\n",
        "    csk_triples = d['csk_triples']\n",
        "    csk_entities = d['csk_entities']\n",
        "    raw_vocab = d['vocab_dict']\n",
        "    kb_dict = d['dict_csk']\n",
        "    dict_csk_entities = d['dict_csk_entities']\n",
        "    dict_csk_triples = d['dict_csk_triples']\n",
        "\n",
        "    data_train, data_test = [], []\n",
        "\n",
        "    if config.is_train:\n",
        "        with open('%s/trainset3.txt' % config.data_dir) as f:\n",
        "            for idx, line in enumerate(f):\n",
        "                if idx % 100000 == 0: print('read train file line %d' % idx)\n",
        "                data_train.append(json.loads(line))\n",
        "\n",
        "\n",
        "    with open('%s/testset.txt' % config.data_dir) as f:\n",
        "        for line in f:\n",
        "            data_test.append(json.loads(line))\n",
        "\n",
        "    return raw_vocab, data_train, data_test\n",
        "\n",
        "def build_vocab(path, raw_vocab, config, trans='transE'):\n",
        "\n",
        "    print(\"Creating word vocabulary...\")\n",
        "    vocab_list = ['_PAD','_GO', '_EOS', '_UNK', ] + sorted(raw_vocab, key=raw_vocab.get, reverse=True)\n",
        "    if len(vocab_list) > config.symbols:\n",
        "        vocab_list = vocab_list[:config.symbols]\n",
        "\n",
        "    print(\"Creating entity vocabulary...\")\n",
        "    entity_list = ['_NONE', '_PAD_H', '_PAD_R', '_PAD_T', '_NAF_H', '_NAF_R', '_NAF_T']\n",
        "    with open('%s/entity.txt' % path) as f:\n",
        "        for i, line in enumerate(f):\n",
        "            e = line.strip()\n",
        "            entity_list.append(e)\n",
        "\n",
        "    print(\"Creating relation vocabulary...\")\n",
        "    relation_list = []\n",
        "    with open('%s/relation.txt' % path) as f:\n",
        "        for i, line in enumerate(f):\n",
        "            r = line.strip()\n",
        "            relation_list.append(r)\n",
        "\n",
        "    print(\"Loading word vectors...\")\n",
        "    vectors = {}\n",
        "    with open('%s/glove.840B.300d.txt' % path, encoding='utf-8') as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i % 100000 == 0:\n",
        "                print(\"    processing line %d\" % i)\n",
        "            s = line.strip()\n",
        "            word = s[:s.find(' ')]\n",
        "            vector = s[s.find(' ')+1:]\n",
        "            vectors[word] = vector\n",
        "\n",
        "    embed = []\n",
        "    for word in vocab_list:\n",
        "        if word in vectors:\n",
        "            #vector = map(float, vectors[word].split())\n",
        "            vector = vectors[word].split()\n",
        "        else:\n",
        "            vector = np.zeros((config.embed_units), dtype=np.float32)\n",
        "        embed.append(vector)\n",
        "    embed = np.array(embed, dtype=np.float32)\n",
        "\n",
        "    print(\"Loading entity vectors...\")\n",
        "    entity_embed = []\n",
        "    with open('%s/entity_%s.txt' % (path, trans)) as f:\n",
        "        for i, line in enumerate(f):\n",
        "            s = line.strip().split('\\t')\n",
        "            #entity_embed.append(map(float, s))\n",
        "            entity_embed.append(s)\n",
        "\n",
        "    print(\"Loading relation vectors...\")\n",
        "    relation_embed = []\n",
        "    with open('%s/relation_%s.txt' % (path, trans)) as f:\n",
        "        for i, line in enumerate(f):\n",
        "            s = line.strip().split('\\t')\n",
        "            relation_embed.append(s)\n",
        "\n",
        "    entity_relation_embed = np.array(entity_embed+relation_embed, dtype=np.float32)\n",
        "    entity_embed = np.array(entity_embed, dtype=np.float32)\n",
        "    relation_embed = np.array(relation_embed, dtype=np.float32)\n",
        "\n",
        "    word2id = dict()\n",
        "    entity2id = dict()\n",
        "    for word in vocab_list:\n",
        "        word2id[word] = len(word2id)\n",
        "    for entity in entity_list + relation_list:\n",
        "        entity2id[entity] = len(entity2id)\n",
        "\n",
        "    return word2id, entity2id, vocab_list, embed, entity_list, entity_embed, relation_list, relation_embed, entity_relation_embed\n",
        "\n",
        "def gen_batched_data(data, config, word2id, entity2id):\n",
        "    global csk_entities, csk_triples, kb_dict, dict_csk_entities, dict_csk_triples\n",
        "\n",
        "    encoder_len = max([len(item['post']) for item in data])+1\n",
        "\n",
        "    decoder_len = max([len(item['response']) for item in data])+1\n",
        "    triple_num = max([len(item['all_triples_one_hop']) for item in data])\n",
        "    entity_len = max([len(item['all_entities_one_hop']) + max(item['post_triples']) for item in data])\n",
        "    only_two_entity_len = max([len(item['only_two']) for item in data])\n",
        "    triple_num_one_two = max([len(item['one_two_triple']) for item in data])\n",
        "    triple_len_one_two = max([len(tri) for item in data for tri in item['one_two_triple']])\n",
        "    posts_id = np.full((len(data), encoder_len), 0, dtype=int)\n",
        "    responses_id = np.full((len(data), decoder_len), 0, dtype=int)\n",
        "    responses_length = []\n",
        "    # posts_length = []\n",
        "    local_entity_length = []\n",
        "    only_two_entity_length = []\n",
        "    local_entity = []\n",
        "    only_two_entity = []\n",
        "    kb_fact_rels = np.full((len(data), triple_num), 2, dtype=int)\n",
        "    kb_adj_mats = np.empty(len(data), dtype=object)\n",
        "    q2e_adj_mats = np.full((len(data), entity_len), 0, dtype=int)\n",
        "    match_entity_one_hop = np.full((len(data), decoder_len), -1, dtype=int)\n",
        "    match_entity_only_two = np.full((len(data), decoder_len), -1, dtype=int)\n",
        "    one_two_triples_id = []\n",
        "    g2l_only_two_list = []\n",
        "    # o2t_entity_index_list = []\n",
        "\n",
        "    next_id = 0\n",
        "    for item in data:\n",
        "        # posts\n",
        "        for i, post_word in enumerate(padding(item['post'], encoder_len)):\n",
        "            if post_word in word2id:\n",
        "                posts_id[next_id, i] = word2id[post_word]\n",
        "\n",
        "            else:\n",
        "                posts_id[next_id, i] = word2id['_UNK']\n",
        "\n",
        "        # responses\n",
        "        for i, response_word in enumerate(padding(item['response'], decoder_len)):\n",
        "            if response_word in word2id:\n",
        "                responses_id[next_id, i] = word2id[response_word]\n",
        "\n",
        "            else:\n",
        "                responses_id[next_id, i] = word2id['_UNK']\n",
        "\n",
        "        # responses_length\n",
        "        responses_length.append(len(item['response']) + 1)\n",
        "\n",
        "        # local_entity\n",
        "        local_entity_tmp = []\n",
        "        for i in range(len(item['post_triples'])):\n",
        "            if item['post_triples'][i] == 0:\n",
        "                continue\n",
        "            elif item['post'][i] not in entity2id:\n",
        "                continue\n",
        "            elif entity2id[item['post'][i]] in local_entity_tmp:\n",
        "                continue\n",
        "            else:\n",
        "                local_entity_tmp.append(entity2id[item['post'][i]])\n",
        "\n",
        "        for entity_index in item['all_entities_one_hop']:\n",
        "            if csk_entities[entity_index] not in entity2id:\n",
        "                continue\n",
        "            if entity2id[csk_entities[entity_index]] in local_entity_tmp:\n",
        "                continue\n",
        "            else:\n",
        "                local_entity_tmp.append(entity2id[csk_entities[entity_index]])\n",
        "        local_entity_len_tmp = len(local_entity_tmp)\n",
        "        local_entity_tmp += [1] * (entity_len - len(local_entity_tmp))\n",
        "        local_entity.append(local_entity_tmp)\n",
        "\n",
        "        # kb_adj_mat and kb_fact_rel\n",
        "        g2l = dict()\n",
        "        for i in range(len(local_entity_tmp)):\n",
        "            g2l[local_entity_tmp[i]] = i\n",
        "\n",
        "        entity2fact_e, entity2fact_f = [], []\n",
        "        fact2entity_f, fact2entity_e = [], []\n",
        "\n",
        "        tmp_count = 0\n",
        "        for i in range(len(item['all_triples_one_hop'])):\n",
        "            sbj = csk_triples[item['all_triples_one_hop'][i]].split()[0][:-1]\n",
        "            rel = csk_triples[item['all_triples_one_hop'][i]].split()[1][:-1]\n",
        "            obj = csk_triples[item['all_triples_one_hop'][i]].split()[2]\n",
        "\n",
        "            if (sbj not in entity2id) or (obj not in entity2id):\n",
        "                continue\n",
        "            if (entity2id[sbj] not in g2l) or (entity2id[obj] not in g2l):\n",
        "                continue\n",
        "\n",
        "            entity2fact_e += [g2l[entity2id[sbj]]]\n",
        "            entity2fact_f += [tmp_count]\n",
        "            fact2entity_f += [tmp_count]\n",
        "            fact2entity_e += [g2l[entity2id[obj]]]\n",
        "            kb_fact_rels[next_id, tmp_count] = entity2id[rel]\n",
        "            tmp_count += 1\n",
        "\n",
        "        kb_adj_mats[next_id] = (np.array(entity2fact_f, dtype=int), np.array(entity2fact_e, dtype=int), np.array([1.0] * len(entity2fact_f))), (np.array(fact2entity_e, dtype=int), np.array(fact2entity_f, dtype=int), np.array([1.0] * len(fact2entity_e)))\n",
        "\n",
        "        # q2e_adj_mat\n",
        "        for i in range(len(item['post_triples'])):\n",
        "            if item['post_triples'][i] == 0:\n",
        "                continue\n",
        "            elif item['post'][i] not in entity2id:\n",
        "                continue\n",
        "            else:\n",
        "                q2e_adj_mats[next_id, g2l[entity2id[item['post'][i]]]] = 1\n",
        "\n",
        "        # match_entity_one_hop\n",
        "        for i in range(len(item['match_response_index_one_hop'])):\n",
        "            if item['match_response_index_one_hop'][i] == -1:\n",
        "                continue\n",
        "            if csk_entities[item['match_response_index_one_hop'][i]] not in entity2id:\n",
        "                continue\n",
        "            if entity2id[csk_entities[item['match_response_index_one_hop'][i]]] not in g2l:\n",
        "                continue\n",
        "            else:\n",
        "                match_entity_one_hop[next_id, i] = g2l[entity2id[csk_entities[item['match_response_index_one_hop'][i]]]]\n",
        "\n",
        "        # only_two_entity\n",
        "        only_two_entity_tmp = []\n",
        "        for entity_index in item['only_two']:\n",
        "            if csk_entities[entity_index] not in entity2id:\n",
        "                continue\n",
        "            if entity2id[csk_entities[entity_index]] in only_two_entity_tmp:\n",
        "                continue\n",
        "            else:\n",
        "                only_two_entity_tmp.append(entity2id[csk_entities[entity_index]])\n",
        "        only_two_entity_len_tmp = len(only_two_entity_tmp)\n",
        "        only_two_entity_tmp += [1] * (only_two_entity_len - len(only_two_entity_tmp))\n",
        "        only_two_entity.append(only_two_entity_tmp)\n",
        "\n",
        "        # match_entity_two_hop\n",
        "        g2l_only_two = dict()\n",
        "        for i in range(len(only_two_entity_tmp)):\n",
        "            g2l_only_two[only_two_entity_tmp[i]] = i\n",
        "\n",
        "        for i in range(len(item['match_response_index_only_two'])):\n",
        "            if item['match_response_index_only_two'][i] == -1:\n",
        "                continue\n",
        "            if csk_entities[item['match_response_index_only_two'][i]] not in entity2id:\n",
        "                continue\n",
        "            else:\n",
        "                match_entity_only_two[next_id, i] = g2l_only_two[entity2id[csk_entities[item['match_response_index_only_two'][i]]]]\n",
        "\n",
        "        # one_two_triple\n",
        "        one_two_triples_id.append(padding_triple_id(entity2id, [[csk_triples[x].split(', ') for x in triple] for triple in item['one_two_triple']], triple_num_one_two, triple_len_one_two))\n",
        "\n",
        "        ############################ g2l_only_two\n",
        "        g2l_only_two_list.append(g2l_only_two)\n",
        "\n",
        "        # local_entity_length\n",
        "        local_entity_length.append(local_entity_len_tmp)\n",
        "\n",
        "        # only_two_entity_length\n",
        "        only_two_entity_length.append(only_two_entity_len_tmp)\n",
        "\n",
        "        next_id += 1\n",
        "\n",
        "    batched_data = {'query_text': np.array(posts_id),\n",
        "            'answer_text': np.array(responses_id),\n",
        "            'local_entity': np.array(local_entity),\n",
        "            'responses_length': responses_length,\n",
        "            'q2e_adj_mat': np.array(q2e_adj_mats),\n",
        "            'kb_adj_mat': build_kb_adj_mat(kb_adj_mats, config.fact_dropout),\n",
        "            'kb_fact_rel': np.array(kb_fact_rels),\n",
        "            'match_entity_one_hop': np.array(match_entity_one_hop),\n",
        "            'only_two_entity': np.array(only_two_entity),\n",
        "            'match_entity_only_two': np.array(match_entity_only_two),\n",
        "            'one_two_triples_id': np.array(one_two_triples_id),\n",
        "            'word2id': word2id,\n",
        "            'entity2id': entity2id,\n",
        "            'local_entity_length': local_entity_length,\n",
        "            'only_two_entity_length': only_two_entity_length}\n",
        "\n",
        "    return batched_data"
      ],
      "metadata": {
        "id": "RctkuElmc9sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train / Test"
      ],
      "metadata": {
        "id": "pyNM7myhd61a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#coding:utf-8\n",
        "csk_triples, csk_entities, kb_dict = [], [], []\n",
        "dict_csk_entities, dict_csk_triples = {}, {}\n",
        "\n",
        "class Config():\n",
        "    def __init__(self, path):\n",
        "        self.config_path = path\n",
        "        self._get_config()\n",
        "\n",
        "    def _get_config(self):\n",
        "        with open(\"/content/drive/MyDrive/Colab Notebooks/ConceptFlow-master/config.yml\", \"r\") as setting:\n",
        "            config = yaml.load(setting, Loader=yaml.FullLoader)\n",
        "        self.is_train = config['is_train']\n",
        "        self.test_model_path = config['test_model_path']\n",
        "        self.embed_units = config['embed_units']\n",
        "        self.symbols = config['symbols']\n",
        "        self.units = config['units']\n",
        "        self.layers = config['layers']\n",
        "        self.batch_size = config['batch_size']\n",
        "        self.data_dir = config['data_dir']\n",
        "        self.num_epoch = config['num_epoch']\n",
        "        self.lr_rate = config['lr_rate']\n",
        "        self.lstm_dropout = config['lstm_dropout']\n",
        "        self.linear_dropout = config['linear_dropout']\n",
        "        self.max_gradient_norm = config['max_gradient_norm']\n",
        "        self.trans_units = config['trans_units']\n",
        "        self.gnn_layers = config['gnn_layers']\n",
        "        self.fact_dropout = config['fact_dropout']\n",
        "        self.fact_scale = config['fact_scale']\n",
        "        self.pagerank_lambda = config['pagerank_lambda']\n",
        "        self.result_dir_name = config['result_dir_name']\n",
        "\n",
        "    def list_all_member(self):\n",
        "        for name, value in vars(self).items():\n",
        "            print('%s = %s' % (name, value))\n",
        "\n",
        "\n",
        "def run(model, data_train, config, word2id, entity2id):\n",
        "    batched_data = gen_batched_data(data_train, config, word2id, entity2id)\n",
        "\n",
        "    if model.is_inference == True:\n",
        "        word_index, selector = model(batched_data)\n",
        "        return word_index, selector\n",
        "    else:\n",
        "        decoder_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, word_neg_num, local_neg_num, only_two_neg_num = model(batched_data)\n",
        "        return decoder_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, word_neg_num, local_neg_num, only_two_neg_num\n",
        "\n",
        "def train(config, model, data_train, data_test, word2id, entity2id, model_optimizer):\n",
        "  for epoch in range(config.num_epoch):\n",
        "    print (\"epoch: \", epoch)\n",
        "    sentence_ppx_loss = 0\n",
        "    sentence_ppx_word_loss = 0\n",
        "    sentence_ppx_local_loss = 0\n",
        "    sentence_ppx_only_two_loss = 0\n",
        "\n",
        "    # word_cut = use_cuda(torch.Tensor([0]))\n",
        "    # local_cut = use_cuda(torch.Tensor([0]))\n",
        "    # only_two_cut = use_cuda(torch.Tensor([0]))\n",
        "    word_cut = torch.tensor([0], device=device)\n",
        "    local_cut = torch.tensor([0], device=device)\n",
        "    only_two_cut = torch.tensor([0], device=device)\n",
        "\n",
        "    count = 0\n",
        "    for iteration in range(len(data_train) // config.batch_size):\n",
        "        decoder_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, word_neg_num, local_neg_num, \\\n",
        "            only_two_neg_num = run(model, data_train[(iteration * config.batch_size):(iteration * \\\n",
        "            config.batch_size + config.batch_size)], config, word2id, entity2id)\n",
        "        sentence_ppx_loss += torch.sum(sentence_ppx).data\n",
        "        sentence_ppx_word_loss += torch.sum(sentence_ppx_word).data\n",
        "        sentence_ppx_local_loss += torch.sum(sentence_ppx_local).data\n",
        "        sentence_ppx_only_two_loss += torch.sum(sentence_ppx_only_two).data\n",
        "\n",
        "        # print('word_cut', word_cut.type())\n",
        "        # print('word_neg_num',word_neg_num.type())\n",
        "        word_cut = word_cut.float()\n",
        "        local_cut= local_cut.float()\n",
        "        only_two_cut = only_two_cut.float()\n",
        "        # print('word_cut',word_cut.type())\n",
        "\n",
        "        word_cut += word_neg_num\n",
        "        local_cut += local_neg_num\n",
        "        only_two_cut += only_two_neg_num\n",
        "\n",
        "        model_optimizer.zero_grad()\n",
        "        decoder_loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm(model.parameters(), config.max_gradient_norm)\n",
        "        model_optimizer.step()\n",
        "\n",
        "        if count % 50 == 0:\n",
        "            print (\"iteration:\", iteration, \"Loss:\", decoder_loss.data)\n",
        "        count += 1\n",
        "\n",
        "    print (\"perplexity for epoch\", epoch + 1, \":\", np.exp(sentence_ppx_loss.cpu() / len(data_train)), \" ppx_word: \", \\\n",
        "        np.exp(sentence_ppx_word_loss.cpu() / (len(data_train) - int(word_cut))), \" ppx_local: \", \\\n",
        "        np.exp(sentence_ppx_local_loss.cpu() / (len(data_train) - int(local_cut))), \" ppx_only_two: \", \\\n",
        "        np.exp(sentence_ppx_only_two_loss.cpu() / (len(data_train) - int(only_two_cut))))\n",
        "\n",
        "    torch.save(model.state_dict(), config.result_dir_name + '/' + '_epoch_' + str(epoch + 1) + '.pkl')\n",
        "    ppx, ppx_word, ppx_local, ppx_only_two = evaluate(model, data_test, config, word2id, entity2id, epoch + 1)\n",
        "    ppx_f = open(config.result_dir_name + '/result.txt','a')\n",
        "    ppx_f.write(\"epoch \" + str(epoch + 1) + \" ppx: \" + str(ppx) + \" ppx_word: \" + str(ppx_word) + \" ppx_local: \" + \\\n",
        "        str(ppx_local) + \" ppx_only_two: \" + str(ppx_only_two) + '\\n')\n",
        "    ppx_f.close()\n",
        "\n",
        "def evaluate(model, data_test, config, word2id, entity2id, epoch = 0, model_path = None):\n",
        "  if model_path != None:\n",
        "      model.load_state_dict(torch.load(model_path))\n",
        "  sentence_ppx_loss = 0\n",
        "  sentence_ppx_word_loss = 0\n",
        "  sentence_ppx_local_loss = 0\n",
        "  sentence_ppx_only_two_loss = 0\n",
        "  word_cut = use_cuda(torch.Tensor([0]))\n",
        "  local_cut = use_cuda(torch.Tensor([0]))\n",
        "  only_two_cut = use_cuda(torch.Tensor([0]))\n",
        "  count = 0\n",
        "  id2word = dict()\n",
        "  for key in word2id.keys():\n",
        "      id2word[word2id[key]] = key\n",
        "\n",
        "\n",
        "  for iteration in range(len(data_test) // config.batch_size):\n",
        "    decoder_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, word_neg_num, \\\n",
        "        local_neg_num, only_two_neg_num = run(model, data_test[(iteration * config.batch_size):(iteration * \\\n",
        "        config.batch_size + config.batch_size)], config, word2id, entity2id)\n",
        "    sentence_ppx_loss += torch.sum(sentence_ppx).data\n",
        "    sentence_ppx_word_loss += torch.sum(sentence_ppx_word).data\n",
        "    sentence_ppx_local_loss += torch.sum(sentence_ppx_local).data\n",
        "    sentence_ppx_only_two_loss += torch.sum(sentence_ppx_only_two).data\n",
        "\n",
        "    word_cut = word_cut.float()\n",
        "    local_cut= local_cut.float()\n",
        "    only_two_cut = only_two_cut.float()\n",
        "\n",
        "    word_cut += word_neg_num\n",
        "    local_cut += local_neg_num\n",
        "    only_two_cut += only_two_neg_num\n",
        "\n",
        "    if count % 50 == 0:\n",
        "        print (\"iteration for evaluate:\", iteration, \"Loss:\", decoder_loss.data)\n",
        "    count += 1\n",
        "\n",
        "  model.is_inference = False\n",
        "  if model_path != None:\n",
        "    print('    perplexity on test set:', np.exp(sentence_ppx_loss.cpu() / len(data_test)), \\\n",
        "        np.exp(sentence_ppx_word_loss.cpu() / (len(data_test) - int(word_cut))), np.exp(sentence_ppx_local_loss.cpu() / (len(data_test) \\\n",
        "        - int(local_cut))), np.exp(sentence_ppx_only_two_loss.cpu() / (len(data_test) - int(only_two_cut))))\n",
        "    exit()\n",
        "  print('    perplexity on test set:', np.exp(sentence_ppx_loss.cpu() / len(data_test)), np.exp(sentence_ppx_word_loss.cpu() / \\\n",
        "    (len(data_test) - int(word_cut))), np.exp(sentence_ppx_local_loss.cpu() / (len(data_test) - int(local_cut))), \\\n",
        "    np.exp(sentence_ppx_only_two_loss.cpu() / (len(data_test) - int(only_two_cut))))\n",
        "  return np.exp(sentence_ppx_loss.cpu() / len(data_test)), np.exp(sentence_ppx_word_loss.cpu() / (len(data_test) - int(word_cut))), \\\n",
        "    np.exp(sentence_ppx_local_loss.cpu() / (len(data_test) - int(local_cut))), np.exp(sentence_ppx_only_two_loss.cpu() / \\\n",
        "    (len(data_test) - int(only_two_cut)))\n",
        "\n",
        "def main():\n",
        "    config = Config('config.yml')\n",
        "    config.list_all_member()\n",
        "    raw_vocab, data_train, data_test = prepare_data(config)\n",
        "    word2id, entity2id, vocab, embed, entity_vocab, entity_embed, relation_vocab, relation_embed, entity_relation_embed = build_vocab(config.data_dir, raw_vocab, config = config)\n",
        "    # model = use_cuda(ConceptFlow(config, embed, entity_relation_embed))\n",
        "\n",
        "    model = ConceptFlow(config, embed, entity_relation_embed)\n",
        "\n",
        "    model = model.to(device=device)\n",
        "    model_optimizer = torch.optim.Adam(model.parameters(), lr = config.lr_rate)\n",
        "\n",
        "    if not os.path.exists(config.result_dir_name):\n",
        "        os.mkdir(config.result_dir_name)\n",
        "    ppx_f = open(config.result_dir_name + '/result.txt','a')\n",
        "    for name, value in vars(config).items():\n",
        "        ppx_f.write('%s = %s' % (name, value) + '\\n')\n",
        "\n",
        "    if config.is_train == False:\n",
        "        evaluate(model, data_test, config, word2id, entity2id, 0, model_path = config.test_model_path)\n",
        "        exit()\n",
        "    train(config, model, data_train, data_test, word2id, entity2id, model_optimizer)"
      ],
      "metadata": {
        "id": "xHvhcILkQ8qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbHiqvaanhz8",
        "outputId": "d36c534d-403b-4794-8e5d-1da797dbd75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config_path = config.yml\n",
            "is_train = True\n",
            "test_model_path = None\n",
            "embed_units = 300\n",
            "symbols = 30000\n",
            "units = 512\n",
            "layers = 2\n",
            "batch_size = 10\n",
            "data_dir = /content/drive/MyDrive/Colab Notebooks/ConceptFlow(ECCF)_data\n",
            "num_epoch = 20\n",
            "lr_rate = 0.0001\n",
            "lstm_dropout = 0.3\n",
            "linear_dropout = 0.2\n",
            "max_gradient_norm = 5\n",
            "trans_units = 100\n",
            "gnn_layers = 3\n",
            "fact_dropout = 0.0\n",
            "fact_scale = 1\n",
            "pagerank_lambda = 0.8\n",
            "result_dir_name = training_output\n",
            "read train file line 0\n",
            "Creating word vocabulary...\n",
            "Creating entity vocabulary...\n",
            "Creating relation vocabulary...\n",
            "Loading word vectors...\n",
            "    processing line 0\n",
            "    processing line 100000\n",
            "    processing line 200000\n",
            "    processing line 300000\n",
            "    processing line 400000\n",
            "    processing line 500000\n",
            "    processing line 600000\n",
            "    processing line 700000\n",
            "    processing line 800000\n",
            "    processing line 900000\n",
            "    processing line 1000000\n",
            "    processing line 1100000\n",
            "    processing line 1200000\n",
            "    processing line 1300000\n",
            "    processing line 1400000\n",
            "    processing line 1500000\n",
            "    processing line 1600000\n",
            "Loading entity vectors...\n",
            "Loading relation vectors...\n",
            "epoch:  0\n",
            "iteration: 0 Loss: tensor(11.9379, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(7.5276, device='cuda:0')\n",
            "perplexity for epoch 1 : tensor(2903.5793)  ppx_word:  tensor(4749.7988)  ppx_local:  tensor(141.7914)  ppx_only_two:  tensor(95.7288)\n",
            "iteration for evaluate: 0 Loss: tensor(6.7202, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(7.0328, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(7.0025, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.2429, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.1895, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(7.0243, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.1497, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(7.0763, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.3784, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(6.9897, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.9824, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.2158, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.9053, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.9437, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.9302, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.3817, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(7.0292, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.2691, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(7.0602, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(7.1558, device='cuda:0')\n",
            "    perplexity on test set: tensor(415.8138) tensor(498.8073) tensor(125.7405) tensor(101.4107)\n",
            "epoch:  1\n",
            "iteration: 0 Loss: tensor(6.7201, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(6.7764, device='cuda:0')\n",
            "perplexity for epoch 2 : tensor(265.9440)  ppx_word:  tensor(309.1581)  ppx_local:  tensor(104.5853)  ppx_only_two:  tensor(90.0876)\n",
            "iteration for evaluate: 0 Loss: tensor(6.3201, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(6.6235, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8898, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.1540, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.1171, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.8862, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.0752, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.9194, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.2531, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(6.9206, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.8245, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.1352, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.7760, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6990, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.7073, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.1869, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(6.8376, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.1338, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.8670, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.9629, device='cuda:0')\n",
            "    perplexity on test set: tensor(356.4152) tensor(429.5007) tensor(99.0562) tensor(98.1047)\n",
            "epoch:  2\n",
            "iteration: 0 Loss: tensor(6.3250, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(6.6025, device='cuda:0')\n",
            "perplexity for epoch 3 : tensor(223.4118)  ppx_word:  tensor(257.3581)  ppx_local:  tensor(92.6460)  ppx_only_two:  tensor(81.9720)\n",
            "iteration for evaluate: 0 Loss: tensor(6.1655, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(6.5013, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8037, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.1786, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.1225, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.6934, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.0326, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.8544, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.1787, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(6.9159, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.7690, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0593, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.7324, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6300, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.6545, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.0919, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(6.7826, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.1320, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.7750, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.9159, device='cuda:0')\n",
            "    perplexity on test set: tensor(337.7048) tensor(405.4479) tensor(96.7869) tensor(97.1525)\n",
            "epoch:  3\n",
            "iteration: 0 Loss: tensor(6.1530, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(6.4763, device='cuda:0')\n",
            "perplexity for epoch 4 : tensor(198.6553)  ppx_word:  tensor(227.3488)  ppx_local:  tensor(86.9600)  ppx_only_two:  tensor(73.5519)\n",
            "iteration for evaluate: 0 Loss: tensor(5.9957, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(6.3853, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.7547, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.2144, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.1198, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.6569, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.0135, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.8550, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.2803, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(6.9267, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.7614, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0886, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.7674, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.5855, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.6837, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.1423, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(6.8076, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.1459, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.7388, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.8516, device='cuda:0')\n",
            "    perplexity on test set: tensor(336.1298) tensor(403.1006) tensor(93.1846) tensor(98.1513)\n",
            "epoch:  4\n",
            "iteration: 0 Loss: tensor(5.9793, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(6.3600, device='cuda:0')\n",
            "perplexity for epoch 5 : tensor(184.9756)  ppx_word:  tensor(210.6185)  ppx_local:  tensor(85.3863)  ppx_only_two:  tensor(66.1897)\n",
            "iteration for evaluate: 0 Loss: tensor(5.9024, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(6.3118, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.7963, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.3249, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.2170, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.6945, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.0618, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.9073, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.3695, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.0224, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.8050, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.1362, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.7829, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6376, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.7259, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.2390, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(6.8800, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.2128, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.7858, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.8943, device='cuda:0')\n",
            "    perplexity on test set: tensor(348.4441) tensor(421.3837) tensor(90.0539) tensor(100.7617)\n",
            "epoch:  5\n",
            "iteration: 0 Loss: tensor(5.9089, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(6.3037, device='cuda:0')\n",
            "perplexity for epoch 6 : tensor(175.5277)  ppx_word:  tensor(200.3834)  ppx_local:  tensor(80.8557)  ppx_only_two:  tensor(59.9547)\n",
            "iteration for evaluate: 0 Loss: tensor(5.8007, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(6.1656, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.7678, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.2891, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.1930, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.5952, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.0189, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.8441, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.2885, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(6.9946, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.7246, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0306, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.7375, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.5749, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.6441, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.1527, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(6.8524, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.1362, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.6983, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.8135, device='cuda:0')\n",
            "    perplexity on test set: tensor(332.5502) tensor(401.3788) tensor(85.5309) tensor(104.6736)\n",
            "epoch:  6\n",
            "iteration: 0 Loss: tensor(5.7850, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(6.2035, device='cuda:0')\n",
            "perplexity for epoch 7 : tensor(165.4796)  ppx_word:  tensor(189.1563)  ppx_local:  tensor(75.8106)  ppx_only_two:  tensor(54.8632)\n",
            "iteration for evaluate: 0 Loss: tensor(5.7285, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(6.0882, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.7679, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.3390, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.2513, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.5940, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.0494, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.8858, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.3512, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.0506, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.7639, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0679, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.7616, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.5927, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.6929, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.2348, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(6.8904, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.1737, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.7155, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.8280, device='cuda:0')\n",
            "    perplexity on test set: tensor(340.4892) tensor(413.3763) tensor(81.9638) tensor(109.4267)\n",
            "epoch:  7\n",
            "iteration: 0 Loss: tensor(5.7226, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(6.1639, device='cuda:0')\n",
            "perplexity for epoch 8 : tensor(157.2793)  ppx_word:  tensor(179.8470)  ppx_local:  tensor(71.9719)  ppx_only_two:  tensor(50.3683)\n",
            "iteration for evaluate: 0 Loss: tensor(5.6652, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(6.0363, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.7860, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.3594, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.2753, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.6184, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.0740, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.8921, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.3573, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.0681, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.7792, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0567, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.7532, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6141, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.7108, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.2552, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(6.9332, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.2168, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.7555, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.8592, device='cuda:0')\n",
            "    perplexity on test set: tensor(341.7384) tensor(414.9597) tensor(80.9313) tensor(113.8887)\n",
            "epoch:  8\n",
            "iteration: 0 Loss: tensor(5.6561, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(6.0903, device='cuda:0')\n",
            "perplexity for epoch 9 : tensor(148.8565)  ppx_word:  tensor(170.3721)  ppx_local:  tensor(69.4506)  ppx_only_two:  tensor(46.6289)\n",
            "iteration for evaluate: 0 Loss: tensor(5.6057, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.9699, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8307, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.3996, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.3173, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.6582, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.1171, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.9333, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.3864, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.0934, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.8147, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0637, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.7590, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6305, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.7336, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.3292, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(6.9511, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.2567, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.7916, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.8877, device='cuda:0')\n",
            "    perplexity on test set: tensor(349.1439) tensor(424.3138) tensor(82.1069) tensor(117.0148)\n",
            "epoch:  9\n",
            "iteration: 0 Loss: tensor(5.6003, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(6.0443, device='cuda:0')\n",
            "perplexity for epoch 10 : tensor(141.4662)  ppx_word:  tensor(162.2419)  ppx_local:  tensor(64.8953)  ppx_only_two:  tensor(43.2374)\n",
            "iteration for evaluate: 0 Loss: tensor(5.5174, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.9105, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8470, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.3997, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.3202, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.6429, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.1306, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.9533, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.3954, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.0780, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.8323, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0478, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.7869, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6099, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.7472, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.3441, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(6.9586, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.3038, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.8009, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.9306, device='cuda:0')\n",
            "    perplexity on test set: tensor(352.2843) tensor(426.2528) tensor(85.7761) tensor(121.5787)\n",
            "epoch:  10\n",
            "iteration: 0 Loss: tensor(5.5277, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(5.9733, device='cuda:0')\n",
            "perplexity for epoch 11 : tensor(133.1726)  ppx_word:  tensor(153.5108)  ppx_local:  tensor(59.4421)  ppx_only_two:  tensor(40.0313)\n",
            "iteration for evaluate: 0 Loss: tensor(5.4835, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.8466, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8102, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.4216, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.2795, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.5849, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.1548, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.8948, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.3795, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.0599, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.8562, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0228, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.8338, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.5977, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.7910, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.2992, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(6.9998, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.2544, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.8117, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.9038, device='cuda:0')\n",
            "    perplexity on test set: tensor(347.5246) tensor(415.2858) tensor(91.9344) tensor(126.6028)\n",
            "epoch:  11\n",
            "iteration: 0 Loss: tensor(5.4886, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(5.8634, device='cuda:0')\n",
            "perplexity for epoch 12 : tensor(123.8546)  ppx_word:  tensor(143.4149)  ppx_local:  tensor(52.9569)  ppx_only_two:  tensor(37.3751)\n",
            "iteration for evaluate: 0 Loss: tensor(5.4224, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.7335, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8261, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.4373, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.2916, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.5855, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.1806, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.9276, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.3946, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.0877, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.8917, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0047, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.8429, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.5951, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.7748, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.3305, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(7.0268, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.3000, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.7918, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.9208, device='cuda:0')\n",
            "    perplexity on test set: tensor(348.8327) tensor(415.8091) tensor(93.1742) tensor(132.1904)\n",
            "epoch:  12\n",
            "iteration: 0 Loss: tensor(5.4188, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(5.8033, device='cuda:0')\n",
            "perplexity for epoch 13 : tensor(114.6876)  ppx_word:  tensor(133.6012)  ppx_local:  tensor(45.8602)  ppx_only_two:  tensor(34.7327)\n",
            "iteration for evaluate: 0 Loss: tensor(5.3408, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.6461, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8637, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.4920, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.3153, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.6305, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.1769, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.9335, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.4004, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.1587, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.9796, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(6.9949, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.8750, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6062, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.7955, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.3192, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(7.0164, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.3108, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.8051, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.9644, device='cuda:0')\n",
            "    perplexity on test set: tensor(355.5919) tensor(412.3940) tensor(119.9012) tensor(137.5775)\n",
            "epoch:  13\n",
            "iteration: 0 Loss: tensor(5.3582, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(5.7437, device='cuda:0')\n",
            "perplexity for epoch 14 : tensor(107.4045)  ppx_word:  tensor(125.8748)  ppx_local:  tensor(42.1109)  ppx_only_two:  tensor(32.3178)\n",
            "iteration for evaluate: 0 Loss: tensor(5.2897, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.6102, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8982, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.5276, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.3441, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.6224, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.1900, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.9305, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.4631, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.1605, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.9770, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(6.9950, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.8853, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6201, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.7839, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.2092, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(7.0368, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.3146, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.8317, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.9345, device='cuda:0')\n",
            "    perplexity on test set: tensor(359.2697) tensor(413.2328) tensor(130.5990) tensor(140.9592)\n",
            "epoch:  14\n",
            "iteration: 0 Loss: tensor(5.2759, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(5.6885, device='cuda:0')\n",
            "perplexity for epoch 15 : tensor(101.0213)  ppx_word:  tensor(118.9828)  ppx_local:  tensor(38.1427)  ppx_only_two:  tensor(30.1950)\n",
            "iteration for evaluate: 0 Loss: tensor(5.1489, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.5009, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8492, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.5039, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.3295, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.6851, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.1980, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.9461, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.4530, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.0845, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(6.9712, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0215, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.8407, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.5902, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.8285, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.2643, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(7.0207, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.3671, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.8731, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.9636, device='cuda:0')\n",
            "    perplexity on test set: tensor(360.6763) tensor(406.1437) tensor(153.3489) tensor(140.2003)\n",
            "epoch:  15\n",
            "iteration: 0 Loss: tensor(5.1562, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(5.5975, device='cuda:0')\n",
            "perplexity for epoch 16 : tensor(95.7063)  ppx_word:  tensor(113.0264)  ppx_local:  tensor(34.8977)  ppx_only_two:  tensor(28.5445)\n",
            "iteration for evaluate: 0 Loss: tensor(5.0854, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.4295, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8269, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.5472, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.3168, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.6764, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.1892, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(6.9468, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.4836, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.2048, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(7.0279, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.0339, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.8740, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.5923, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.8078, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.2343, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(7.0748, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.3751, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.8305, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.9528, device='cuda:0')\n",
            "    perplexity on test set: tensor(361.1131) tensor(403.1749) tensor(162.8568) tensor(143.1812)\n",
            "epoch:  16\n",
            "iteration: 0 Loss: tensor(5.0706, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(5.4803, device='cuda:0')\n",
            "perplexity for epoch 17 : tensor(89.6399)  ppx_word:  tensor(105.8132)  ppx_local:  tensor(32.9063)  ppx_only_two:  tensor(26.7034)\n",
            "iteration for evaluate: 0 Loss: tensor(5.0703, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.4876, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8612, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.6281, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.4098, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.7051, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.2839, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(7.0099, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.6112, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.2376, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(7.0824, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.1188, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.9524, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6984, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.8885, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.3516, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(7.1352, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.4075, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.8814, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(6.9966, device='cuda:0')\n",
            "    perplexity on test set: tensor(378.8152) tensor(420.3135) tensor(187.0413) tensor(144.2615)\n",
            "epoch:  17\n",
            "iteration: 0 Loss: tensor(5.0968, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(5.4630, device='cuda:0')\n",
            "perplexity for epoch 18 : tensor(85.2914)  ppx_word:  tensor(100.5367)  ppx_local:  tensor(31.7865)  ppx_only_two:  tensor(25.0197)\n",
            "iteration for evaluate: 0 Loss: tensor(4.9836, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.4038, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.8656, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.6630, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.4197, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.7505, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.2382, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(7.0151, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.6066, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.1841, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(7.1281, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.1315, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.9661, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6751, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.8776, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.3450, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(7.1444, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.4678, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.9200, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(7.0003, device='cuda:0')\n",
            "    perplexity on test set: tensor(378.9089) tensor(413.3292) tensor(206.2876) tensor(147.2858)\n",
            "epoch:  18\n",
            "iteration: 0 Loss: tensor(4.9763, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(5.3746, device='cuda:0')\n",
            "perplexity for epoch 19 : tensor(80.4350)  ppx_word:  tensor(94.6510)  ppx_local:  tensor(30.9874)  ppx_only_two:  tensor(23.4705)\n",
            "iteration for evaluate: 0 Loss: tensor(4.8873, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.3184, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.9050, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.7383, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.4191, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.8027, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.2952, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(7.0047, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.6374, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.2399, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(7.2372, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.1289, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(6.9879, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.6753, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.8995, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.4301, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(7.1586, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.4644, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.9328, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(7.0460, device='cuda:0')\n",
            "    perplexity on test set: tensor(382.9700) tensor(416.2814) tensor(212.1055) tensor(154.3647)\n",
            "epoch:  19\n",
            "iteration: 0 Loss: tensor(4.9077, device='cuda:0')\n",
            "iteration: 50 Loss: tensor(5.3364, device='cuda:0')\n",
            "perplexity for epoch 20 : tensor(75.9180)  ppx_word:  tensor(89.4178)  ppx_local:  tensor(29.0254)  ppx_only_two:  tensor(22.0555)\n",
            "iteration for evaluate: 0 Loss: tensor(4.8399, device='cuda:0')\n",
            "iteration for evaluate: 50 Loss: tensor(5.2785, device='cuda:0')\n",
            "iteration for evaluate: 100 Loss: tensor(6.9045, device='cuda:0')\n",
            "iteration for evaluate: 150 Loss: tensor(7.8034, device='cuda:0')\n",
            "iteration for evaluate: 200 Loss: tensor(7.4657, device='cuda:0')\n",
            "iteration for evaluate: 250 Loss: tensor(6.8919, device='cuda:0')\n",
            "iteration for evaluate: 300 Loss: tensor(7.3630, device='cuda:0')\n",
            "iteration for evaluate: 350 Loss: tensor(7.0374, device='cuda:0')\n",
            "iteration for evaluate: 400 Loss: tensor(7.6588, device='cuda:0')\n",
            "iteration for evaluate: 450 Loss: tensor(7.2971, device='cuda:0')\n",
            "iteration for evaluate: 500 Loss: tensor(7.2315, device='cuda:0')\n",
            "iteration for evaluate: 550 Loss: tensor(7.1866, device='cuda:0')\n",
            "iteration for evaluate: 600 Loss: tensor(7.0135, device='cuda:0')\n",
            "iteration for evaluate: 650 Loss: tensor(6.7026, device='cuda:0')\n",
            "iteration for evaluate: 700 Loss: tensor(6.9072, device='cuda:0')\n",
            "iteration for evaluate: 750 Loss: tensor(7.4517, device='cuda:0')\n",
            "iteration for evaluate: 800 Loss: tensor(7.1594, device='cuda:0')\n",
            "iteration for evaluate: 850 Loss: tensor(7.4881, device='cuda:0')\n",
            "iteration for evaluate: 900 Loss: tensor(6.9299, device='cuda:0')\n",
            "iteration for evaluate: 950 Loss: tensor(7.1246, device='cuda:0')\n",
            "    perplexity on test set: tensor(400.7006) tensor(432.5110) tensor(233.3036) tensor(156.4732)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign cuda GPU located at location '0' to a variable\n",
        "# cuda0 = torch.device('cuda:0')\n",
        "#Performing the addition on GPU\n",
        "a = torch.ones(3, 2, device=device) #creating a tensor 'a' on GPU\n",
        "b = torch.ones(3, 2, device=device) #creating a tensor 'b' on GPU\n",
        "c = a + print(c)"
      ],
      "metadata": {
        "id": "L12xNa1dcO6z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "a5cf6147-24d0-4926-bc28-adac5c85c437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'c' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-36e457f4f75d>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#creating a tensor 'a' on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#creating a tensor 'b' on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_cut = use_cuda(torch.Tensor([0]))\n",
        "local_cut = use_cuda(torch.Tensor([0]))\n",
        "only_two_cut = use_cuda(torch.Tensor([0]))"
      ],
      "metadata": {
        "id": "ji1BjZDw2UjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign cuda GPU located at location '0' to a variable\n",
        "cuda0 = torch.device('cuda:0')\n",
        "#Performing the addition on GPU\n",
        "d = torch.ones(3, 2, device=cuda0) #creating a tensor 'a' on GPU\n",
        "e = torch.ones(3, 2, device=cuda0) #creating a tensor 'b' on GPU\n",
        "f = a + b\n",
        "print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lWAD5R62ty6",
        "outputId": "d18cb0dd-3ea3-4af2-ea66-03ad7d9c6930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2.],\n",
            "        [2., 2.],\n",
            "        [2., 2.]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qplx-3IJ_k1w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}