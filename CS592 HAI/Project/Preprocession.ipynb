{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOM7cwzL9ylTQWj3qnQFdyr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"VaqG1pZOo5wC"},"outputs":[],"source":["def prepare_data(config):\n","    global csk_entities, csk_triples, kb_dict, dict_csk_entities, dict_csk_triples\n","\n","    with open('%s/resource.txt' % config.data_dir) as f:\n","        d = json.loads(f.readline())\n","\n","    csk_triples = d['csk_triples']\n","    csk_entities = d['csk_entities']\n","    raw_vocab = d['vocab_dict']\n","    kb_dict = d['dict_csk']\n","    dict_csk_entities = d['dict_csk_entities']\n","    dict_csk_triples = d['dict_csk_triples']\n","\n","    data_train, data_test = [], []\n","\n","    if config.is_train:\n","        with open('%s/trainset_half_half.txt' % config.data_dir) as f:\n","            for idx, line in enumerate(f):\n","                if idx % 100000 == 0: print('read train file line %d' % idx)\n","                data_train.append(json.loads(line))\n","\n","\n","    with open('%s/testset3.txt' % config.data_dir) as f:\n","        for line in f:\n","            data_test.append(json.loads(line))\n","\n","    return raw_vocab, data_train, data_test\n","\n","def build_vocab(path, raw_vocab, config, trans='transE'):\n","\n","    print(\"Creating word vocabulary...\")\n","    vocab_list = ['_PAD','_GO', '_EOS', '_UNK', ] + sorted(raw_vocab, key=raw_vocab.get, reverse=True)\n","    if len(vocab_list) > config.symbols:\n","        vocab_list = vocab_list[:config.symbols]\n","\n","    print(\"Creating entity vocabulary...\")\n","    entity_list = ['_NONE', '_PAD_H', '_PAD_R', '_PAD_T', '_NAF_H', '_NAF_R', '_NAF_T']\n","    with open('%s/entity.txt' % path) as f:\n","        for i, line in enumerate(f):\n","            e = line.strip()\n","            entity_list.append(e)\n","\n","    print(\"Creating relation vocabulary...\")\n","    relation_list = []\n","    with open('%s/relation.txt' % path) as f:\n","        for i, line in enumerate(f):\n","            r = line.strip()\n","            relation_list.append(r)\n","\n","    print(\"Loading word vectors...\")\n","    vectors = {}\n","    with open('%s/glove.840B.300d.txt' % path, encoding='utf-8') as f:\n","        for i, line in enumerate(f):\n","            if i % 100000 == 0:\n","                print(\"    processing line %d\" % i)\n","            s = line.strip()\n","            word = s[:s.find(' ')]\n","            vector = s[s.find(' ')+1:]\n","            vectors[word] = vector\n","\n","    embed = []\n","    for word in vocab_list:\n","        if word in vectors:\n","            #vector = map(float, vectors[word].split())\n","            vector = vectors[word].split()\n","        else:\n","            vector = np.zeros((config.embed_units), dtype=np.float32)\n","        embed.append(vector)\n","    embed = np.array(embed, dtype=np.float32)\n","\n","    print(\"Loading entity vectors...\")\n","    entity_embed = []\n","    with open('%s/entity_%s.txt' % (path, trans)) as f:\n","        for i, line in enumerate(f):\n","            s = line.strip().split('\\t')\n","            #entity_embed.append(map(float, s))\n","            entity_embed.append(s)\n","\n","    print(\"Loading relation vectors...\")\n","    relation_embed = []\n","    with open('%s/relation_%s.txt' % (path, trans)) as f:\n","        for i, line in enumerate(f):\n","            s = line.strip().split('\\t')\n","            relation_embed.append(s)\n","\n","    entity_relation_embed = np.array(entity_embed+relation_embed, dtype=np.float32)\n","    entity_embed = np.array(entity_embed, dtype=np.float32)\n","    relation_embed = np.array(relation_embed, dtype=np.float32)\n","\n","    word2id = dict()\n","    entity2id = dict()\n","    for word in vocab_list:\n","        word2id[word] = len(word2id)\n","    for entity in entity_list + relation_list:\n","        entity2id[entity] = len(entity2id)\n","\n","    return word2id, entity2id, vocab_list, embed, entity_list, entity_embed, relation_list, relation_embed, entity_relation_embed\n","\n","def gen_batched_data(data, config, word2id, entity2id):\n","    global csk_entities, csk_triples, kb_dict, dict_csk_entities, dict_csk_triples\n","\n","    encoder_len = max([len(item['post']) for item in data])+1\n","\n","    decoder_len = max([len(item['response']) for item in data])+1\n","    triple_num = max([len(item['all_triples_one_hop']) for item in data])\n","    entity_len = max([len(item['all_entities_one_hop']) + max(item['post_triples']) for item in data])\n","    only_two_entity_len = max([len(item['only_two']) for item in data])\n","    triple_num_one_two = max([len(item['one_two_triple']) for item in data])\n","    triple_len_one_two = max([len(tri) for item in data for tri in item['one_two_triple']])\n","    posts_id = np.full((len(data), encoder_len), 0, dtype=int)\n","    responses_id = np.full((len(data), decoder_len), 0, dtype=int)\n","    responses_length = []\n","    # posts_length = []\n","    local_entity_length = []\n","    only_two_entity_length = []\n","    local_entity = []\n","    only_two_entity = []\n","    kb_fact_rels = np.full((len(data), triple_num), 2, dtype=int)\n","    kb_adj_mats = np.empty(len(data), dtype=object)\n","    q2e_adj_mats = np.full((len(data), entity_len), 0, dtype=int)\n","    match_entity_one_hop = np.full((len(data), decoder_len), -1, dtype=int)\n","    match_entity_only_two = np.full((len(data), decoder_len), -1, dtype=int)\n","    one_two_triples_id = []\n","    g2l_only_two_list = []\n","    # o2t_entity_index_list = []\n","\n","    next_id = 0\n","    for item in data:\n","        # posts\n","        for i, post_word in enumerate(padding(item['post'], encoder_len)):\n","            if post_word in word2id:\n","                posts_id[next_id, i] = word2id[post_word]\n","\n","            else:\n","                posts_id[next_id, i] = word2id['_UNK']\n","\n","        # responses\n","        for i, response_word in enumerate(padding(item['response'], decoder_len)):\n","            if response_word in word2id:\n","                responses_id[next_id, i] = word2id[response_word]\n","\n","            else:\n","                responses_id[next_id, i] = word2id['_UNK']\n","\n","        # responses_length\n","        responses_length.append(len(item['response']) + 1)\n","\n","        # local_entity\n","        local_entity_tmp = []\n","        for i in range(len(item['post_triples'])):\n","            if item['post_triples'][i] == 0:\n","                continue\n","            elif item['post'][i] not in entity2id:\n","                continue\n","            elif entity2id[item['post'][i]] in local_entity_tmp:\n","                continue\n","            else:\n","                local_entity_tmp.append(entity2id[item['post'][i]])\n","\n","        for entity_index in item['all_entities_one_hop']:\n","            if csk_entities[entity_index] not in entity2id:\n","                continue\n","            if entity2id[csk_entities[entity_index]] in local_entity_tmp:\n","                continue\n","            else:\n","                local_entity_tmp.append(entity2id[csk_entities[entity_index]])\n","        local_entity_len_tmp = len(local_entity_tmp)\n","        local_entity_tmp += [1] * (entity_len - len(local_entity_tmp))\n","        local_entity.append(local_entity_tmp)\n","\n","        # kb_adj_mat and kb_fact_rel\n","        g2l = dict()\n","        for i in range(len(local_entity_tmp)):\n","            g2l[local_entity_tmp[i]] = i\n","\n","        entity2fact_e, entity2fact_f = [], []\n","        fact2entity_f, fact2entity_e = [], []\n","\n","        tmp_count = 0\n","        for i in range(len(item['all_triples_one_hop'])):\n","            sbj = csk_triples[item['all_triples_one_hop'][i]].split()[0][:-1]\n","            rel = csk_triples[item['all_triples_one_hop'][i]].split()[1][:-1]\n","            obj = csk_triples[item['all_triples_one_hop'][i]].split()[2]\n","\n","            if (sbj not in entity2id) or (obj not in entity2id):\n","                continue\n","            if (entity2id[sbj] not in g2l) or (entity2id[obj] not in g2l):\n","                continue\n","\n","            entity2fact_e += [g2l[entity2id[sbj]]]\n","            entity2fact_f += [tmp_count]\n","            fact2entity_f += [tmp_count]\n","            fact2entity_e += [g2l[entity2id[obj]]]\n","            kb_fact_rels[next_id, tmp_count] = entity2id[rel]\n","            tmp_count += 1\n","\n","        kb_adj_mats[next_id] = (np.array(entity2fact_f, dtype=int), np.array(entity2fact_e, dtype=int), np.array([1.0] * len(entity2fact_f))), (np.array(fact2entity_e, dtype=int), np.array(fact2entity_f, dtype=int), np.array([1.0] * len(fact2entity_e)))\n","\n","        # q2e_adj_mat\n","        for i in range(len(item['post_triples'])):\n","            if item['post_triples'][i] == 0:\n","                continue\n","            elif item['post'][i] not in entity2id:\n","                continue\n","            else:\n","                q2e_adj_mats[next_id, g2l[entity2id[item['post'][i]]]] = 1\n","\n","        # match_entity_one_hop\n","        for i in range(len(item['match_response_index_one_hop'])):\n","            if item['match_response_index_one_hop'][i] == -1:\n","                continue\n","            if csk_entities[item['match_response_index_one_hop'][i]] not in entity2id:\n","                continue\n","            if entity2id[csk_entities[item['match_response_index_one_hop'][i]]] not in g2l:\n","                continue\n","            else:\n","                match_entity_one_hop[next_id, i] = g2l[entity2id[csk_entities[item['match_response_index_one_hop'][i]]]]\n","\n","        # only_two_entity\n","        only_two_entity_tmp = []\n","        for entity_index in item['only_two']:\n","            if csk_entities[entity_index] not in entity2id:\n","                continue\n","            if entity2id[csk_entities[entity_index]] in only_two_entity_tmp:\n","                continue\n","            else:\n","                only_two_entity_tmp.append(entity2id[csk_entities[entity_index]])\n","        only_two_entity_len_tmp = len(only_two_entity_tmp)\n","        only_two_entity_tmp += [1] * (only_two_entity_len - len(only_two_entity_tmp))\n","        only_two_entity.append(only_two_entity_tmp)\n","\n","        # match_entity_two_hop\n","        g2l_only_two = dict()\n","        for i in range(len(only_two_entity_tmp)):\n","            g2l_only_two[only_two_entity_tmp[i]] = i\n","\n","        for i in range(len(item['match_response_index_only_two'])):\n","            if item['match_response_index_only_two'][i] == -1:\n","                continue\n","            if csk_entities[item['match_response_index_only_two'][i]] not in entity2id:\n","                continue\n","            else:\n","                match_entity_only_two[next_id, i] = g2l_only_two[entity2id[csk_entities[item['match_response_index_only_two'][i]]]]\n","\n","        # one_two_triple\n","        one_two_triples_id.append(padding_triple_id(entity2id, [[csk_triples[x].split(', ') for x in triple] for triple in item['one_two_triple']], triple_num_one_two, triple_len_one_two))\n","\n","        ############################ g2l_only_two\n","        g2l_only_two_list.append(g2l_only_two)\n","\n","        # local_entity_length\n","        local_entity_length.append(local_entity_len_tmp)\n","\n","        # only_two_entity_length\n","        only_two_entity_length.append(only_two_entity_len_tmp)\n","\n","        next_id += 1\n","\n","    batched_data = {'query_text': np.array(posts_id),\n","            'answer_text': np.array(responses_id),\n","            'local_entity': np.array(local_entity),\n","            'responses_length': responses_length,\n","            'q2e_adj_mat': np.array(q2e_adj_mats),\n","            'kb_adj_mat': build_kb_adj_mat(kb_adj_mats, config.fact_dropout),\n","            'kb_fact_rel': np.array(kb_fact_rels),\n","            'match_entity_one_hop': np.array(match_entity_one_hop),\n","            'only_two_entity': np.array(only_two_entity),\n","            'match_entity_only_two': np.array(match_entity_only_two),\n","            'one_two_triples_id': np.array(one_two_triples_id),\n","            'word2id': word2id,\n","            'entity2id': entity2id,\n","            'local_entity_length': local_entity_length,\n","            'only_two_entity_length': only_two_entity_length}\n","\n","    return batched_data"]}]}