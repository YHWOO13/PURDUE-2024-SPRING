{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1IwzaiXdwF3k9yioETTMO366EMJwoqb3u","authorship_tag":"ABX9TyOtXDCJRnhNGTtMu6OsED5M"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["class CentralEncoder(nn.Module):\n","  def __init__(self, config, gnn_layers, embed_units, trans_units, word_embedding, entity_embedding):\n","    super(CentralEncoder, self).__init__()\n","    self.k = 2 + 1\n","    self.gnn_layers = gnn_layers # 3\n","    self.WordEmbedding = word_embedding\n","    self.EntityEmbedding = entity_embedding\n","    self.embed_units = embed_units # 300\n","    self.trans_units = trans_units # 100\n","    self.pagerank_lambda = config.pagerank_lambda # 0.8\n","    self.fact_scale = config.fact_scale # 1\n","\n","    self.node_encoder = nn.LSTM(input_size = self.embed_units, hidden_size = self.trans_units, batch_first=True, bidirectional=False)\n","    self.lstm_drop = nn.Dropout(p = config.lstm_dropout)\n","    self.softmax_d1 = nn.Softmax(dim = 1)\n","    self.linear_drop = nn.Dropout(p = config.linear_dropout)\n","    self.relu = nn.ReLU()\n","\n","    for i in range(self.gnn_layers):\n","        self.add_module('q2e_linear' + str(i), nn.Linear(in_features=self.trans_units, out_features=self.trans_units))\n","        self.add_module('d2e_linear' + str(i), nn.Linear(in_features=self.trans_units, out_features=self.trans_units))\n","        self.add_module('e2q_linear' + str(i), nn.Linear(in_features=self.k * self.trans_units, out_features=self.trans_units))\n","        self.add_module('e2d_linear' + str(i), nn.Linear(in_features=self.k * self.trans_units, out_features=self.trans_units))\n","        self.add_module('e2e_linear' + str(i), nn.Linear(in_features=self.k * self.trans_units, out_features=self.trans_units))\n","\n","        #use kb\n","        self.add_module('kb_head_linear' + str(i), nn.Linear(in_features=self.trans_units, out_features=self.trans_units))\n","        self.add_module('kb_tail_linear' + str(i), nn.Linear(in_features=self.trans_units, out_features=self.trans_units))\n","        self.add_module('kb_self_linear' + str(i), nn.Linear(in_features=self.trans_units, out_features=self.trans_units))\n","\n","  def forward(self, batch_size, max_local_entity, max_fact, query_text, local_entity, q2e_adj_mat, kb_adj_mat, kb_fact_rel, query_mask):\n","    # normalized adj matrix\n","    pagerank_f = Variable(torch.from_numpy(q2e_adj_mat).type('torch.FloatTensor'), requires_grad=True).to(device) # use_cuda(Variable(torch.from_numpy(q2e_adj_mat).type('torch.FloatTensor'), requires_grad=True))\n","    q2e_adj_mat = Variable(torch.from_numpy(q2e_adj_mat).type('torch.FloatTensor'), requires_grad=False).to(device) # use_cuda\n","    assert pagerank_f.requires_grad == True\n","\n","    # encode query\n","    query_word_emb = self.WordEmbedding(query_text)\n","    query_hidden_emb, (query_node_emb, _) = self.node_encoder(self.lstm_drop(query_word_emb), self.init_hidden(1, batch_size, self.trans_units))\n","    query_node_emb = query_node_emb.squeeze(dim=0).unsqueeze(dim=1)\n","    query_rel_emb = query_node_emb\n","\n","    # build kb_adj_matrix from sparse matrix\n","    (e2f_batch, e2f_f, e2f_e, e2f_val), (f2e_batch, f2e_e, f2e_f, f2e_val) = kb_adj_mat\n","    entity2fact_index = torch.LongTensor([e2f_batch, e2f_f, e2f_e]).to(device)\n","    entity2fact_val = torch.FloatTensor(e2f_val).to(device)\n","    entity2fact_mat = use_cuda(torch.sparse.FloatTensor(entity2fact_index, entity2fact_val, torch.Size([batch_size, max_fact, max_local_entity])))#       entity2fact_mat = use_cuda(torch.sparse.FloatTensor(entity2fact_index, entity2fact_val, torch.Size([batch_size, max_fact, max_local_entity])))\n","\n","    fact2entity_index = torch.LongTensor([f2e_batch, f2e_e, f2e_f])\n","    fact2entity_val = torch.FloatTensor(f2e_val)\n","    fact2entity_mat = use_cuda(torch.sparse.FloatTensor(fact2entity_index, fact2entity_val, torch.Size([batch_size, max_local_entity, max_fact])))\n","\n","    local_fact_emb = self.EntityEmbedding(kb_fact_rel)\n","    local_fact_emb= local_fact_emb.to(device=device)\n","    # attention fact2question\n","    div = float(np.sqrt(self.trans_units))\n","    fact2query_sim = torch.bmm(query_hidden_emb, local_fact_emb.transpose(1, 2)) / div\n","    fact2query_sim = self.softmax_d1(fact2query_sim + (1 - query_mask.unsqueeze(dim=2)) * VERY_NEG_NUMBER)\n","\n","    fact2query_att = torch.sum(fact2query_sim.unsqueeze(dim=3) * query_hidden_emb.unsqueeze(dim=2), dim=1)\n","\n","    W = torch.sum(fact2query_att * local_fact_emb, dim=2) / div\n","    W_max = torch.max(W, dim=1, keepdim=True)[0]\n","    W_tilde = torch.exp(W - W_max)\n","    e2f_softmax = self.sparse_bmm(entity2fact_mat.transpose(1, 2), W_tilde.unsqueeze(dim=2)).squeeze(dim=2)\n","    e2f_softmax = torch.clamp(e2f_softmax, min=VERY_SMALL_NUMBER)\n","    e2f_out_dim = use_cuda(Variable(torch.sum(entity2fact_mat.to_dense(), dim=1), requires_grad=False))\n","\n","    # load entity embedding\n","    local_entity_emb = self.EntityEmbedding(local_entity)\n","\n","    # label propagation on entities\n","    for i in range(self.gnn_layers):\n","        # get linear transformation functions for each layer\n","        q2e_linear = getattr(self, 'q2e_linear' + str(i))\n","        d2e_linear = getattr(self, 'd2e_linear' + str(i))\n","        e2q_linear = getattr(self, 'e2q_linear' + str(i))\n","        e2d_linear = getattr(self, 'e2d_linear' + str(i))\n","        e2e_linear = getattr(self, 'e2e_linear' + str(i))\n","\n","        kb_self_linear = getattr(self, 'kb_self_linear' + str(i))\n","        kb_head_linear = getattr(self, 'kb_head_linear' + str(i))\n","        kb_tail_linear = getattr(self, 'kb_tail_linear' + str(i))\n","\n","        # start propagation\n","        next_local_entity_emb = local_entity_emb\n","\n","        # STEP 1: propagate from question, documents, and facts to entities\n","        # question -> entity\n","        q2e_emb = q2e_linear(self.linear_drop(query_node_emb)).expand(batch_size, max_local_entity, self.trans_units)\n","        next_local_entity_emb = torch.cat((next_local_entity_emb, q2e_emb), dim=2)\n","\n","        # fact -> entity\n","        e2f_emb = self.relu(kb_self_linear(local_fact_emb) + self.sparse_bmm(entity2fact_mat, kb_head_linear(self.linear_drop(local_entity_emb))))\n","        e2f_softmax_normalized = W_tilde.unsqueeze(dim=2) * self.sparse_bmm(entity2fact_mat, (pagerank_f / e2f_softmax).unsqueeze(dim=2))\n","        e2f_emb = e2f_emb * e2f_softmax_normalized\n","        f2e_emb = self.relu(kb_self_linear(local_entity_emb) + self.sparse_bmm(fact2entity_mat, kb_tail_linear(self.linear_drop(e2f_emb))))\n","\n","        pagerank_f = self.pagerank_lambda * self.sparse_bmm(fact2entity_mat, e2f_softmax_normalized).squeeze(dim=2) + (1 - self.pagerank_lambda) * pagerank_f\n","\n","        # STEP 2: combine embeddings from fact\n","        next_local_entity_emb = torch.cat((next_local_entity_emb, self.fact_scale * f2e_emb), dim=2)\n","\n","        # STEP 3: propagate from entities to update question, documents, and facts\n","        # entity -> query\n","        query_node_emb = torch.bmm(pagerank_f.unsqueeze(dim=1), e2q_linear(self.linear_drop(next_local_entity_emb)))\n","        # update entity\n","        local_entity_emb = self.relu(e2e_linear(self.linear_drop(next_local_entity_emb)))\n","\n","    return local_entity_emb\n","\n","  def init_hidden(self, num_layer, batch_size, hidden_size):\n","    return (use_cuda(Variable(torch.zeros(num_layer, batch_size, hidden_size))),\n","              use_cuda(Variable(torch.zeros(num_layer, batch_size, hidden_size))))\n","\n","  def sparse_bmm(self, X, Y):\n","    \"\"\"Batch multiply X and Y where X is sparse, Y is dense.\n","    Args:\n","        X: Sparse tensor of size BxMxN. Consists of two tensors,\n","            I:3xZ indices, and V:1xZ values.\n","        Y: Dense tensor of size BxNxK.\n","    Returns:\n","        batched-matmul(X, Y): BxMxK\n","    \"\"\"\n","\n","    class LeftMMFixed(torch.autograd.Function):\n","        \"\"\"\n","        Implementation of matrix multiplication of a Sparse Variable with a Dense Variable, returning a Dense one.\n","        This is added because there's no autograd for sparse yet. No gradient computed on the sparse weights.\n","        \"\"\"\n","\n","        @staticmethod\n","        def forward(ctx, sparse_weights, x):\n","            ctx.save_for_backward(sparse_weights)\n","            return torch.mm(sparse_weights, x)\n","\n","        @staticmethod\n","        def backward(ctx, grad_output):\n","            sparse_weights, = ctx.saved_tensors\n","            return None, torch.mm(sparse_weights.t(), grad_output)\n","\n","    def sparse_mm_fixed(sparse_weights, x):\n","        return LeftMMFixed.apply(sparse_weights, x)\n","\n","    I = X._indices()\n","    V = X._values()\n","    B, M, N = X.size()\n","    _, _, K = Y.size()\n","    Z = I.size()[1]\n","    lookup = Y[I[0, :], I[2, :], :]\n","    X_I = torch.stack((I[0, :] * M + I[1, :], torch.arange(Z, device=X.device, dtype=torch.long)), 0)\n","    S = torch.sparse.FloatTensor(X_I, V, torch.Size([B * M, Z])).to(device=X.device)\n","    prod = sparse_mm_fixed(S, lookup)\n","    return prod.view(B, M, K)"],"metadata":{"id":"XWBpw1TnbNyp"},"execution_count":null,"outputs":[]}]}