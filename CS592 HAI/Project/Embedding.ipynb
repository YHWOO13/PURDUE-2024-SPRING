{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNooSWcYnbLHCIlhTBiFxLC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"boTBPCPeomMi"},"outputs":[],"source":["# #coding:utf-8\n","# import torch\n","# import numpy as np\n","# from torch.autograd import Variable\n","# import torch.nn as nn\n","# from torch.nn import utils as nn_utils\n","\n","VERY_SMALL_NUMBER = 1e-10\n","VERY_NEG_NUMBER = -100000000000\n","\n","def use_cuda(var):\n","    if torch.cuda.is_available():\n","        var = var.to(device=device)\n","        return var\n","    else:\n","        return var\n","'''\n","def use_cuda(var):\n","    if torch.cuda.is_available():\n","        return var.cuda()\n","    else:\n","        return var\n","'''\n","\n","\n","class EntityEmbedding(nn.Module):\n","  def __init__(self, entity_embed, trans_units):\n","    super(EntityEmbedding, self).__init__()\n","    self.trans_units = trans_units\n","    self.entity_embedding = nn.Embedding(num_embeddings = entity_embed.shape[0] + 7, embedding_dim = self.trans_units, padding_idx = 0)\n","    entity_embed = torch.tensor(entity_embed, device=device)\n","\n","    entity_embed = torch.cat((torch.zeros(7, self.trans_units, device=device), entity_embed), 0)\n","    self.entity_embedding.weight = nn.Parameter(torch.tensor(entity_embed, device=device))\n","    self.entity_embedding.weight.requires_grad = True\n","    self.entity_linear = nn.Linear(in_features = self.trans_units, out_features = self.trans_units)\n","\n","  def forward(self, entity):\n","    entity_emb = self.entity_embedding(entity)\n","    entity_emb = self.entity_linear(entity_emb)\n","    return entity_emb\n","\n","\n","\n","class WordEmbedding(nn.Module):\n","    def __init__(self, word_embed, embed_units):\n","      super(WordEmbedding, self).__init__()\n","\n","      self.embed_units = embed_units\n","      self.word_embedding = nn.Embedding(num_embeddings = word_embed.shape[0], embedding_dim = self.embed_units, padding_idx = 0)\n","      self.word_embedding.weight = nn.Parameter(torch.tensor(word_embed, device=device))\n","      self.word_embedding.weight.requires_grad = True\n","\n","    def forward(self, query_text):\n","      return self.word_embedding(query_text)"]}]}