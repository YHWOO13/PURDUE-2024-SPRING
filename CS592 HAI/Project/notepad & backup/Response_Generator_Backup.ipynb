{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25202,
     "status": "ok",
     "timestamp": 1713337033090,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "fpfbjtB_cyb-",
    "outputId": "f3a6793f-368e-4fe8-ec6b-053c1852636b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3282,
     "status": "ok",
     "timestamp": 1713337046497,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "xB_2nqyZbg1e"
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import numpy as np\n",
    "import json\n",
    "# from model import ConceptFlow, use_cuda\n",
    "# from preprocession import prepare_data, build_vocab, gen_batched_data\n",
    "import torch\n",
    "import warnings\n",
    "import yaml\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import utils as nn_utils\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"D:\\\\13.PURDUE\\\\Data\\\\ConceptFlow(ECCF)_data\"\n",
    "with open('%s/resource.txt' % data_dir) as f:\n",
    "    d = json.loads(f.readline())\n",
    "\n",
    "    csk_triples = d['csk_triples']\n",
    "    csk_entities = d['csk_entities']\n",
    "    raw_vocab = d['vocab_dict']\n",
    "    kb_dict = d['dict_csk']\n",
    "    dict_csk_entities = d['dict_csk_entities']\n",
    "    dict_csk_triples = d['dict_csk_triples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New post preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities set()\n",
      "triples []\n",
      "entities {'ŒµŒπŒΩŒ±Œπ', 'ŸÖÿß_Ÿáÿ∞ÿß', '„Éâ„Çπ', 'wast', 'has', 'oes', 'bes', '–µ', '„Åß„Åí„Åô', 'inar', '„Åñ„Çä', '„Åß„Åô', 'm√°r_k√©s≈ë_van', \"where's\", 'isness', '–∑–∞—Ä–µ–º', '„ÅÑ„Çâ„Å£„Åó„ÇÉ„Çã', 'je', 'ada_apa', 'mura', 'er', 'e', 'c√©rb', 'nƒ©', 'es', 'found_in_subway', 'Œ±ŒªŒ∑Œ∏œéœÉ_Œ±ŒΩŒ≠œÉœÑŒ∑', 'id', 'sta', 'qu_est_ce_que_c_est', '„Å£„Åô', \"she'sn't\", \"who's\", 'ŸäŸÉŸàŸÜ', '„Å†', '„ÅßÂæ°Â∫ß„Çä„Åæ„Åô', '„Å©„Åô', 'is', 'expensive', 'ei_ole', 'v', '√©', 'tr√≠narb', 'ialah', 'est√°', 'a', 'en_1', 'bilgisayarsa', '3', 'êåπêçÉêçÑ', '„ÅßÂæ°Â∫ß„ÅÑ„Åæ„Åô', 'beest', 'r', 'infinite_stratos', 'n', 'tabansƒ±zsa', 'ê¨Äê¨Øê¨ôê¨å', '„Åò„ÇÉ', 'ist', '„Å£„Å¶„ÅÆ„ÅØ', '—î—Å—Ç—ä', 'ÊòØ', '—î', '√≥nar', 'evse', 'th√¨', 'isentropic', 'being', 'jeste', '„Å®Ë®Ä„ÅÜ„ÅÆ„ÅØ', 'adalah', '√°t º√©ii', 'yw', 'IS_(Infinite_Stratos)', 'your+name+is+mud-p#Component-3', 'jest', '„Åß„Åî„Çì„Åô', 'beeth', '„Åì„Çå„ÅØ„Å™„Çì„Åß„Åô„Åã', 'dar', 'computing', \"it's\"}\n",
      "triples [('„Å©„Åô', 'Synonym', 'is'), ('n', 'Synonym', 'is'), ('en_1', 'RelatedTo', 'is'), ('„Åñ„Çä', 'Synonym', 'is'), ('is', 'Synonym', 'jest'), ('„Éâ„Çπ', 'Synonym', 'is'), ('„ÅÑ„Çâ„Å£„Åó„ÇÉ„Çã', 'Synonym', 'is'), ('is', 'Synonym', 'jeste'), ('v', 'Synonym', 'is'), ('„Å£„Å¶„ÅÆ„ÅØ', 'Synonym', 'is'), ('„Åß„Åî„Çì„Åô', 'Synonym', 'is'), ('„Å†', 'Synonym', 'is'), ('3', 'Synonym', 'is'), ('„Åß„Åí„Åô', 'Synonym', 'is'), ('„Å£„Åô', 'Synonym', 'is'), ('„ÅßÂæ°Â∫ß„Çä„Åæ„Åô', 'Synonym', 'is'), ('is', 'Synonym', 'je'), ('„ÅßÂæ°Â∫ß„ÅÑ„Åæ„Åô', 'Synonym', 'is'), ('„Åß„Åô', 'Synonym', 'is'), ('n', 'HasContext', 'computing'), ('„Å®Ë®Ä„ÅÜ„ÅÆ„ÅØ', 'Synonym', 'is'), ('v', 'Synonym', 'is'), ('is', 'Synonym', 'ÊòØ'), ('√°t º√©ii', 'RelatedTo', 'is'), ('bilgisayarsa', 'RelatedTo', 'is'), ('is', 'Synonym', 'je'), ('is', 'Synonym', 'êåπêçÉêçÑ'), ('m√°r_k√©s≈ë_van', 'RelatedTo', 'is'), ('is', 'DerivedFrom', 'n'), ('is', 'Synonym', 'th√¨'), ('ei_ole', 'RelatedTo', 'is'), ('„Åì„Çå„ÅØ„Å™„Çì„Åß„Åô„Åã', 'RelatedTo', 'is'), ('beeth', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('is', 'Synonym', 'oes'), (\"where's\", 'RelatedTo', 'is'), ('r', 'RelatedTo', 'is'), ('is', 'CapableOf', 'found_in_subway'), ('–∑–∞—Ä–µ–º', 'RelatedTo', 'is'), ('r', 'RelatedTo', 'is'), ('√≥nar', 'RelatedTo', 'is'), ('wast', 'RelatedTo', 'is'), ('is', 'Synonym', 'es'), ('is', 'Synonym', 'is'), ('tabansƒ±zsa', 'RelatedTo', 'is'), ('isentropic', 'DerivedFrom', 'is'), (\"it's\", 'RelatedTo', 'is'), ('is', 'Synonym', 'bes'), ('ialah', 'RelatedTo', 'is'), ('is', 'Synonym', 'ŸäŸÉŸàŸÜ'), ('v', 'RelatedTo', 'is'), ('is', 'Synonym', '„Åò„ÇÉ'), (\"she'sn't\", 'RelatedTo', 'is'), ('infinite_stratos', 'ExternalURL', 'IS_(Infinite_Stratos)'), ('a', 'RelatedTo', 'is'), ('ada_apa', 'RelatedTo', 'is'), ('n', 'RelatedTo', 'is'), ('is', 'Synonym', '–µ'), ('is', 'Synonym', 'est√°'), ('tr√≠narb', 'RelatedTo', 'is'), ('id', 'Synonym', 'is'), ('evse', 'RelatedTo', 'is'), ('is', 'Synonym', 'e'), ('is', 'Synonym', 'er'), ('being', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('dar', 'RelatedTo', 'is'), ('is', 'Synonym', 'ŒµŒπŒΩŒ±Œπ'), ('inar', 'RelatedTo', 'is'), ('is', 'Synonym', 'ist'), ('isness', 'DerivedFrom', 'is'), ('mura', 'RelatedTo', 'is'), ('ÊòØ', 'Synonym', 'is'), ('Œ±ŒªŒ∑Œ∏œéœÉ_Œ±ŒΩŒ≠œÉœÑŒ∑', 'RelatedTo', 'is'), ('is', 'Synonym', 'yw'), ('is', 'Synonym', 'is'), ('ŸÖÿß_Ÿáÿ∞ÿß', 'RelatedTo', 'is'), ('is', 'ExternalURL', 'your+name+is+mud-p#Component-3'), ('v', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), (\"who's\", 'RelatedTo', 'is'), ('beest', 'RelatedTo', 'is'), ('is', 'Synonym', 'ê¨Äê¨Øê¨ôê¨å'), ('adalah', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('is', 'Synonym', '—î'), ('is', 'CapableOf', 'expensive'), ('is', 'Synonym', '√©'), ('v', 'RelatedTo', 'is'), ('is', 'Synonym', 'sta'), ('has', 'SimilarTo', 'is'), ('is', 'Synonym', '—î—Å—Ç—ä'), ('nƒ©', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('qu_est_ce_que_c_est', 'RelatedTo', 'is'), ('v', 'Synonym', 'is'), ('c√©rb', 'RelatedTo', 'is')]\n",
      "entities {'related', 'Áõ∏ÂÖ≥', 'family', 'anthropology', 'connexe', 'unrelated', 'linguistics', 'thunder', 'n', 'wn', 'relative', 'structure_and_strategy', 'ÊúâÁ∏Å', 'a', 'v', 'interrelated', 'nectendus'}\n",
      "triples [('family', 'RelatedTo', 'related'), ('anthropology', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'RelatedTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('ÊúâÁ∏Å', 'Synonym', 'related'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('a', 'RelatedTo', 'related'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'RelatedTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('n', 'Synonym', 'related'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('n', 'Synonym', 'related'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'linguistics'), ('wn', 'SimilarTo', 'wn'), ('wn', 'RelatedTo', 'wn'), ('linguistics', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'anthropology'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'RelatedTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('a', 'RelatedTo', 'related'), ('n', 'Synonym', 'related'), ('wn', 'Synonym', 'wn'), ('thunder', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'related'), ('v', 'RelatedTo', 'related'), ('interrelated', 'DerivedFrom', 'related'), ('a', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'connexe'), ('a', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'related'), ('unrelated', 'DerivedFrom', 'related'), ('a', 'RelatedTo', 'related'), ('nectendus', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'related'), ('v', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'relative'), ('n', 'RelatedTo', 'related'), ('Áõ∏ÂÖ≥', 'Synonym', 'related'), ('structure_and_strategy', 'ReceivesAction', 'related'), ('a', 'RelatedTo', 'related'), ('n', 'RelatedTo', 'related'), ('n', 'RelatedTo', 'related')]\n",
      "entities {'cis', '„Å´Âèñ„Å£„Å¶', '‰πó„Åõ„Çã', 't√´', 'til', 'go', 'te', 'Âæπ„Åô', 'put', 'into', 'naartoe', 'a', 'un', '·Éõ·Éì·Éî', 'tu', 'unto', 'open', 'stop_standing_in', 'abys', '‡πÉ‡∏´‡πâ', 'gra»õie', 'kepada', '„Å´ÂØæ„Åô„Çã', '1', 'do', 'hitherto', 'en_2', 'na', 'ËøÑ', '⁄©Ÿà', 'Ëºâ„Åõ„Çã', 'r', 'ÂØæ', 'n', 'ËøÑ„ÇÇ', 'ad', 'enjoy_company_of_friends', \"so's\", 'thitherto', '’°’º', 'be', 'da', 'u', 'ok', '‡§∏‡•á', 'bread', 'apud', 'usque', '„Å´„Åã„Åë„Å¶', 'onto', 'za', 'ÈÄö„Åô', '„Å´Âèñ„Çä„Åæ„Åó„Å¶', 'ÈÄè„Åô', 'prema', 'k', 'en_1', 'come', 'to', 'v', 'other+fish+to+fry-p#Component-3'}\n",
      "triples [('„Å´Âèñ„Å£„Å¶', 'Synonym', 'to'), ('ad', 'RelatedTo', 'to'), ('da', 'RelatedTo', 'to'), ('Âæπ„Åô', 'Synonym', 'to'), ('’°’º', 'RelatedTo', 'to'), ('put', 'RelatedTo', 'to'), ('onto', 'DerivedFrom', 'to'), ('a', 'RelatedTo', 'to'), ('za', 'RelatedTo', 'to'), ('„Å´Âèñ„Çä„Åæ„Åó„Å¶', 'Synonym', 'to'), ('come', 'DerivedFrom', 'to'), ('ËøÑ„ÇÇ', 'Synonym', 'to'), ('v', 'Synonym', 'to'), ('„Å´„Åã„Åë„Å¶', 'Synonym', 'to'), ('v', 'RelatedTo', 'to'), ('ÈÄö„Åô', 'Synonym', 'to'), ('ÈÄè„Åô', 'Synonym', 'to'), ('v', 'Synonym', 'to'), ('k', 'RelatedTo', 'to'), ('n', 'RelatedTo', 'to'), ('to', 'EtymologicallyDerivedFrom', 'to'), ('‰πó„Åõ„Çã', 'Synonym', 'to'), ('do', 'DerivedFrom', 'to'), ('to', 'Synonym', 'a'), ('be', 'DerivedFrom', 'to'), ('bread', 'DerivedFrom', 'to'), ('un', 'RelatedTo', 'to'), ('to', 'EtymologicallyDerivedFrom', 'to'), ('til', 'RelatedTo', 'to'), ('v', 'Synonym', 'to'), ('na', 'RelatedTo', 'to'), ('ad', 'RelatedTo', 'to'), ('ÂØæ', 'Synonym', 'to'), ('unto', 'DerivedFrom', 'to'), ('ad', 'RelatedTo', 'to'), ('do', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('prema', 'RelatedTo', 'to'), ('u', 'RelatedTo', 'to'), ('ËøÑ', 'Synonym', 'to'), ('1', 'Synonym', 'to'), ('v', 'Synonym', 'to'), ('into', 'DerivedFrom', 'to'), ('1', 'Synonym', 'to'), ('hitherto', 'DerivedFrom', 'to'), ('„Å´ÂØæ„Åô„Çã', 'Synonym', 'to'), ('Ëºâ„Åõ„Çã', 'Synonym', 'to'), ('·Éõ·Éì·Éî', 'RelatedTo', 'to'), ('to', 'Synonym', 'tu'), ('v', 'RelatedTo', 'to'), ('n', 'RelatedTo', 'to'), ('thitherto', 'DerivedFrom', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('n', 'RelatedTo', 'to'), ('‡§∏‡•á', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('a', 'RelatedTo', 'to'), ('abys', 'Synonym', 'to'), ('to', 'ExternalURL', 'other+fish+to+fry-p#Component-3'), ('to', 'Synonym', 't√´'), ('n', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('n', 'HasContext', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('gra»õie', 'RelatedTo', 'to'), ('tu', 'RelatedTo', 'to'), ('r', 'Synonym', 'to'), ('a', 'RelatedTo', 'to'), ('cis', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('to', 'Synonym', 'te'), ('v', 'RelatedTo', 'to'), ('en_2', 'RelatedTo', 'to'), ('ok', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('r', 'RelatedTo', 'to'), ('apud', 'RelatedTo', 'to'), ('usque', 'Synonym', 'to'), ('naartoe', 'RelatedTo', 'to'), ('‡πÉ‡∏´‡πâ', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), (\"so's\", 'RelatedTo', 'to'), ('n', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('en_1', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('r', 'Antonym', 'open'), ('⁄©Ÿà', 'RelatedTo', 'to'), ('kepada', 'RelatedTo', 'to'), ('n', 'RelatedTo', 'to'), ('go', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('enjoy_company_of_friends', 'MotivatedByGoal', 'to'), ('stop_standing_in', 'MotivatedByGoal', 'to')]\n",
      "entities {'coffee', 'made_from_coffee_beans', 'food', 'plant', 'popular_drink', 'mug', 'n', 'internet_cafe', 'served_hot', 'caffeine', 'coffee_shop', 'sugar', 'stimulant'}\n",
      "triples [('coffee', 'IsA', 'stimulant'), ('coffee', 'HasA', 'caffeine'), ('coffee', 'HasProperty', 'served_hot'), ('food', 'Synonym', 'food'), ('coffee', 'AtLocation', 'coffee_shop'), ('sugar', 'AtLocation', 'coffee'), ('mug', 'UsedFor', 'coffee'), ('coffee', 'ReceivesAction', 'made_from_coffee_beans'), ('sugar', 'RelatedTo', 'coffee'), ('food', 'Synonym', 'food'), ('plant', 'Synonym', 'plant'), ('plant', 'Synonym', 'plant'), ('n', 'RelatedTo', 'coffee'), ('n', 'Synonym', 'coffee'), ('food', 'Synonym', 'food'), ('plant', 'Synonym', 'plant'), ('coffee', 'IsA', 'popular_drink'), ('coffee', 'AtLocation', 'internet_cafe'), ('food', 'Synonym', 'food'), ('food', 'Synonym', 'food')]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_response_format() missing 3 required positional arguments: 'post', 'entities', and 'triples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-214-51dda23a3336>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"What is related to coffee?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-214-51dda23a3336>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mall_triples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtriples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mformatted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_response_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_entities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_triples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_response_format() missing 3 required positional arguments: 'post', 'entities', and 'triples'"
     ]
    }
   ],
   "source": [
    "question = ''\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', questions)\n",
    "\n",
    "def query_conceptnet(keyword):\n",
    "    url = f\"http://api.conceptnet.io/c/en/{keyword}?limit=100\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "def extract_data(concept_data):\n",
    "    entities = set()\n",
    "    triples = []\n",
    "    for edge in concept_data['edges']:\n",
    "        start = edge['start']['@id'].split('/')[-1]\n",
    "        end = edge['end']['@id'].split('/')[-1]\n",
    "        rel = edge['rel']['@id'].split('/')[-1]\n",
    "        triples.append((start, rel, end))\n",
    "        entities.update([start, end])\n",
    "    return entities, triples\n",
    "\n",
    "\n",
    "# if res not in csk_entities:\n",
    "#     print('Come again? Could you ask again a little bit detail?')\n",
    "\n",
    "    \n",
    "def generate_response_format(dict_csk_entities, dict_csk_triples, post, entities, triples):\n",
    "    res = re.findall( r'\\w+|[^\\s\\w]+', post)\n",
    "\n",
    "    # Simulate entity and triple index mapping\n",
    "    entity2id = {entity: idx for idx, entity in enumerate(entities)}\n",
    "    triple_indices = 0  # Assuming each triple gets a unique index\n",
    "    post_triples = np.zeros(len(res))\n",
    "    \n",
    "#     for idx, split in enumerate(res):\n",
    "#         if split in dict_csk_entities:\n",
    "            \n",
    "        \n",
    "# #         else:\n",
    "# #             continue\n",
    "        \n",
    "            \n",
    "    \n",
    "    data_format = {\n",
    "        \"all_entities_one_hop\": list(entity2id.values()),\n",
    "        \"post_triples\": list(triple_indices),  # Example: every triple is used\n",
    "        \"post\": res,\n",
    "        \"response\": [\"This\", \"is\", \"an\", \"example\", \"response\", \".\"],\n",
    "        \"match_response_index_one_hop\": [-1] * len(entity2id),  # Simplified example\n",
    "        \"only_two\": list(entity2id.values())[::2],  # Every second entity for example\n",
    "        \"match_response_index_only_two\": [-1] * len(entity2id),  # Simplified example\n",
    "        \"all_triples_one_hop\": list(triple_indices)  # Simplified example\n",
    "    }\n",
    "    return data_format\n",
    "\n",
    "def main(question):\n",
    "    keywords = question.split()  # Simple keyword extraction from the question\n",
    "    all_entities = set()\n",
    "    all_triples = []\n",
    "\n",
    "    for keyword in keywords:\n",
    "        concept_data = query_conceptnet(keyword)\n",
    "#         print('concept_data',concept_data)\n",
    "        entities, triples = extract_data(concept_data)\n",
    "        print('entities', entities)\n",
    "        print('triples', triples)\n",
    "        all_entities.update(entities)\n",
    "        all_triples.extend(triples)\n",
    "\n",
    "    formatted_data = generate_response_format(all_entities, all_triples)\n",
    "    print(formatted_data)\n",
    "\n",
    "# Example usage\n",
    "main(\"What is related to coffee?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'wear', 'glasses', 'and', 'at', 'the', 'age', 'of', '17', 'I', 'would', 'have', 'been', 'mortified', 'to', 'be', 'seen', 'in', 'a', 'photo', 'wearing', 'them', '.']\n"
     ]
    }
   ],
   "source": [
    "res = re.findall( r'\\w+|[^\\s\\w]+', post)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19445\n"
     ]
    }
   ],
   "source": [
    "print(dict_csk_entities[\"dumb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "lllist = dict_csk_entities.keys()\n",
    "\n",
    "if 'chicago' in lllist:\n",
    "    print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "comet\n",
      "20750\n",
      "yes entity in tmp_pair\n",
      "no\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(dict_csk_entities['chicago'])\n",
    "post_entities = ['comet', 'sky', 'felt', 'cowboys', 'shoot', 'celestial', 'shooters']\n",
    "tmp_pair = ['deaf', 'comet']\n",
    "test = []\n",
    "entity = 'comet'\n",
    "key = tmp_pair[1]\n",
    "print(key)\n",
    "# num = dict_csk_entities[tmp_pair[1]]\n",
    "print(num)\n",
    "if entity in tmp_pair:\n",
    "    print('yes entity in tmp_pair')\n",
    "    if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "        print('yes')\n",
    "        print('tmp_pair[1]', tmp_pair[1])\n",
    "        if tmp_pair[1] in dict_csk_entities:\n",
    "            print('yes2')\n",
    "            print(tmp_pair[1])\n",
    "            num = dict_csk_entities.get(tmp_pair[1])\n",
    "            print('num',num)\n",
    "            test.append(num)\n",
    "    #     print(test)\n",
    "            print('true')\n",
    "    else:\n",
    "        print('no')\n",
    "# print(dict_csk_entities[tmp_pair[1]])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tmp_pair[1] in dict_csk_entities:\n",
    "    num = dict_csk_entities.get(tmp_pair[1])\n",
    "    num = dict_csk_entities[tmp_pair[1]]\n",
    "    print('num',num)\n",
    "    print(tmp_pair[1])\n",
    "# print(dict_csk_entities['chicago'])\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comet\n",
      " comet\n",
      "At least one entity is in post_entities.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ea7dd4b4eac0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"At least one entity is in post_entities.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Print num if it's defined\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num' is not defined"
     ]
    }
   ],
   "source": [
    "# Example dictionary\n",
    "dict_csk_entities = {'chicago': 123, 'new york': 456}\n",
    "\n",
    "post_entities = ['comet', 'sky', 'felt', 'cowboys', 'shoot', 'celestial', 'shooters']\n",
    "tmp_pair = ['deaf', 'comet']\n",
    "tee = ' comet'\n",
    "print(tee.lstrip())\n",
    "print(tee)\n",
    "entity = 'comet'\n",
    "testt = []\n",
    "# Check if both entities in tmp_pair are not in post_entities\n",
    "if tmp_pair[0] not in post_entities and tmp_pair[1] not in post_entities:\n",
    "    # Check if tmp_pair[1] exists in the dictionary\n",
    "    if tmp_pair[1] in dict_csk_entities:\n",
    "        num = dict_csk_entities[tmp_pair[1]]\n",
    "        testt.append(num)\n",
    "    else:\n",
    "        # If not found, handle it accordingly\n",
    "        print(f\"Entity '{tmp_pair[1]}' not found in dict_csk_entities.\")\n",
    "else:\n",
    "    # Handle the case where one or both entities are in post_entities\n",
    "    print(\"At least one entity is in post_entities.\")\n",
    "\n",
    "print(num)  # Print num if it's defined\n",
    "print(testt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = \"I wear glasses and at the age of 17 I would have been mortified to be seen in a photo wearing them.\"\n",
    "post2 = \"As the comet streaked across the Chicago sky, I felt as though time had looped back to the Wild West, where cowboys would shoot at such celestial wonders with their six-shooters.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_entities ['wear', 'glasses', 'age', '17', 'photo', 'wearing']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "a = create_all_entities_one_hop(post, dict_csk_triples, csk_entities, csk_triples)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loop, RelatedTo, chicago', 'shoot, RelatedTo, comet', 'dumb, RelatedTo, deaf', 'walk, RelatedTo, door', 'pier, RelatedTo, boat']\n"
     ]
    }
   ],
   "source": [
    "a = csk_triples[:5]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = \"I wear glasses and at the age of 17 I would have been mortified to be seen in a photo wearing them.\"\n",
    "post2 = \"As the comet streaked across the Chicago sky, I felt as though time had looped back to the Wild West, where cowboys would shoot at such celestial wonders with their six-shooters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all_entities_one_hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_entities ['comet', 'sky', 'felt', 'cowboys', 'shoot', 'celestial', 'shooters']\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "csk_triples = csk_triples\n",
    "dict_csk_entities = dict_csk_entities\n",
    "csk_entities = csk_entities\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', post2)\n",
    "post_entities = []\n",
    "\n",
    "tmp_e = ''\n",
    "tmp_o = ''\n",
    "\n",
    "for i in res:\n",
    "    if i in csk_entities:\n",
    "        post_entities.append(i)\n",
    "    else:\n",
    "        continue\n",
    "print('post_entities',post_entities)\n",
    "\n",
    "# find one hope from post entities\n",
    "tmp_entities_one_hop = []\n",
    "for e_o in csk_triples:\n",
    "    e_o = e_o.split(',')\n",
    "#         print(e_o)\n",
    "    for entity in post_entities:\n",
    "        tmp_e = e_o[0]\n",
    "        tmp_o = e_o[2].lstrip()\n",
    "        tmp_pair = [tmp_e, tmp_o]\n",
    "#         print('tmp_pair', tmp_pair)\n",
    "#         print('entity', entity)\n",
    "        if entity in tmp_pair:\n",
    "#             print('yes entity in tmp_pair')\n",
    "            if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "#                 print('yes it satsify the first if')\n",
    "                if tmp_pair[1] in dict_csk_entities:\n",
    "#                     print(dict_csk_entities[tmp_pair[1]])\n",
    "                    tmp_entities_one_hop.append(dict_csk_entities[tmp_pair[1]])\n",
    "#                     print('1')\n",
    "            elif entity == tmp_pair[1] and tmp_pair[0] not in post_entities:\n",
    "#                 print('yes it satsify the second if')\n",
    "                if tmp_pair[0] in dict_csk_entities:\n",
    "#                     print('yes it satsify thir one')\n",
    "#                     num = dict_csk_entities[tmp_pair[0]]\n",
    "#                     tmp_entities_one_hop.append(num)\n",
    "                    tmp_entities_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "\n",
    "#                     print(2)\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        tmp_e = []\n",
    "        tmp_o = []\n",
    "        tmp_pair = []\n",
    "# print(tmp_entities_one_hop)\n",
    "# entities_one_hop = []\n",
    "# [entities_one_hop.append(x) for x in tmp_entities_one_hop if x not in entities_one_hop]\n",
    "# print(entities_one_hop)\n",
    "# print(len(entities_one_hop))\n",
    "\n",
    "entities_one_hop = [i for n, i in enumerate(tmp_entities_one_hop) if i not in tmp_entities_one_hop[:n]]\n",
    "print(len(entities_one_hop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# all_triples_one_hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_entities ['comet', 'sky', 'felt', 'cowboys', 'shoot', 'celestial', 'shooters']\n",
      "205\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "## csk_triples = csk_triples\n",
    "dict_csk_entities = dict_csk_entities\n",
    "csk_entities = csk_entities\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', post2)\n",
    "post_entities = []\n",
    "\n",
    "tmp_e = ''\n",
    "tmp_o = ''\n",
    "\n",
    "for i in res:\n",
    "    if i in csk_entities:\n",
    "        post_entities.append(i)\n",
    "    else:\n",
    "        continue\n",
    "print('post_entities',post_entities)\n",
    "\n",
    "# find one hope from post entities\n",
    "tmp_entities_one_hop = []\n",
    "all_triples_one_hop = []\n",
    "for e_o in csk_triples:\n",
    "    e_o_split = e_o.split(',')\n",
    "    for entity in post_entities:\n",
    "        tmp_e = e_o_split[0]\n",
    "        tmp_o = e_o_split[2].lstrip()\n",
    "        tmp_pair = [tmp_e, tmp_o]\n",
    "        if entity in tmp_pair:\n",
    "            if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "                all_triples_one_hop.append(dict_csk_triples[e_o])\n",
    "                if tmp_pair[1] in dict_csk_entities:\n",
    "                    tmp_entities_one_hop.append(dict_csk_entities[tmp_pair[1]])\n",
    "            elif entity == tmp_pair[1] and tmp_pair[0] not in post_entities:\n",
    "                all_triples_one_hop.append(dict_csk_triples[e_o])\n",
    "                if tmp_pair[0] in dict_csk_entities:\n",
    "                    tmp_entities_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        tmp_e = []\n",
    "        tmp_o = []\n",
    "        tmp_pair = []\n",
    "# print(tmp_entities_one_hop)\n",
    "# entities_one_hop = []\n",
    "# [entities_one_hop.append(x) for x in tmp_entities_one_hop if x not in entities_one_hop]\n",
    "# print(entities_one_hop)\n",
    "# print(len(entities_one_hop))\n",
    "\n",
    "entities_one_hop = [i for n, i in enumerate(tmp_entities_one_hop) if i not in tmp_entities_one_hop[:n]]\n",
    "print(len(entities_one_hop))\n",
    "print(len(all_triples_one_hop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_e2 = ''\n",
    "tmp_o2 = ''\n",
    "tmp_pair2= []\n",
    "# find two hope from post entities\n",
    "lsk = ['loop, RelatedTo, chicago', 'shoot, RelatedTo, comet', 'dumb, RelatedTo, deaf', 'walk, RelatedTo, door', 'pier, RelatedTo, boat']\n",
    "# id2word = dict()\n",
    "# for key in word2id.keys():\n",
    "#     id2word[word2id[key]] = key\n",
    "swapped_dict = {value: key for key, value in dict_csk_entities.items()}\n",
    "entities_one_hop = []\n",
    "for i in id_one_hop:\n",
    "    entities_one_hop.append(swapped_dict[i])\n",
    "entities_one_hop2 = entities_one_hop[:5]\n",
    "    \n",
    "tmp_entities_two_hop = []\n",
    "for e_o2 in csk_triples:\n",
    "    e_o2 = e_o2.split(',')\n",
    "    for entity2 in entities_one_hop:\n",
    "        tmp_e2 = e_o2[0]\n",
    "        tmp_o2 = e_o2[2].lstrip()\n",
    "        tmp_pair2 = [tmp_e2, tmp_o2]\n",
    "        if entity2 in tmp_pair2:\n",
    "            if entity2 == tmp_pair2[0] and tmp_pair2[1] not in post_entities and tmp_pair2[1] not in entities_one_hop:\n",
    "                if tmp_pair2[1] in dict_csk_entities:\n",
    "                    tmp_entities_two_hop.append(dict_csk_entities[tmp_pair2[1]])\n",
    "            elif entity2 == tmp_pair2[1] and tmp_pair2[0] not in post_entities and tmp_pair2[0] not in entities_one_hop:\n",
    "                if tmp_pair2[0] in dict_csk_entities:\n",
    "                    tmp_entities_two_hop.append(dict_csk_entities[tmp_pair2[0]])\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        tmp_e2 = []\n",
    "        tmp_o2 = []\n",
    "        tmp_pair2 = []\n",
    "\n",
    "# print(len(tmp_entities_two_hop))\n",
    "entities_two_hop = [i for n, i in enumerate(tmp_entities_two_hop) if i not in tmp_entities_two_hop[:n]]\n",
    "print(len(entities_two_hop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_entities_one_hop(post, dict_csk_entities, csk_entities, csk_triples):\n",
    "    csk_triples = csk_triples\n",
    "    dict_csk_entities = dict_csk_entities\n",
    "    csk_entities = csk_entities\n",
    "    res = re.findall( r'\\w+|[^\\s\\w]+', post)\n",
    "    post_entities = []\n",
    "\n",
    "    tmp_e = ''\n",
    "    tmp_o = ''\n",
    "    \n",
    "    for i in res:\n",
    "        if i in csk_entities:\n",
    "            post_entities.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    print('post_entities',post_entities)\n",
    "\n",
    "    # find one hopt from post entities\n",
    "    tmp_entities_one_hop = []\n",
    "    for e_o in csk_triples:\n",
    "        e_o = e_o.split(',')\n",
    "#         print(e_o)\n",
    "        for entity in post_entities:\n",
    "            tmp_e = e_o[0]\n",
    "            tmp_o = e_o[2].lstrip()\n",
    "            tmp_pair = [tmp_e, tmp_o]\n",
    "    #         print('tmp_pair', tmp_pair)\n",
    "    #         print('entity', entity)\n",
    "            if entity in tmp_pair:\n",
    "    #             print('yes entity in tmp_pair')\n",
    "                if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "    #                 print('yes it satsify the first if')\n",
    "                    if tmp_pair[1] in dict_csk_entities:\n",
    "    #                     print(dict_csk_entities[tmp_pair[1]])\n",
    "                        tmp_entities_one_hop.append(dict_csk_entities[tmp_pair[1]])\n",
    "    #                     print('1')\n",
    "                elif entity == tmp_pair[1] and tmp_pair[0] not in post_entities:\n",
    "    #                 print('yes it satsify the second if')\n",
    "                    if tmp_pair[0] in dict_csk_entities:\n",
    "    #                     print('yes it satsify thir one')\n",
    "    #                     num = dict_csk_entities[tmp_pair[0]]\n",
    "    #                     tmp_entities_one_hop.append(num)\n",
    "                        tmp_entities_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "\n",
    "    #                     print(2)\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "            tmp_e = []\n",
    "            tmp_o = []\n",
    "            tmp_pair = []\n",
    "    # print(tmp_entities_one_hop)\n",
    "    # entities_one_hop = []\n",
    "    # [entities_one_hop.append(x) for x in tmp_entities_one_hop if x not in entities_one_hop]\n",
    "    # print(entities_one_hop)\n",
    "    # print(len(entities_one_hop))\n",
    "\n",
    "    entities_one_hop = [i for n, i in enumerate(tmp_entities_one_hop) if i not in tmp_entities_one_hop[:n]]    \n",
    "    \n",
    "    return entities_one_hop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create all_entities_one_hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_entities_one_hop(post, dict_csk_entities, csk_entities, csk_triples):\n",
    "    csk_triples = csk_triples\n",
    "    dict_csk_entities = dict_csk_entities\n",
    "    csk_entities = csk_entities\n",
    "    post_entities = []\n",
    "    count = 1\n",
    "    res = re.findall( r'\\w+|[^\\s\\w]+', post)\n",
    "    tmp_e = ''\n",
    "    tmp_o = ''\n",
    "    # find entities in post\n",
    "    for i in res:\n",
    "        if i in csk_entities:\n",
    "            post_entities.append(i)\n",
    "        else:\n",
    "            continue\n",
    "    print(post_entities)\n",
    "    # find one hopt from post entities# find one hopt from post entities\n",
    "\n",
    "    tmp_entities_one_hop = []\n",
    "    for e_o in csk_triples:\n",
    "        e_o = e_o.split(',')\n",
    "        for entity in post_entities:\n",
    "            tmp_e = e_o[0]\n",
    "            tmp_o = e_o[2].lstrip()\n",
    "            tmp_pair = [tmp_e, tmp_o]\n",
    "            if entity in tmp_pair:\n",
    "                if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "                    if tmp_pair[1] in dict_csk_entities:\n",
    "                        tmp_entities_one_hop.append(dict_csk_entities[tmp_pair[1]])\n",
    "                elif entity == tmp_pair[1] and tmp_pair[0] not in post_entities:\n",
    "                    if tmp_pair[0] in dict_csk_entities:\n",
    "                        num = dict_csk_entities[tmp_pair[0]]\n",
    "                        tmp_entities_one_hop.append(num)\n",
    "                        tmp_entities_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "            tmp_e = []\n",
    "            tmp_o = []\n",
    "            tmp_pair = []\n",
    "    print('tmp_entities_one_hop', tmp_entities_one_hop)\n",
    "    entities_one_hop = [i for n, i in enumerate(tmp_entities_one_hop) if i not in tmp_entities_one_hop[:n]]\n",
    "    print(len(entities_one_hop))\n",
    "    return entities_one_hop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one_two_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_one_hop\n",
    "entities_two_hop\n",
    "all_triples_one_hop.append(dict_csk_triples[e_o])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_two_triples = []\n",
    "tmp_one_two_triples = []\n",
    "\n",
    "for o_h_t in entities_one_hop:\n",
    "    for t_h_t in entities_two_hop:\n",
    "        for e_o3 in csk_triples:\n",
    "            e_o3_split = e_o3.split(',')\n",
    "            tmp_e3 = e_o3_split[0]\n",
    "            tmp_o3 = e_o3_split[2].lstrip()\n",
    "            if [o_h_t, t_h_t] == [tmp_e3, tmp_o3]:\n",
    "                tmp_one_two_triples.append(dict_csk_triples[e_o])\n",
    "            elif [o_h_t, t_h_t] == [tmp_o3, tmp_e3]:\n",
    "                tmp_one_two_triples.append(dict_csk_triples[e_o])\n",
    "            else:\n",
    "                continue\n",
    "            one_two_triples.append(tmp_one_two_triples)\n",
    "            tmp_one_two_triples = []\n",
    "print(len(one_two_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_e2 = ''\n",
    "tmp_o2 = ''\n",
    "tmp_pair2= []\n",
    "# find two hope from post entities\n",
    "lsk = ['loop, RelatedTo, chicago', 'shoot, RelatedTo, comet', 'dumb, RelatedTo, deaf', 'walk, RelatedTo, door', 'pier, RelatedTo, boat']\n",
    "# id2word = dict()\n",
    "# for key in word2id.keys():\n",
    "#     id2word[word2id[key]] = key\n",
    "swapped_dict = {value: key for key, value in dict_csk_entities.items()}\n",
    "entities_one_hop = []\n",
    "for i in id_one_hop:\n",
    "    entities_one_hop.append(swapped_dict[i])\n",
    "entities_one_hop2 = entities_one_hop[:5]\n",
    "threshold2 = 0.01\n",
    "tmp_entities_two_hop = []\n",
    "for e_o2 in csk_triples:\n",
    "    e_o2 = e_o2.split(',')\n",
    "    for entity2 in entities_one_hop:\n",
    "        tmp_e2 = e_o2[0]\n",
    "        tmp_o2 = e_o2[2].lstrip()\n",
    "        tmp_pair2 = [tmp_e2, tmp_o2]\n",
    "        if entity2 in tmp_pair2:\n",
    "            if entity2 == tmp_pair2[0] and tmp_pair2[1] not in post_entities and tmp_pair2[1] not in entities_one_hop:\n",
    "                if entity2 in w2v_model.key_to_index and tmp_pair2[1] in w2v_model.key_to_index: # check both of them in w2vec vocab\n",
    "                    sim_score2 = w2v_model.similarity(entity2,tmp_pair2[1])\n",
    "                    if tmp_pair2[1] in dict_csk_entities and sim_score2 > threshold2:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair2[1]])                  \n",
    "                    else:\n",
    "                        if tmp_pair2[1] in dict_csk_entities:\n",
    "                            tmp_entities_two_hop.append(dict_csk_entities[tmp_pair2[1]])\n",
    "            elif entity2 == tmp_pair2[1] and tmp_pair2[0] not in post_entities and tmp_pair2[0] not in entities_one_hop:\n",
    "                if entity2 in w2v_model.key_to_index and tmp_pair2[0] in w2v_model.key_to_index: # check both of them in w2vec vocab\n",
    "                    sim_score2 = w2v_model.similarity(entity2,tmp_pair2[0])\n",
    "                    if tmp_pair2[0] in dict_csk_entities and sim_score2>threshold2:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair2[0]])                  \n",
    "                    else: \n",
    "                        if tmp_pair2[0] in dict_csk_entities:\n",
    "                            tmp_entities_two_hop.append(dict_csk_entities[tmp_pair2[0]])\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        tmp_e2 = []\n",
    "        tmp_o2 = []\n",
    "        tmp_pair2 = []\n",
    "\n",
    "# print(len(tmp_entities_two_hop))\n",
    "entities_two_hop = [i for n, i in enumerate(tmp_entities_two_hop) if i not in tmp_entities_two_hop[:n]]\n",
    "print(len(entities_two_hop))\n",
    "# print(entities_two_hop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_post_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the post_triples list\n",
    "def create_post_triples(post, csk_entities):\n",
    "    csk_entities = csk_entities\n",
    "    post_csk_entities = {}\n",
    "    count = 1\n",
    "    res = re.findall( r'\\w+|[^\\s\\w]+', post)\n",
    "\n",
    "    for i in res:\n",
    "#         print(i)\n",
    "        if i in post_csk_entities.keys():\n",
    "            continue\n",
    "\n",
    "        elif i in csk_entities:\n",
    "            post_csk_entities[i] = count\n",
    "            count = count+1\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    post_triples = []\n",
    "    for word in res:\n",
    "        if word in post_csk_entities:\n",
    "            post_triples.append(post_csk_entities[word])\n",
    "        else:\n",
    "            post_triples.append(0)  # Use 0 for words that are not in dict_csk_entities\n",
    "    return post_triples\n",
    "\n",
    "# Generate the post_triples list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Triples: [0, 1, 2, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "post_triples = create_post_triples(post, csk_entities)\n",
    "\n",
    "print(\"Post Triples:\", post_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'related', 'to', 'coffee!']\n",
      "['What', 'is', 'related', 'to', 'coffee', '!']\n",
      "[0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "questions=\"What is related to coffee!\"\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', questions)\n",
    "\n",
    "print(questions.split())\n",
    "print(res)\n",
    "print(np.zeros(len(res)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "2_60L7hXzJA9"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713337068693,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "UYav9uIszKYC"
   },
   "outputs": [],
   "source": [
    "def padding(sent, l):\n",
    "    return sent + ['_EOS'] + ['_PAD'] * (l-len(sent)-1)\n",
    "\n",
    "def padding_triple_id(entity2id, triple, num, l):\n",
    "    newtriple = []\n",
    "    for i in range(len(triple)):\n",
    "        for j in range(len(triple[i])):\n",
    "            for k in range(len(triple[i][j])):\n",
    "                if triple[i][j][k] in entity2id:\n",
    "                    triple[i][j][k] = entity2id[triple[i][j][k]]\n",
    "                else:\n",
    "                    triple[i][j][k] = entity2id['_NONE']\n",
    "\n",
    "    for tri in triple:\n",
    "        newtriple.append(tri + [[entity2id['_PAD_H'], entity2id['_PAD_R'], entity2id['_PAD_T']]] * (l - len(tri)))\n",
    "    pad_triple = [[entity2id['_PAD_H'], entity2id['_PAD_R'], entity2id['_PAD_T']]] * l\n",
    "    return newtriple + [pad_triple] * (num - len(newtriple))\n",
    "\n",
    "def build_kb_adj_mat(kb_adj_mats, fact_dropout):\n",
    "    \"\"\"Create sparse matrix representation for batched data\"\"\"\n",
    "    mats0_batch = np.array([], dtype=int)\n",
    "    mats0_0 = np.array([], dtype=int)\n",
    "    mats0_1 = np.array([], dtype=int)\n",
    "    vals0 = np.array([], dtype=float)\n",
    "\n",
    "    mats1_batch = np.array([], dtype=int)\n",
    "    mats1_0 = np.array([], dtype=int)\n",
    "    mats1_1 = np.array([], dtype=int)\n",
    "    vals1 = np.array([], dtype=float)\n",
    "\n",
    "    for i in range(kb_adj_mats.shape[0]):\n",
    "        (mat0_0, mat0_1, val0), (mat1_0, mat1_1, val1) = kb_adj_mats[i]\n",
    "        assert len(val0) == len(val1)\n",
    "        num_fact = len(val0)\n",
    "        num_keep_fact = int(np.floor(num_fact * (1 - fact_dropout)))\n",
    "        mask_index = np.random.permutation(num_fact)[ : num_keep_fact]\n",
    "        # mat0\n",
    "        mats0_batch = np.append(mats0_batch, np.full(len(mask_index), i, dtype=int))\n",
    "        mats0_0 = np.append(mats0_0, mat0_0[mask_index])\n",
    "        mats0_1 = np.append(mats0_1, mat0_1[mask_index])\n",
    "        vals0 = np.append(vals0, val0[mask_index])\n",
    "        # mat1\n",
    "        mats1_batch = np.append(mats1_batch, np.full(len(mask_index), i, dtype=int))\n",
    "        mats1_0 = np.append(mats1_0, mat1_0[mask_index])\n",
    "        mats1_1 = np.append(mats1_1, mat1_1[mask_index])\n",
    "        vals1 = np.append(vals1, val1[mask_index])\n",
    "\n",
    "    return (mats0_batch, mats0_0, mats0_1, vals0), (mats1_batch, mats1_0, mats1_1, vals1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csk_triples = csk_triples\n",
    "dict_csk_entities = dict_csk_entities\n",
    "csk_entities = csk_entities\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', post3)\n",
    "\n",
    "post_entities = []\n",
    "one_hop_triples_id = []\n",
    "tmp_only_two = []\n",
    "only_two = []\n",
    "one_two_triple = []\n",
    "tmp_one_two_triple = []\n",
    "\n",
    "tmp_e = ''\n",
    "tmp_o = ''\n",
    "for i in res:\n",
    "    if i in csk_entities:\n",
    "        post_entities.append(i)\n",
    "    else:\n",
    "        continue\n",
    "print('post_entities',post_entities)\n",
    "\n",
    "threshold_one_hop_triples = 0.4\n",
    "threshold_two_hop_triples = 0.5\n",
    "for entity in post_entities:\n",
    "    kb_d = kb_dict[entity]\n",
    "    for e_o in kb_d:\n",
    "        e_o_split = e_o.split(',')\n",
    "        tmp_e = e_o_split[0]\n",
    "        tmp_o = e_o_split[2].lstrip()\n",
    "        tmp_pair = [tmp_e, tmp_o]\n",
    "        if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "            tmp_id_one_hop.append(dict_csk_entities[tmp_pair[1]])\n",
    "            one_hop_triples_id.append(dict_csk_triples[e_o])\n",
    "            if tmp_pair[1] in kb_dict:\n",
    "                tmp_pair1_kb_d = kb_dict[tmp_pair[1]]\n",
    "                for tmp_pair1_e_o in tmp_pair1_kb_d:\n",
    "                    tmp_pair1_e_o_split = tmp_pair1_e_o.split(',')\n",
    "                    tmp_pair1_e = tmp_pair1_e_o_split[0]\n",
    "                    tmp_pair1_o = tmp_pair1_e_o_split[2].lstrip()\n",
    "                    if tmp_pair1_e in w2v_model.key_to_index and tmp_pair1_o in w2v_model.key_to_index:\n",
    "                        sim_score = w2v_model.similarity(tmp_pair1_e,tmp_pair1_o)\n",
    "                        sim_socre2 = w2v_model.similarity(entity,tmp_pair1_o)\n",
    "                        if sim_score > threshold_one_hop_triples: # and sim_score2 > threshold_two_hop_triples:\n",
    "                            one_hop_triples_id.append(dict_csk_triples[tmp_pair1_e_o])\n",
    "#                         elif sim_score > threshold_one_hop_triples or sim_score2 > threshold_two_hop_triples:\n",
    "                        else:\n",
    "                            tmp_one_two_triple.append(dict_csk_triples[tmp_pair1_e_o])\n",
    "                            tmp_only_two.append(tmp_pair1_o)\n",
    "                    else:\n",
    "                        continue\n",
    "#                     id_two_hop = [i for n, i in enumerate(tmp_one_two_triple) if i not in tmp_one_two_triple[:n]]\n",
    "                    one_two_triple.append(tmp_one_two_triple)\n",
    "                    only_two.append(tmp_only_two)\n",
    "                tmp_one_two_triple = []\n",
    "                tmp_only_tow = []\n",
    "        elif entity == tmp_pair[1] and tmp_pair[0] not in post_entities:\n",
    "            tmp_id_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "            one_hop_triples_id.append(dict_csk_triples[e_o])\n",
    "            if tmp_pair[0] in kb_dict:\n",
    "                tmp_pair2_kb_d = kb_dict[tmp_pair[0]]\n",
    "                tmp_one_two_triple = []\n",
    "                for tmp_pair2_e_o in tmp_pair2_kb_d:\n",
    "                    tmp_pair2_e_o_split = tmp_pair2_e_o.split(',')\n",
    "                    tmp_pair2_e = tmp_pair2_e_o_split[0]\n",
    "                    tmp_pair2_o = tmp_pair2_e_o_split[2].lstrip()\n",
    "                    if tmp_pair2_e in w2v_model.key_to_index and tmp_pair2_o in w2v_model.key_to_index:\n",
    "                        sim_score = w2v_model.similarity(tmp_pair2_e,tmp_pair2_o)\n",
    "                        sim_socre2 = w2v_model.similarity(entity,tmp_pair2_o)\n",
    "                        if sim_score > threshold_one_hop_triples: # and sim_score2 > threshold_two_hop_triples:\n",
    "                            one_hop_triples_id.append(dict_csk_triples[tmp_pair2_e_o])\n",
    "#                         elif sim_score > threshold_one_hop_triples or sim_score2 > threshold_two_hop_triples:\n",
    "                        else:\n",
    "                            tmp_one_two_triple.append(dict_csk_triples[tmp_pair2_e_o])\n",
    "                            tmp_only_two.append(tmp_pair2_o)\n",
    "                    else:\n",
    "                        continue\n",
    "                    one_two_triple.append(tmp_one_two_triple)\n",
    "                    only_two.append(tmp_only_two)\n",
    "                tmp_one_two_triple = []\n",
    "                tmp_only_tow = []\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ all_entities_one_hop ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ\n",
    "id_one_hop = [i for n, i in enumerate(tmp_id_one_hop) if i not in tmp_id_one_hop[:n]]\n",
    "# print(id_one_hop)\n",
    "print('before', len(id_one_hop))\n",
    "            \n",
    "# ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ all_triples_one_hop ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ‚òÜ\n",
    "one_hop_triples_id = [i for n, i in enumerate(one_hop_triples_id) if i not in one_hop_triples_id[:n]]\n",
    "\n",
    "print('one_hop_triples_id', len(one_hop_triples_id))\n",
    "print(one_hop_triples_id)\n",
    "\n",
    "one_two_triple = [i for n, i in enumerate(one_two_triple) if i not in one_two_triple[:n]]\n",
    "# only_two = [i for n, i in enumerate(only_two) if i not in only_two[:n]]\n",
    "# new_only_two = []\n",
    "# [new_only_two.append(item) for item in only_two if item not in new_only_two]\n",
    "new_only_two = list(set(only_two[0]))\n",
    "new = []\n",
    "for j in one_two_triple:\n",
    "    length = len(j)\n",
    "    if length > 2:\n",
    "        new.append(random.sample(j, 3))\n",
    "    else:\n",
    "        new.append(j)\n",
    "one_two_triple = new\n",
    "\n",
    "# print('two_hop_triples_id', len(one_two_triple))\n",
    "\n",
    "only_two_two = []\n",
    "for entity in new_only_two:\n",
    "    only_two_two.append(dict_csk_entities[entity])\n",
    "# print(only_two_two)\n",
    "only_two_two_two = random.sample(only_two_two, 100)\n",
    "# print(len(only_two_two_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE6yA-kXrJhr"
   },
   "source": [
    "# Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1713337075255,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "LH3SVLXRhgi7"
   },
   "outputs": [],
   "source": [
    "csk_triples, csk_entities, kb_dict = [], [], []\n",
    "dict_csk_entities, dict_csk_triples = {}, {}\n",
    "class Config():\n",
    "  def __init__(self, path):\n",
    "    self.config_path = path\n",
    "    self._get_config()\n",
    "\n",
    "  def _get_config(self):\n",
    "    with open(self.config_path, \"r\") as setting:\n",
    "      config = yaml.load(setting,Loader=yaml.FullLoader)\n",
    "    self.is_train = config['is_train']\n",
    "    self.test_model_path = config['test_model_path']\n",
    "    self.embed_units = config['embed_units']\n",
    "    self.symbols = config['symbols']\n",
    "    self.units = config['units']\n",
    "    self.layers = config['layers']\n",
    "    self.batch_size = config['batch_size']\n",
    "    self.data_dir = config['data_dir']\n",
    "    self.num_epoch = config['num_epoch']\n",
    "    self.lr_rate = config['lr_rate']\n",
    "    self.lstm_dropout = config['lstm_dropout']\n",
    "    self.linear_dropout = config['linear_dropout']\n",
    "    self.max_gradient_norm = config['max_gradient_norm']\n",
    "    self.trans_units = config['trans_units']\n",
    "    self.gnn_layers = config['gnn_layers']\n",
    "    self.fact_dropout = config['fact_dropout']\n",
    "    self.fact_scale = config['fact_scale']\n",
    "    self.pagerank_lambda = config['pagerank_lambda']\n",
    "    self.result_dir_name = config['result_dir_name']\n",
    "    self.generated_path = config['generated_path']\n",
    "\n",
    "  def list_all_member(self):\n",
    "    for name, value in vars(self).items():\n",
    "        print('%s = %s' % (name, value))\n",
    "\n",
    "\n",
    "def run(model, data_train, config, word2id, entity2id):\n",
    "    batched_data = gen_batched_data(data_train, config, word2id, entity2id)\n",
    "    if model.is_inference == True:\n",
    "        word_index, selector = model(batched_data)\n",
    "        return word_index, selector\n",
    "    else:\n",
    "        decoder_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, word_neg_num, local_neg_num, only_two_neg_num = model(batched_data)\n",
    "        return decoder_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, word_neg_num, local_neg_num, only_two_neg_num\n",
    "\n",
    "\n",
    "def generate(model, data_test, config, word2id, entity2id, epoch = 0, model_path = None):\n",
    "  if model_path != None:\n",
    "      model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))\n",
    "\n",
    "  count = 0\n",
    "  model.is_inference = True\n",
    "  id2word = dict()\n",
    "  for key in word2id.keys():\n",
    "      id2word[word2id[key]] = key\n",
    "\n",
    "  def write_batch_res_text(word_index, id2word, selector = None):\n",
    "      w = open(config.generated_path + '/generated_res_Scr.txt', 'a')\n",
    "      batch_size = len(word_index)\n",
    "      decoder_len = len(word_index[0])\n",
    "      text = []\n",
    "      if selector != None:\n",
    "          for i in range(batch_size):\n",
    "              tmp_dict = dict()\n",
    "              tmp = []\n",
    "              for j in range(decoder_len):\n",
    "                  if word_index[i][j] == 2:\n",
    "                      break\n",
    "                  tmp.append(id2word[word_index[i][j]])\n",
    "              # print(tmp)\n",
    "              tmp_dict['res_text'] = tmp\n",
    "              local_tmp = []\n",
    "              only_two_tmp = []\n",
    "              for j in range(len(tmp)):\n",
    "                  if selector[i][j] == 1:\n",
    "                      local_tmp.append(tmp[j])\n",
    "                  if selector[i][j] == 2:\n",
    "                      only_two_tmp.append(tmp[j])\n",
    "              tmp_dict['local'] = local_tmp\n",
    "              tmp_dict['only_two'] = only_two_tmp\n",
    "              text.append(tmp_dict)\n",
    "\n",
    "      w.write(json.dumps(model_path+ '\\n'))\n",
    "      for line in text:\n",
    "          print(line)\n",
    "          w.write(json.dumps(line) + '\\n')\n",
    "      w.close()\n",
    "\n",
    "  for iteration in range(len(data_test)):\n",
    "    word_index, selector = run(model, data_test[(iteration * config.batch_size):(iteration * \\\n",
    "          config.batch_size + config.batch_size)], config, word2id, entity2id)\n",
    "\n",
    "    if count % 50 == 0:\n",
    "        print (\"generate:\", iteration)\n",
    "    count += 1\n",
    "    write_batch_res_text(word_index, id2word, selector=selector)\n",
    "\n",
    "\n",
    "def main():\n",
    "  config = Config('/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/config.yml')\n",
    "  config.list_all_member()\n",
    "  raw_vocab, _, data_test = prepare_data(config)\n",
    "  word2id, entity2id, vocab, embed, entity_vocab, entity_embed, relation_vocab, relation_embed, entity_relation_embed = build_vocab(config.data_dir, raw_vocab, config = config)\n",
    "  model = use_cuda(ConceptFlow(config, embed, entity_relation_embed))\n",
    "\n",
    "  model_optimizer = torch.optim.Adam(model.parameters(), lr = config.lr_rate)\n",
    "\n",
    "  if not os.path.exists(config.generated_path):\n",
    "      os.mkdir(config.generated_path)\n",
    "\n",
    "  generate(model, data_test, config, word2id, entity2id, model_path=config.test_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1713335677369,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "Xt6MESz2w0tg",
    "outputId": "3d7f6eef-a51c-4b58-e21d-431854cb43b9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/config.yml'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.realpath('/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 54867,
     "status": "error",
     "timestamp": 1713337133052,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "GH850HdUhgp_",
    "outputId": "017076ab-ac85-494e-8173-0fc96ba79baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_path = /content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/config.yml\n",
      "is_train = False\n",
      "test_model_path = /content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/training_output/_epoch_4.pkl\n",
      "embed_units = 300\n",
      "symbols = 30000\n",
      "units = 512\n",
      "layers = 2\n",
      "batch_size = 30\n",
      "data_dir = /content/drive/MyDrive/Colab Notebooks/ConceptFlow(ECCF)_data\n",
      "num_epoch = 5\n",
      "lr_rate = 0.0001\n",
      "lstm_dropout = 0.3\n",
      "linear_dropout = 0.2\n",
      "max_gradient_norm = 5\n",
      "trans_units = 100\n",
      "gnn_layers = 3\n",
      "fact_dropout = 0.0\n",
      "fact_scale = 1\n",
      "pagerank_lambda = 0.8\n",
      "result_dir_name = /content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/training_output\n",
      "generated_path = /content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/inference_output\n",
      "Creating word vocabulary...\n",
      "Creating entity vocabulary...\n",
      "Creating relation vocabulary...\n",
      "Loading word vectors...\n",
      "    processing line 0\n",
      "    processing line 100000\n",
      "    processing line 200000\n",
      "    processing line 300000\n",
      "    processing line 400000\n",
      "    processing line 500000\n",
      "    processing line 600000\n",
      "    processing line 700000\n",
      "    processing line 800000\n",
      "    processing line 900000\n",
      "    processing line 1000000\n",
      "    processing line 1100000\n",
      "    processing line 1200000\n",
      "    processing line 1300000\n",
      "    processing line 1400000\n",
      "    processing line 1500000\n",
      "    processing line 1600000\n",
      "Loading entity vectors...\n",
      "Loading relation vectors...\n",
      "generate: 0\n",
      "{'res_text': ['i', 'do', \"n't\", 'know', 'if', 'i', \"'m\", 'wearing', 'this', '.'], 'local': ['wearing'], 'only_two': []}\n",
      "{'res_text': ['i', \"'ll\", 'shake', 'my', 'ass'], 'local': ['shake', 'ass'], 'only_two': []}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-86ed81c010d4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerated_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m   \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-86ed81c010d4>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, data_test, config, word2id, entity2id, epoch, model_path)\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;31m# for iteration in range(len(data_test) // config.batch_size):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       word_index, selector = run(model, data_test[(iteration * config.batch_size):(iteration * \\\n\u001b[0m\u001b[1;32m     94\u001b[0m           config.batch_size + config.batch_size)], config, word2id, entity2id)\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-86ed81c010d4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, data_train, config, word2id, entity2id)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mbatched_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_batched_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_inference\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7835ad0550cd>\u001b[0m in \u001b[0;36mgen_batched_data\u001b[0;34m(data, config, word2id, entity2id)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mcsk_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsk_triples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkb_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_csk_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_csk_triples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mencoder_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mdecoder_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vxoVGUzyvJRD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_entities ['comet', 'sky', 'felt', 'cowboys', 'shoot', 'celestial', 'shooters']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'w2v_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e0d611ce062a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtmp_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtmp_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpost_entities\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtmp_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# check both of them in w2vec vocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m                     \u001b[0msim_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtmp_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mtmp_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdict_csk_entities\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msim_score\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w2v_model' is not defined"
     ]
    }
   ],
   "source": [
    "csk_triples = csk_triples\n",
    "dict_csk_entities = dict_csk_entities\n",
    "csk_entities = csk_entities\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', post2)\n",
    "post_entities = []\n",
    "\n",
    "tmp_e = ''\n",
    "tmp_o = ''\n",
    "\n",
    "for i in res:\n",
    "    if i in csk_entities:\n",
    "        post_entities.append(i)\n",
    "    else:\n",
    "        continue\n",
    "print('post_entities',post_entities)\n",
    "\n",
    "\n",
    "# find one_hop_triples from post entities\n",
    "all_triples_one_hop = []\n",
    "\n",
    "# find one_hop_entities from post entities\n",
    "tmp_id_one_hop = []\n",
    "\n",
    "threshold = 0.24\n",
    "for entity in post_entities:\n",
    "    for e_o in csk_triples:\n",
    "        e_o_split = e_o.split(',')\n",
    "        tmp_e = e_o_split[0]\n",
    "        tmp_o = e_o_split[2].lstrip()\n",
    "        tmp_pair = [tmp_e, tmp_o]\n",
    "        if entity in tmp_pair:\n",
    "            if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "                if entity in w2v_model.key_to_index and tmp_pair[1] in w2v_model.key_to_index: # check both of them in w2vec vocab\n",
    "                    sim_score = w2v_model.similarity(entity,tmp_pair[1])\n",
    "                    all_triples_one_hop.append(dict_csk_triples[e_o])\n",
    "                    if tmp_pair[1] in dict_csk_entities and sim_score>threshold:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair[1]])                    \n",
    "                else:\n",
    "                    all_triples_one_hop.append(dict_csk_triples[e_o])\n",
    "                    if tmp_pair[1] in dict_csk_entities:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair[1]])\n",
    "                        \n",
    "            elif entity == tmp_pair[1] and tmp_pair[0] not in post_entities:\n",
    "                if entity in w2v_model.key_to_index and tmp_pair[0] in w2v_model.key_to_index: # check both of them in w2vec vocab\n",
    "                    sim_score = w2v_model.similarity(entity,tmp_pair[0])\n",
    "                    if tmp_pair[0] in dict_csk_entities and sim_score>threshold:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "                        all_triples_one_hop.append(dict_csk_triples[e_o])\n",
    "                else:\n",
    "                    if tmp_pair[0] in dict_csk_entities:\n",
    "                        all_triples_one_hop.append(dict_csk_triples[e_o])\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        tmp_e = []\n",
    "        tmp_o = []\n",
    "        tmp_pair = []\n",
    "\n",
    "id_one_hop = [i for n, i in enumerate(tmp_id_one_hop) if i not in tmp_id_one_hop[:n]]\n",
    "\n",
    "# print(id_one_hop)\n",
    "print(len(id_one_hop))\n",
    "print(id_one_hop)\n",
    "print(len(all_triples_one_hop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-1cd77f38390a>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-1cd77f38390a>\"\u001b[1;36m, line \u001b[1;32m32\u001b[0m\n\u001b[1;33m    elif: sim_score > threshold:\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "csk_triples = csk_triples\n",
    "dict_csk_entities = dict_csk_entities\n",
    "csk_entities = csk_entities\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', post3)\n",
    "post_entities = []\n",
    "\n",
    "tmp_e = ''\n",
    "tmp_o = ''\n",
    "\n",
    "for i in res:\n",
    "    if i in csk_entities:\n",
    "        post_entities.append(i)\n",
    "    else:\n",
    "        continue\n",
    "print('post_entities',post_entities)\n",
    "one_hop_triples_id = []\n",
    "threshold = 0.4\n",
    "for entity in post_entities:\n",
    "    kb_d = kb_dict[entity]\n",
    "    for e_o in kb_d:\n",
    "        e_o_split = e_o.split(',')\n",
    "        tmp_e = e_o_split[0]\n",
    "        tmp_o = e_o_split[2].lstrip()\n",
    "        tmp_pair = [tmp_e, tmp_o]\n",
    "        sim_score = w2v_model.similarity(tmp_e,tmp_o)\n",
    "        if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "            tmp_id_one_hop.append(dict_csk_entities[tmp_pair[1]])\n",
    "            one_hop_triples_id.append(dict_csk_triples[e_o])\n",
    "        elif entity == tmp_pair[1] and tmp_pair[0] not in post_entities:\n",
    "            tmp_id_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "            one_hop_triples_id.append(dict_csk_triples[e_o])\n",
    "        elif: sim_score > threshold:\n",
    "            one_hop_triples_id.append(dict_csk_triples[e_o])                \n",
    "        else:\n",
    "            continue\n",
    "\n",
    "id_one_hop = [i for n, i in enumerate(tmp_id_one_hop) if i not in tmp_id_one_hop[:n]]\n",
    "print(id_one_hop)\n",
    "print(len(id_one_hop))\n",
    "\n",
    "one_hop_triples_id = [i for n, i in enumerate(one_hop_triples_id) if i not in one_hop_triples_id[:n]]\n",
    "\n",
    "print('one_hop_triples_id', len(one_hop_triples_id))\n",
    "print(one_hop_triples_id)\n",
    "\n",
    "swapped_dict = {value: key for key, value in dict_csk_entities.items()}\n",
    "one_hop_e = ''\n",
    "\n",
    "\n",
    "    one_hop_e = swapped_dict[one_hop_id]\n",
    "#     print(one_hop_e)\n",
    "    if one_hop_e in kb_dict:\n",
    "        kb_d = kb_dict[one_hop_e]\n",
    "        tmp_one_hop_triples = []\n",
    "        for e_o in kb_d:\n",
    "            tmp_one_hop_triples.append(dict_csk_triples[e_o])\n",
    "            if len(tmp_one_hop_triples) >= 2:\n",
    "                samplelist = random.choices(tmp_one_hop_triples, k=2)\n",
    "            else:\n",
    "                samplelist = [dict_csk_triples[e_o]]\n",
    "        one_hop_triples_id.extend(samplelist)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(len(one_hop_triples_id))\n",
    "print(one_hop_triples_id)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# last of last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csk_triples = csk_triples\n",
    "dict_csk_entities = dict_csk_entities\n",
    "csk_entities = csk_entities\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', post3)\n",
    "post_entities = []\n",
    "\n",
    "tmp_e = ''\n",
    "tmp_o = ''\n",
    "\n",
    "for i in res:\n",
    "    if i in csk_entities:\n",
    "        post_entities.append(i)\n",
    "    else:\n",
    "        continue\n",
    "print('post_entities',post_entities)\n",
    "one_hop_triples_id = []\n",
    "threshold = 0.4\n",
    "for entity in post_entities:\n",
    "    kb_d = kb_dict[entity]\n",
    "    for e_o in kb_d:\n",
    "        e_o_split = e_o.split(',')\n",
    "        tmp_e = e_o_split[0]\n",
    "        tmp_o = e_o_split[2].lstrip()\n",
    "        tmp_pair = [tmp_e, tmp_o]\n",
    "        if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "            tmp_id_one_hop.append(dict_csk_entities[tmp_pair[1]])\n",
    "            one_hop_triples_id.append(dict_csk_triples[e_o])\n",
    "            if tmp_pair[1] in kb_dict:\n",
    "                tmp_pair1_kb_d = kb_dict[tmp_pair[1]]\n",
    "                for tmp_pair1_e_o in tmp_pair1_kb_d:\n",
    "                    tmp_pair1_e_o_split = tmp_pair1_e_o.split(',')\n",
    "                    tmp_pair1_e = tmp_pair1_e_o_split[0]\n",
    "                    tmp_pair1_o = tmp_pair1_e_o_split[2].lstrip()\n",
    "                    if tmp_pair1_e in w2v_model.key_to_index and tmp_pair1_o in w2v_model.key_to_index:\n",
    "                        sim_score = w2v_model.similarity(tmp_pair1_e,tmp_pair1_o)\n",
    "                        if sim_score > threshold:\n",
    "                            one_hop_triples_id.append(dict_csk_triples[tmp_pair1_e_o])\n",
    "                    else:\n",
    "                        continue\n",
    "        elif entity == tmp_pair[1] and tmp_pair[0] not in post_entities:\n",
    "            tmp_id_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "            one_hop_triples_id.append(dict_csk_triples[e_o])\n",
    "            if tmp_pair[0] in kb_dict:\n",
    "                tmp_pair2_kb_d = kb_dict[tmp_pair[0]]\n",
    "                for tmp_pair2_e_o in tmp_pair2_kb_d:\n",
    "                    tmp_pair2_e_o_split = tmp_pair2_e_o.split(',')\n",
    "                    tmp_pair2_e = tmp_pair2_e_o_split[0]\n",
    "                    tmp_pair2_o = tmp_pair2_e_o_split[2].lstrip()\n",
    "                    if tmp_pair2_e in w2v_model.key_to_index and tmp_pair2_o in w2v_model.key_to_index:\n",
    "                        sim_score = w2v_model.similarity(tmp_pair2_e,tmp_pair2_o)\n",
    "                        if sim_score > threshold:\n",
    "                            one_hop_triples_id.append(dict_csk_triples[tmp_pair2_e_o])\n",
    "                    else:\n",
    "                        continue               \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "id_one_hop = [i for n, i in enumerate(tmp_id_one_hop) if i not in tmp_id_one_hop[:n]]\n",
    "# print(id_one_hop)\n",
    "print('before', len(id_one_hop))\n",
    "            \n",
    "# for entity in post_entities:\n",
    "#     kb_d = kb_dict[entity]\n",
    "#     for e_o in kb_d:\n",
    "#         kb_e_o = kb_dict[e_o]\n",
    "#         for kb_e_o_t in kb_e_o:\n",
    "#             kb_e_o_split = kb_e_o_t.split(',')\n",
    "#             kb_tmp_e = kb_e_o_split[0]\n",
    "#             kb_tmp_o = kb_e_o_split[2].lstrip()\n",
    "#             sim_score = w2v_model.similarity(kb_tmp_e,kb_tmp_o)\n",
    "            \n",
    "#             if sim_score > threshold:\n",
    "#                 one_hop_triples_id.append(dict_csk_triples[kb_e_o_t])\n",
    "#             else:\n",
    "#                 continue\n",
    "                \n",
    "\n",
    "            \n",
    "# id_one_hop = [i for n, i in enumerate(tmp_id_one_hop) if i not in tmp_id_one_hop[:n]]\n",
    "# # print(id_one_hop)\n",
    "# print('after', len(id_one_hop))\n",
    "\n",
    "one_hop_triples_id = [i for n, i in enumerate(one_hop_triples_id) if i not in one_hop_triples_id[:n]]\n",
    "\n",
    "print('one_hop_triples_id', len(one_hop_triples_id))\n",
    "print(one_hop_triples_id)\n",
    "\n",
    "# swapped_dict = {value: key for key, value in dict_csk_entities.items()}\n",
    "# one_hop_e = ''\n",
    "\n",
    "\n",
    "#     one_hop_e = swapped_dict[one_hop_id]\n",
    "# #     print(one_hop_e)\n",
    "#     if one_hop_e in kb_dict:\n",
    "#         kb_d = kb_dict[one_hop_e]\n",
    "#         tmp_one_hop_triples = []\n",
    "#         for e_o in kb_d:\n",
    "#             tmp_one_hop_triples.append(dict_csk_triples[e_o])\n",
    "#             if len(tmp_one_hop_triples) >= 2:\n",
    "#                 samplelist = random.choices(tmp_one_hop_triples, k=2)\n",
    "#             else:\n",
    "#                 samplelist = [dict_csk_triples[e_o]]\n",
    "#         one_hop_triples_id.extend(samplelist)\n",
    "#     else:\n",
    "#         continue\n",
    "\n",
    "# print(len(one_hop_triples_id))\n",
    "# print(one_hop_triples_id)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_two_triple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d1baea7b18c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mswapped_dict_triple\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdict_csk_triples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mone_two_triple_tmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_two_triple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# print(one_two_triple_tmp)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# for entity in post_entities:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#     for tmp_id in one_two_triple_tmp:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'one_two_triple' is not defined"
     ]
    }
   ],
   "source": [
    "swapped_dict_triple = {value: key for key, value in dict_csk_triples.items()}\n",
    "one_two_triple_tmp = one_two_triple[:5]\n",
    "# print(one_two_triple_tmp)\n",
    "# for entity in post_entities:\n",
    "#     for tmp_id in one_two_triple_tmp:\n",
    "#         print(tmp_id)\n",
    "#         for j in tmp_id:\n",
    "#             e_o_split = swapped_dict_triple[j].split(',')\n",
    "#             tmp_e = e_o_split[0]\n",
    "#             tmp_o = e_o_split[2].lstrip()\n",
    "#             tmp_pair = [tmp_e, tmp_o]\n",
    "#             if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "#                 tmp_id_one_hop.append(dict_csk_entities[tmp_pair[1]])\n",
    "#                 one_hop_triples_id.append(dict_csk_triples[e_o])\n",
    "#                 if tmp_pair[1] in kb_dict:\n",
    "#                     tmp_pair1_kb_d = kb_dict[tmp_pair[1]]\n",
    "#                     for tmp_pair1_e_o in tmp_pair1_kb_d:\n",
    "#                         tmp_pair1_e_o_split = tmp_pair1_e_o.split(',')\n",
    "#                         tmp_pair1_e = tmp_pair1_e_o_split[0]\n",
    "#                         tmp_pair1_o = tmp_pair1_e_o_split[2].lstrip()\n",
    "#     print(swapped_dict_triple[29445])\n",
    "new_tmp_ = []\n",
    "new_new = []\n",
    "threshold_two_hop_triples3 = 0.1\n",
    "for entity in post_entities:\n",
    "    kb_d = kb_dict[entity]\n",
    "    for e_o in kb_d:\n",
    "        e_o_split = e_o.split(',')\n",
    "        tmp_e = e_o_split[0]\n",
    "        tmp_o = e_o_split[2].lstrip()\n",
    "        tmp_pair = [tmp_e, tmp_o]\n",
    "        if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "            if tmp_pair[1] in ex:\n",
    "                for j in ex[tmp_pair[1]]:\n",
    "                    tmp_pair1_kb_d = swapped_dict_triple[j]\n",
    "#                     print(tmp_pair1_kb_d)\n",
    "                    tmp_pair_1_s = tmp_pair1_kb_d.split(',')\n",
    "                    tmp_pair1_e = tmp_pair_1_s[0]\n",
    "                    tmp_pair1_o = tmp_pair_1_s[2].lstrip()\n",
    "                    if tmp_pair1_e in w2v_model.key_to_index and tmp_pair1_o in w2v_model.key_to_index:\n",
    "                        sim_socre2 = w2v_model.similarity(entity,tmp_pair1_o)\n",
    "                        if sim_score2 > threshold_two_hop_triples3:\n",
    "                            new_tmp_.append(dict_csk_triples[tmp_pair1_kb_d])\n",
    "#                         elif sim_score > threshold_one_hop_triples or sim_score2 > threshold_two_hop_triples:\n",
    "                        else:\n",
    "                            continue\n",
    "                    new_new.append(new_tmp_)\n",
    "                new_tmp_ = []\n",
    "        elif entity == tmp_pair[1] and tmp_pair[0] not in post_entities:\n",
    "            if tmp_pair[0] in ex:\n",
    "                for k in ex[tmp_pair[0]]:\n",
    "                    tmp_pair2_kb_d = swapped_dict_triple[k]\n",
    "                    tmp_pair2_s = tmp_pair2_kb_d.split(',')\n",
    "                    tmp_pair2_e = tmp_pair2_s[0]\n",
    "                    tmp_pair2_o = tmp_pair2_s[2].lstrip()\n",
    "#                     print(tmp_pair2_e, tmp_pair2_o)\n",
    "                    if tmp_pair2_e in w2v_model.key_to_index and tmp_pair2_o in w2v_model.key_to_index:\n",
    "#                         print('yes')\n",
    "                        sim_socre2 = w2v_model.similarity(entity,tmp_pair2_o)\n",
    "                        if sim_score2 > threshold_two_hop_triples3:\n",
    "                            new_tmp_.append(dict_csk_triples[tmp_pair2_kb_d])\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "                    new_new.append(new_tmp_)\n",
    "                new_tmp_ = []\n",
    "        else:\n",
    "            continue\n",
    "print(new_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNEdTDQKImWv3G/elEQrVZp",
   "gpuType": "L4",
   "machine_shape": "hm",
   "mount_file_id": "1Psy_6BF-POj9vN3vC27yL_XoZNDrch1Z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
