{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiKxnFtF05+ANGfn3dvEgg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sjX8X-ExnoWn"},"outputs":[],"source":["class ConceptFlow(nn.Module):\n","\n","  def __init__(self, config, word_embed, entity_embed, is_select=False):\n","    super(ConceptFlow, self).__init__()\n","    self.is_select = is_select\n","    self.is_inference = False\n","\n","    self.trans_units = config.trans_units\n","    self.embed_units = config.embed_units\n","    self.units = config.units\n","    self.layers = config.layers\n","    self.gnn_layers = config.gnn_layers\n","    self.symbols = config.symbols\n","\n","    self.WordEmbedding = WordEmbedding(word_embed, self.embed_units)\n","    self.EntityEmbedding = EntityEmbedding(entity_embed, self.trans_units)\n","    self.CentralEncoder = CentralEncoder(config, self.gnn_layers, self.embed_units, self.trans_units, self.WordEmbedding, self.EntityEmbedding)\n","    self.OuterEncoder = OuterEncoder(self.trans_units, self.EntityEmbedding)\n","\n","    self.softmax_d1 = nn.Softmax(dim = 1)\n","    self.softmax_d2 = nn.Softmax(dim = 2)\n","\n","    self.text_encoder = nn.GRU(input_size = self.embed_units, hidden_size = self.units, num_layers = self.layers, batch_first = True)\n","    self.decoder = nn.GRU(input_size = self.units + self.embed_units, hidden_size = self.units, num_layers = self.layers, batch_first = True)\n","\n","    self.attn_c_linear = nn.Linear(in_features = self.units, out_features = self.units, bias = False)\n","    self.attn_ce_linear = nn.Linear(in_features = self.trans_units, out_features = 2 * self.units, bias = False)\n","    self.attn_co_linear = nn.Linear(in_features = 2 * self.trans_units, out_features = 2 * self.units, bias = False)\n","    self.attn_ct_linear = nn.Linear(in_features = self.trans_units, out_features = 2 * self.units, bias = False)\n","\n","    self.context_linear = nn.Linear(in_features = 4 * self.units, out_features = self.units, bias = False)\n","\n","\n","    self.logits_linear = nn.Linear(in_features = self.units, out_features = self.symbols)\n","    self.selector_linear = nn.Linear(in_features = self.units, out_features = 3)\n","\n","  def forward(self, batch_data):\n","    query_text = batch_data['query_text']\n","    answer_text = batch_data['answer_text']\n","    local_entity = batch_data['local_entity']\n","    responses_length = batch_data['responses_length']\n","    q2e_adj_mat = batch_data['q2e_adj_mat']\n","    kb_adj_mat = batch_data['kb_adj_mat']\n","    kb_fact_rel = batch_data['kb_fact_rel']\n","    match_entity_one_hop = batch_data['match_entity_one_hop']\n","    only_two_entity = batch_data['only_two_entity']\n","    match_entity_only_two = batch_data['match_entity_only_two']\n","    one_two_triples_id = batch_data['one_two_triples_id']\n","    local_entity_length = batch_data['local_entity_length']\n","    only_two_entity_length = batch_data['only_two_entity_length']\n","\n","    if self.is_inference == True:\n","        word2id = batch_data['word2id']\n","        entity2id = batch_data['entity2id']\n","        id2entity = dict()\n","        for key in entity2id.keys():\n","            id2entity[entity2id[key]] = key\n","    else:\n","        id2entity = None\n","\n","    batch_size, max_local_entity = local_entity.shape\n","    _, max_only_two_entity = only_two_entity.shape\n","    _, one_two_triple_num, one_two_triple_len, _ = one_two_triples_id.shape\n","    _, max_fact = kb_fact_rel.shape\n","\n","    # numpy to tensor\n","    local_entity = use_cuda(Variable(torch.from_numpy(local_entity).type('torch.LongTensor'), requires_grad=False))\n","    local_entity_mask = use_cuda((local_entity != 0).type('torch.FloatTensor'))\n","    kb_fact_rel = use_cuda(Variable(torch.from_numpy(kb_fact_rel).type('torch.LongTensor'), requires_grad=False))\n","    query_text = use_cuda(Variable(torch.from_numpy(query_text).type('torch.LongTensor'), requires_grad=False))\n","    answer_text = use_cuda(Variable(torch.from_numpy(answer_text).type('torch.LongTensor'), requires_grad=False))\n","    responses_length = use_cuda(Variable(torch.Tensor(responses_length).type('torch.LongTensor'), requires_grad=False))\n","    query_mask = use_cuda((query_text != 0).type('torch.FloatTensor'))\n","    match_entity_one_hop = use_cuda(Variable(torch.from_numpy(match_entity_one_hop).type('torch.LongTensor'), requires_grad=False))\n","    only_two_entity = use_cuda(Variable(torch.from_numpy(only_two_entity).type('torch.LongTensor'), requires_grad=False))\n","    match_entity_only_two = use_cuda(Variable(torch.from_numpy(match_entity_only_two).type('torch.LongTensor'), requires_grad=False))\n","    one_two_triples_id = use_cuda(Variable(torch.from_numpy(one_two_triples_id).type('torch.LongTensor'), requires_grad=False))\n","\n","\n","    decoder_len = answer_text.shape[1]\n","    encoder_len = query_text.shape[1]\n","    responses_target = answer_text\n","    responses_id = torch.cat((use_cuda(torch.ones([batch_size, 1], device=device).type('torch.LongTensor')),torch.split(answer_text, [decoder_len - 1, 1], 1)[0]), 1)\n","\n","    # ★☆★☆★☆★☆★☆★☆★☆★☆★☆★☆★☆★☆★☆★☆\n","    # encode central graph\n","    # print('batch_size', batch_size)\n","    # print('max_local_entity', max_local_entity)\n","    # print('max_fact',max_fact)\n","    # print('query_text',query_text)\n","    # print('local_entity',local_entity)\n","    # print('q2e_adj_mat',q2e_adj_mat)\n","    # print('kb_adj_mat', kb_adj_mat)\n","    # print('kb_fact_rel',kb_fact_rel)\n","    # print('query_mask', query_mask)\n","\n","    local_entity_emb = self.CentralEncoder(batch_size, max_local_entity, max_fact, query_text, local_entity, q2e_adj_mat, kb_adj_mat, kb_fact_rel, query_mask)\n","    local_entity_emb = local_entity_emb.to(device=device)\n","    # print('local_entity_emb',local_entity_emb)\n","\n","    # encode text\n","    text_encoder_input = self.WordEmbedding(query_text)\n","    text_encoder_output, text_encoder_state = self.text_encoder(text_encoder_input, use_cuda(Variable(torch.zeros(self.layers, batch_size, self.units))))\n","\n","    # encode outer graph\n","    one_two_embed = self.OuterEncoder(batch_size, one_two_triples_id, one_two_triple_num)\n","    one_two_embed = one_two_embed.to(device=device)\n","    # prepare decoder input for training\n","    decoder_input = self.WordEmbedding(responses_id)\n","\n","    # attention key and values\n","    c_attention_keys = self.attn_c_linear(text_encoder_output)\n","    c_attention_values = text_encoder_output\n","    ce_attention_keys, ce_attention_values = torch.split(self.attn_ce_linear(local_entity_emb), [self.units, self.units], 2)\n","    co_attention_keys, co_attention_values = torch.split(self.attn_co_linear(one_two_embed), [self.units, self.units], 2)\n","    only_two_entity_embed = self.EntityEmbedding(only_two_entity)\n","    ct_attention_keys, ct_attention_values = torch.split(self.attn_ct_linear(only_two_entity_embed), [self.units, self.units], 2)\n","\n","\n","    decoder_state = text_encoder_state\n","    decoder_output = use_cuda(torch.empty(0))\n","    ce_alignments = use_cuda(torch.empty(0))\n","    co_alignments = use_cuda(torch.empty(0))\n","    ct_alignments = use_cuda(torch.empty(0))\n","\n","    # central entity mask\n","    local_entity_mask = np.zeros([batch_size, local_entity.shape[1]])\n","    for i in range(batch_size):\n","        local_entity_mask[i][0:local_entity_length[i]] = 1\n","    local_entity_mask = use_cuda(torch.from_numpy(local_entity_mask).type('torch.LongTensor'))\n","\n","    # two-hop entity mask\n","    only_two_entity_mask = np.zeros([batch_size, only_two_entity.shape[1]])\n","    for i in range(batch_size):\n","        only_two_entity_mask[i][0:only_two_entity_length[i]] = 1\n","    only_two_entity_mask = use_cuda(torch.from_numpy(only_two_entity_mask).type('torch.LongTensor'))\n","\n","    context = use_cuda(torch.zeros([batch_size, self.units]))\n","\n","    if not self.is_inference:\n","        for t in range(decoder_len):\n","            decoder_input_t = torch.cat((decoder_input[:,t,:], context), 1).unsqueeze(1)\n","\n","            decoder_output_t, decoder_state = self.decoder(decoder_input_t, decoder_state)\n","            context, ce_alignments_t, co_alignments_t, ct_alignments_t = self.attention(c_attention_keys, c_attention_values, \\\n","                ce_attention_keys, ce_attention_values, co_attention_keys, co_attention_values, ct_attention_keys, \\\n","                decoder_output_t.squeeze(1), local_entity_mask, only_two_entity_mask)\n","            decoder_output_t = context.unsqueeze(1)\n","            ce_alignments = torch.cat((ce_alignments, ce_alignments_t.unsqueeze(1)), 1)\n","\n","            co_alignments = torch.cat((co_alignments, co_alignments_t.unsqueeze(1)), 1)\n","            decoder_output = torch.cat((decoder_output, decoder_output_t), 1)\n","            ct_alignments = torch.cat((ct_alignments, ct_alignments_t.unsqueeze(1)), 1)\n","\n","    else:\n","        word_index = use_cuda(torch.empty(0).type('torch.LongTensor'))\n","        decoder_input_t = self.WordEmbedding(use_cuda(torch.ones([batch_size]).type('torch.LongTensor')))\n","        context = use_cuda(torch.zeros([batch_size, self.units]))\n","        decoder_state = text_encoder_state\n","        selector = use_cuda(torch.empty(0).type('torch.LongTensor'))\n","\n","        for t in range(decoder_len):\n","            decoder_input_t = torch.cat((decoder_input_t, context), 1).unsqueeze(1)\n","            decoder_output_t, decoder_state = self.decoder(decoder_input_t, decoder_state)\n","            context, ce_alignments_t, co_alignments_t, ct_alignments_t = self.attention(c_attention_keys, c_attention_values, \\\n","                ce_attention_keys, ce_attention_values, co_attention_keys, co_attention_values, ct_attention_keys, \\\n","                decoder_output_t.squeeze(1), local_entity_mask, only_two_entity_mask)\n","            ct_alignments = torch.cat((ct_alignments, ct_alignments_t.unsqueeze(1)), 1)\n","            decoder_output_t = context.unsqueeze(1)\n","\n","            decoder_input_t, word_index_t, selector_t = self.inference(decoder_output_t, ce_alignments_t, ct_alignments_t, word2id, \\\n","                local_entity, only_two_entity, id2entity)\n","            word_index = torch.cat((word_index, word_index_t.unsqueeze(1)), 1)\n","            selector = torch.cat((selector, selector_t.unsqueeze(1)), 1)\n","\n","    decoder_mask = np.zeros([batch_size, decoder_len])\n","    for i in range(batch_size):\n","        decoder_mask[i][0:responses_length[i]] = 1\n","    decoder_mask = use_cuda(torch.from_numpy(decoder_mask).type('torch.LongTensor'))\n","\n","    one_hot_entities_local = use_cuda(torch.zeros(batch_size, decoder_len, max_local_entity))\n","    for b in range(batch_size):\n","        for d in range(decoder_len):\n","            if match_entity_one_hop[b][d] == -1:\n","                continue\n","            else:\n","                one_hot_entities_local[b][d][match_entity_one_hop[b][d]] = 1\n","\n","    use_entities_local = torch.sum(one_hot_entities_local, [2])\n","\n","    one_hot_entities_only_two = use_cuda(torch.zeros(batch_size, decoder_len, max_only_two_entity))\n","    for b in range(batch_size):\n","        for d in range(decoder_len):\n","            if match_entity_only_two[b][d] == -1:\n","                continue\n","            else:\n","                one_hot_entities_only_two[b][d][match_entity_only_two[b][d]] = 1\n","\n","    use_entities_only_two = torch.sum(one_hot_entities_only_two, [2])\n","\n","    if not self.is_inference:\n","        decoder_loss, ppx_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, \\\n","            word_neg_num, local_neg_num, only_two_neg_num = self.total_loss(decoder_output, responses_target, decoder_mask, \\\n","            ce_alignments, ct_alignments, use_entities_local, one_hot_entities_local, use_entities_only_two, one_hot_entities_only_two)\n","\n","    if self.is_select:\n","        self.sort(id2entity, ct_alignments, only_two_entity)\n","\n","    if self.is_inference == True:\n","        return word_index.cpu().numpy().tolist(), selector.cpu().numpy().tolist()\n","    return decoder_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, word_neg_num, local_neg_num, only_two_neg_num\n","\n","  def sort(self, id2entity, ct_alignments, only_two_entity):\n","    only_two_score = torch.sum(ct_alignments, 1)\n","    _, sort_local_index = only_two_score.sort(1)\n","    sort_global_index = torch.gather(only_two_entity, 1, sort_local_index)\n","    sort_global_index = sort_global_index.cpu().numpy().tolist()\n","\n","    sort_str = []\n","    for i in range(len(sort_global_index)):\n","      tmp = []\n","      for j in range(len(sort_global_index[i])):\n","        if sort_global_index[i][j] == 1:\n","          continue\n","        tmp.append(id2entity[sort_global_index[i][j]])\n","      sort_str.append(tmp)\n","\n","    sort_f = open('selected_concept.txt','a')\n","    for line in sort_str:\n","      sort_f.write(str(line) + '\\n')\n","    sort_f.close()\n","\n","\n","  def inference(self, decoder_output_t, ce_alignments_t, ct_alignments_t, word2id, local_entity, only_two_entity, id2entity):\n","    batch_size = decoder_output_t.shape[0]\n","\n","    logits = self.logits_linear(decoder_output_t.squeeze(1)) # batch * num_symbols\n","\n","    selector = self.softmax_d1(self.selector_linear(decoder_output_t.squeeze(1)))\n","\n","    (word_prob, word_t) = torch.max(selector[:,0].unsqueeze(1) * self.softmax_d1(logits), dim = 1)\n","    (local_entity_prob, local_entity_l_index_t) = torch.max(selector[:,1].unsqueeze(1) * ce_alignments_t, dim = 1)\n","    (only_two_entity_prob, only_two_entity_l_index_t) = torch.max(selector[:,2].unsqueeze(1) * ct_alignments_t, dim = 1)\n","\n","    selector[:,0] = selector[:,0] * word_prob\n","    selector[:,1] = selector[:,1] * local_entity_prob\n","    selector[:,2] = selector[:,2] * only_two_entity_prob\n","    selector = torch.argmax(selector, dim = 1)\n","\n","    local_entity_l_index_t = local_entity_l_index_t.cpu().numpy().tolist()\n","    only_two_entity_l_index_t = only_two_entity_l_index_t.cpu().numpy().tolist()\n","    word_t = word_t.cpu().numpy().tolist()\n","\n","    word_local_entity_t = []\n","    word_only_two_entity_t = []\n","    word_index_final_t = []\n","    for i in range(batch_size):\n","        if selector[i] == 0:\n","            word_index_final_t.append(word_t[i])\n","            continue\n","        if selector[i] == 1:\n","            local_entity_index_t = int(local_entity[i][local_entity_l_index_t[i]])\n","            local_entity_text = id2entity[local_entity_index_t]\n","            if local_entity_text not in word2id:\n","                local_entity_text = '_UNK'\n","            word_index_final_t.append(word2id[local_entity_text])\n","            continue\n","        if selector[i] == 2:\n","            only_two_entity_index_t = int(only_two_entity[i][only_two_entity_l_index_t[i]])\n","            only_two_entity_text = id2entity[only_two_entity_index_t]\n","            if only_two_entity_text not in word2id:\n","                only_two_entity_text = '_UNK'\n","            word_index_final_t.append(word2id[only_two_entity_text])\n","            continue\n","\n","    word_index_final_t = use_cuda(torch.LongTensor(word_index_final_t))\n","    decoder_input_t = self.WordEmbedding(word_index_final_t)\n","\n","    return decoder_input_t, word_index_final_t, selector\n","\n","  def total_loss(self, decoder_output, responses_target, decoder_mask, ce_alignments, ct_alignments, use_entities_local, \\\n","        entity_targets_local, use_entities_only_two, entity_targets_only_two):\n","    batch_size = decoder_output.shape[0]\n","    decoder_len = responses_target.shape[1]\n","\n","    local_masks = use_cuda(decoder_mask.reshape([-1]).type(\"torch.FloatTensor\"))\n","    local_masks_word = use_cuda((1 - use_entities_local - use_entities_only_two).reshape([-1]).type(\"torch.FloatTensor\")) * local_masks\n","    local_masks_local = use_cuda(use_entities_local.reshape([-1]).type(\"torch.FloatTensor\"))\n","    local_masks_only_two = use_cuda(use_entities_only_two.reshape([-1]).type(\"torch.FloatTensor\"))\n","    logits = self.logits_linear(decoder_output) #batch * decoder_len * num_symbols\n","\n","    word_prob = torch.gather(self.softmax_d2(logits), 2, responses_target.unsqueeze(2)).squeeze(2)\n","\n","    selector_word, selector_local, selector_only_two = torch.split(self.softmax_d2(self.selector_linear(decoder_output)), [1, 1, 1], 2) #batch_size * decoder_len * 1\n","    selector_word = selector_word.squeeze(2)\n","    selector_local = selector_local.squeeze(2)\n","    selector_only_two = selector_only_two.squeeze(2)\n","\n","    entity_prob_local = torch.sum(ce_alignments * entity_targets_local, [2])\n","    entity_prob_only_two = torch.sum(ct_alignments * entity_targets_only_two, [2])\n","\n","    ppx_prob = word_prob * (1 - use_entities_local - use_entities_only_two) + entity_prob_local * use_entities_local + entity_prob_only_two * use_entities_only_two\n","    ppx_word = word_prob * (1 - use_entities_local - use_entities_only_two)\n","    ppx_local = entity_prob_local * use_entities_local\n","    ppx_only_two = entity_prob_only_two * use_entities_only_two\n","\n","    final_prob = word_prob * selector_word * (1 - use_entities_local - use_entities_only_two) + entity_prob_local * selector_local * \\\n","        use_entities_local + entity_prob_only_two * selector_only_two * use_entities_only_two\n","\n","    final_loss = torch.sum(- torch.log(1e-12 + final_prob).reshape([-1]) * local_masks)\n","\n","    sentence_ppx = torch.sum((- torch.log(1e-12 + ppx_prob).reshape([-1]) * local_masks).reshape([batch_size, -1]), 1)\n","    sentence_ppx_word = torch.sum((- torch.log(1e-12 + ppx_word).reshape([-1]) * local_masks_word).reshape([batch_size, -1]), 1)\n","    sentence_ppx_local = torch.sum((- torch.log(1e-12 + ppx_local).reshape([-1]) * local_masks_local).reshape([batch_size, -1]), 1)\n","    sentence_ppx_only_two = torch.sum((- torch.log(1e-12 + ppx_only_two).reshape([-1]) * local_masks_only_two).reshape([batch_size, -1]), 1)\n","\n","    selector_loss = torch.sum(- torch.log(1e-12 + selector_local * use_entities_local + selector_only_two * use_entities_only_two + \\\n","        selector_word * (1 - use_entities_local - use_entities_only_two)).reshape([-1]) * local_masks)\n","\n","    loss = final_loss + selector_loss\n","    total_size = torch.sum(local_masks)\n","    total_size += 1e-12\n","\n","    sum_word = torch.sum(use_cuda(((1 - use_entities_local - use_entities_only_two) * use_cuda(decoder_mask.type(\"torch.FloatTensor\"))).type(\"torch.FloatTensor\")), 1)\n","    sum_local = torch.sum(use_cuda(use_entities_local.type(\"torch.FloatTensor\")), 1)\n","    sum_only_two= torch.sum(use_cuda(use_entities_only_two.type(\"torch.FloatTensor\")), 1)\n","\n","    word_neg_mask = use_cuda((sum_word == 0).type(\"torch.FloatTensor\"))\n","    local_neg_mask = use_cuda((sum_local == 0).type(\"torch.FloatTensor\"))\n","    only_two_neg_mask = use_cuda((sum_only_two == 0).type(\"torch.FloatTensor\"))\n","\n","    word_neg_num = torch.sum(word_neg_mask)\n","    local_neg_num = torch.sum(local_neg_mask)\n","    only_two_neg_num = torch.sum(only_two_neg_mask)\n","\n","    sum_word = sum_word + word_neg_mask\n","    sum_local = sum_local + local_neg_mask\n","    sum_only_two = sum_only_two + only_two_neg_mask\n","\n","    return loss / total_size, 0, sentence_ppx / torch.sum(use_cuda(decoder_mask.type(\"torch.FloatTensor\")), 1), \\\n","        sentence_ppx_word / sum_word, sentence_ppx_local / sum_local, sentence_ppx_only_two / sum_only_two, word_neg_num, \\\n","        local_neg_num, only_two_neg_num\n","\n","\n","\n","  def attention(self, c_attention_keys, c_attention_values, ce_attention_keys, ce_attention_values, co_attention_keys, \\\n","        co_attention_values, ct_attention_keys, decoder_state, local_entity_mask, only_two_entity_mask):\n","    batch_size = ct_attention_keys.shape[0]\n","    only_two_len = ct_attention_keys.shape[1]\n","\n","    c_query = decoder_state.reshape([-1, 1, self.units])\n","    ce_query = decoder_state.reshape([-1, 1, self.units])\n","    co_query = decoder_state.reshape([-1, 1, self.units])\n","    ct_query = decoder_state.reshape([-1, 1, self.units])\n","\n","    c_scores = torch.sum(c_attention_keys * c_query, 2)\n","    ce_scores = torch.sum(ce_attention_keys * ce_query, 2)\n","    co_scores = torch.sum(co_attention_keys * co_query, 2)\n","    ct_scores = torch.sum(ct_attention_keys * ct_query, 2)\n","\n","    c_alignments = self.softmax_d1(c_scores)\n","    ce_alignments = self.softmax_d1(ce_scores)\n","    co_alignments = self.softmax_d1(co_scores)\n","    ct_alignments = self.softmax_d1(ct_scores)\n","\n","    ce_alignments = ce_alignments * use_cuda(local_entity_mask.type(\"torch.FloatTensor\"))\n","    ct_alignments = ct_alignments * use_cuda(only_two_entity_mask.type(\"torch.FloatTensor\"))\n","\n","    c_context = torch.sum(c_alignments.unsqueeze(2) * c_attention_values, 1)\n","    ce_context = torch.sum(ce_alignments.unsqueeze(2) * ce_attention_values, 1)\n","    co_context = torch.sum(co_alignments.unsqueeze(2) * co_attention_values, 1)\n","\n","    context = self.context_linear(torch.cat((decoder_state, c_context, ce_context, co_context), 1))\n","\n","    return context, ce_alignments, co_alignments, ct_alignments"]}]}