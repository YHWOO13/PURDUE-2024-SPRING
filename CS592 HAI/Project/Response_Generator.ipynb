{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25202,
     "status": "ok",
     "timestamp": 1713337033090,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "fpfbjtB_cyb-",
    "outputId": "f3a6793f-368e-4fe8-ec6b-053c1852636b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3282,
     "status": "ok",
     "timestamp": 1713337046497,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "xB_2nqyZbg1e"
   },
   "outputs": [],
   "source": [
    "#coding:utf-8\n",
    "import numpy as np\n",
    "import json\n",
    "# from model import ConceptFlow, use_cuda\n",
    "# from preprocession import prepare_data, build_vocab, gen_batched_data\n",
    "import torch\n",
    "import warnings\n",
    "import yaml\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import gensim.downloader as api\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.nn import utils as nn_utils\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1713337046498,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "2sqEj4IOyaXu"
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 3038,
     "status": "ok",
     "timestamp": 1713337049533,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "GFLPEJdWpZwl",
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `\"'/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592.py\"` not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\utils\\path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[1;34m(name, force_win32)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'File `%r` not found.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: File `\"'/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592.py\"` not found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b7b96f311843>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"'/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/Central.ipynb'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"'/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/Embedding.ipynb'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"'/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/Outer.ipynb'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"'/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/Conceptflow.ipynb'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"'/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/Preprocession.ipynb'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2342\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2344\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2345\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\decorator.py\u001b[0m in \u001b[0;36mfun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    712\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nt'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"^'.*'$\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    715\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: File `\"'/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592.py\"` not found."
     ]
    }
   ],
   "source": [
    "%run '/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/Central.ipynb'\n",
    "%run '/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/Embedding.ipynb'\n",
    "%run '/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/Outer.ipynb'\n",
    "%run '/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/Conceptflow.ipynb'\n",
    "%run '/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/Preprocession.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"D:\\\\13.PURDUE\\\\Data\\\\ConceptFlow(ECCF)_data\"\n",
    "with open('%s/resource.txt' % data_dir) as f:\n",
    "    d = json.loads(f.readline())\n",
    "\n",
    "    csk_triples = d['csk_triples']\n",
    "    csk_entities = d['csk_entities']\n",
    "    raw_vocab = d['vocab_dict']\n",
    "    kb_dict = d['dict_csk']\n",
    "    dict_csk_entities = d['dict_csk_entities']\n",
    "    dict_csk_triples = d['dict_csk_triples']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New post preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entities set()\n",
      "triples []\n",
      "entities {'ειναι', 'ما_هذا', 'ドス', 'wast', 'has', 'oes', 'bes', 'е', 'でげす', 'inar', 'ざり', 'です', 'már_késő_van', \"where's\", 'isness', 'зарем', 'いらっしゃる', 'je', 'ada_apa', 'mura', 'er', 'e', 'cérb', 'nĩ', 'es', 'found_in_subway', 'αληθώσ_ανέστη', 'id', 'sta', 'qu_est_ce_que_c_est', 'っす', \"she'sn't\", \"who's\", 'يكون', 'だ', 'で御座ります', 'どす', 'is', 'expensive', 'ei_ole', 'v', 'é', 'trínarb', 'ialah', 'está', 'a', 'en_1', 'bilgisayarsa', '3', '𐌹𐍃𐍄', 'で御座います', 'beest', 'r', 'infinite_stratos', 'n', 'tabansızsa', '𐬀𐬯𐬙𐬌', 'じゃ', 'ist', 'ってのは', 'єстъ', '是', 'є', 'ónar', 'evse', 'thì', 'isentropic', 'being', 'jeste', 'と言うのは', 'adalah', 'átʼéii', 'yw', 'IS_(Infinite_Stratos)', 'your+name+is+mud-p#Component-3', 'jest', 'でごんす', 'beeth', 'これはなんですか', 'dar', 'computing', \"it's\"}\n",
      "triples [('どす', 'Synonym', 'is'), ('n', 'Synonym', 'is'), ('en_1', 'RelatedTo', 'is'), ('ざり', 'Synonym', 'is'), ('is', 'Synonym', 'jest'), ('ドス', 'Synonym', 'is'), ('いらっしゃる', 'Synonym', 'is'), ('is', 'Synonym', 'jeste'), ('v', 'Synonym', 'is'), ('ってのは', 'Synonym', 'is'), ('でごんす', 'Synonym', 'is'), ('だ', 'Synonym', 'is'), ('3', 'Synonym', 'is'), ('でげす', 'Synonym', 'is'), ('っす', 'Synonym', 'is'), ('で御座ります', 'Synonym', 'is'), ('is', 'Synonym', 'je'), ('で御座います', 'Synonym', 'is'), ('です', 'Synonym', 'is'), ('n', 'HasContext', 'computing'), ('と言うのは', 'Synonym', 'is'), ('v', 'Synonym', 'is'), ('is', 'Synonym', '是'), ('átʼéii', 'RelatedTo', 'is'), ('bilgisayarsa', 'RelatedTo', 'is'), ('is', 'Synonym', 'je'), ('is', 'Synonym', '𐌹𐍃𐍄'), ('már_késő_van', 'RelatedTo', 'is'), ('is', 'DerivedFrom', 'n'), ('is', 'Synonym', 'thì'), ('ei_ole', 'RelatedTo', 'is'), ('これはなんですか', 'RelatedTo', 'is'), ('beeth', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('is', 'Synonym', 'oes'), (\"where's\", 'RelatedTo', 'is'), ('r', 'RelatedTo', 'is'), ('is', 'CapableOf', 'found_in_subway'), ('зарем', 'RelatedTo', 'is'), ('r', 'RelatedTo', 'is'), ('ónar', 'RelatedTo', 'is'), ('wast', 'RelatedTo', 'is'), ('is', 'Synonym', 'es'), ('is', 'Synonym', 'is'), ('tabansızsa', 'RelatedTo', 'is'), ('isentropic', 'DerivedFrom', 'is'), (\"it's\", 'RelatedTo', 'is'), ('is', 'Synonym', 'bes'), ('ialah', 'RelatedTo', 'is'), ('is', 'Synonym', 'يكون'), ('v', 'RelatedTo', 'is'), ('is', 'Synonym', 'じゃ'), (\"she'sn't\", 'RelatedTo', 'is'), ('infinite_stratos', 'ExternalURL', 'IS_(Infinite_Stratos)'), ('a', 'RelatedTo', 'is'), ('ada_apa', 'RelatedTo', 'is'), ('n', 'RelatedTo', 'is'), ('is', 'Synonym', 'е'), ('is', 'Synonym', 'está'), ('trínarb', 'RelatedTo', 'is'), ('id', 'Synonym', 'is'), ('evse', 'RelatedTo', 'is'), ('is', 'Synonym', 'e'), ('is', 'Synonym', 'er'), ('being', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('dar', 'RelatedTo', 'is'), ('is', 'Synonym', 'ειναι'), ('inar', 'RelatedTo', 'is'), ('is', 'Synonym', 'ist'), ('isness', 'DerivedFrom', 'is'), ('mura', 'RelatedTo', 'is'), ('是', 'Synonym', 'is'), ('αληθώσ_ανέστη', 'RelatedTo', 'is'), ('is', 'Synonym', 'yw'), ('is', 'Synonym', 'is'), ('ما_هذا', 'RelatedTo', 'is'), ('is', 'ExternalURL', 'your+name+is+mud-p#Component-3'), ('v', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), (\"who's\", 'RelatedTo', 'is'), ('beest', 'RelatedTo', 'is'), ('is', 'Synonym', '𐬀𐬯𐬙𐬌'), ('adalah', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('is', 'Synonym', 'є'), ('is', 'CapableOf', 'expensive'), ('is', 'Synonym', 'é'), ('v', 'RelatedTo', 'is'), ('is', 'Synonym', 'sta'), ('has', 'SimilarTo', 'is'), ('is', 'Synonym', 'єстъ'), ('nĩ', 'RelatedTo', 'is'), ('v', 'RelatedTo', 'is'), ('qu_est_ce_que_c_est', 'RelatedTo', 'is'), ('v', 'Synonym', 'is'), ('cérb', 'RelatedTo', 'is')]\n",
      "entities {'related', '相关', 'family', 'anthropology', 'connexe', 'unrelated', 'linguistics', 'thunder', 'n', 'wn', 'relative', 'structure_and_strategy', '有縁', 'a', 'v', 'interrelated', 'nectendus'}\n",
      "triples [('family', 'RelatedTo', 'related'), ('anthropology', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'RelatedTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('有縁', 'Synonym', 'related'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('a', 'RelatedTo', 'related'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'RelatedTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('n', 'Synonym', 'related'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('n', 'Synonym', 'related'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'linguistics'), ('wn', 'SimilarTo', 'wn'), ('wn', 'RelatedTo', 'wn'), ('linguistics', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'SimilarTo', 'anthropology'), ('wn', 'Synonym', 'wn'), ('wn', 'SimilarTo', 'wn'), ('wn', 'Synonym', 'wn'), ('wn', 'RelatedTo', 'wn'), ('wn', 'SimilarTo', 'wn'), ('a', 'RelatedTo', 'related'), ('n', 'Synonym', 'related'), ('wn', 'Synonym', 'wn'), ('thunder', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'related'), ('v', 'RelatedTo', 'related'), ('interrelated', 'DerivedFrom', 'related'), ('a', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'connexe'), ('a', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'related'), ('unrelated', 'DerivedFrom', 'related'), ('a', 'RelatedTo', 'related'), ('nectendus', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'related'), ('v', 'RelatedTo', 'related'), ('a', 'RelatedTo', 'relative'), ('n', 'RelatedTo', 'related'), ('相关', 'Synonym', 'related'), ('structure_and_strategy', 'ReceivesAction', 'related'), ('a', 'RelatedTo', 'related'), ('n', 'RelatedTo', 'related'), ('n', 'RelatedTo', 'related')]\n",
      "entities {'cis', 'に取って', '乗せる', 'të', 'til', 'go', 'te', '徹す', 'put', 'into', 'naartoe', 'a', 'un', 'მდე', 'tu', 'unto', 'open', 'stop_standing_in', 'abys', 'ให้', 'grație', 'kepada', 'に対する', '1', 'do', 'hitherto', 'en_2', 'na', '迄', 'کو', '載せる', 'r', '対', 'n', '迄も', 'ad', 'enjoy_company_of_friends', \"so's\", 'thitherto', 'առ', 'be', 'da', 'u', 'ok', 'से', 'bread', 'apud', 'usque', 'にかけて', 'onto', 'za', '通す', 'に取りまして', '透す', 'prema', 'k', 'en_1', 'come', 'to', 'v', 'other+fish+to+fry-p#Component-3'}\n",
      "triples [('に取って', 'Synonym', 'to'), ('ad', 'RelatedTo', 'to'), ('da', 'RelatedTo', 'to'), ('徹す', 'Synonym', 'to'), ('առ', 'RelatedTo', 'to'), ('put', 'RelatedTo', 'to'), ('onto', 'DerivedFrom', 'to'), ('a', 'RelatedTo', 'to'), ('za', 'RelatedTo', 'to'), ('に取りまして', 'Synonym', 'to'), ('come', 'DerivedFrom', 'to'), ('迄も', 'Synonym', 'to'), ('v', 'Synonym', 'to'), ('にかけて', 'Synonym', 'to'), ('v', 'RelatedTo', 'to'), ('通す', 'Synonym', 'to'), ('透す', 'Synonym', 'to'), ('v', 'Synonym', 'to'), ('k', 'RelatedTo', 'to'), ('n', 'RelatedTo', 'to'), ('to', 'EtymologicallyDerivedFrom', 'to'), ('乗せる', 'Synonym', 'to'), ('do', 'DerivedFrom', 'to'), ('to', 'Synonym', 'a'), ('be', 'DerivedFrom', 'to'), ('bread', 'DerivedFrom', 'to'), ('un', 'RelatedTo', 'to'), ('to', 'EtymologicallyDerivedFrom', 'to'), ('til', 'RelatedTo', 'to'), ('v', 'Synonym', 'to'), ('na', 'RelatedTo', 'to'), ('ad', 'RelatedTo', 'to'), ('対', 'Synonym', 'to'), ('unto', 'DerivedFrom', 'to'), ('ad', 'RelatedTo', 'to'), ('do', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('prema', 'RelatedTo', 'to'), ('u', 'RelatedTo', 'to'), ('迄', 'Synonym', 'to'), ('1', 'Synonym', 'to'), ('v', 'Synonym', 'to'), ('into', 'DerivedFrom', 'to'), ('1', 'Synonym', 'to'), ('hitherto', 'DerivedFrom', 'to'), ('に対する', 'Synonym', 'to'), ('載せる', 'Synonym', 'to'), ('მდე', 'RelatedTo', 'to'), ('to', 'Synonym', 'tu'), ('v', 'RelatedTo', 'to'), ('n', 'RelatedTo', 'to'), ('thitherto', 'DerivedFrom', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('n', 'RelatedTo', 'to'), ('से', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('a', 'RelatedTo', 'to'), ('abys', 'Synonym', 'to'), ('to', 'ExternalURL', 'other+fish+to+fry-p#Component-3'), ('to', 'Synonym', 'të'), ('n', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('n', 'HasContext', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('grație', 'RelatedTo', 'to'), ('tu', 'RelatedTo', 'to'), ('r', 'Synonym', 'to'), ('a', 'RelatedTo', 'to'), ('cis', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('to', 'Synonym', 'te'), ('v', 'RelatedTo', 'to'), ('en_2', 'RelatedTo', 'to'), ('ok', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('r', 'RelatedTo', 'to'), ('apud', 'RelatedTo', 'to'), ('usque', 'Synonym', 'to'), ('naartoe', 'RelatedTo', 'to'), ('ให้', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), (\"so's\", 'RelatedTo', 'to'), ('n', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('en_1', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('r', 'Antonym', 'open'), ('کو', 'RelatedTo', 'to'), ('kepada', 'RelatedTo', 'to'), ('n', 'RelatedTo', 'to'), ('go', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('v', 'RelatedTo', 'to'), ('enjoy_company_of_friends', 'MotivatedByGoal', 'to'), ('stop_standing_in', 'MotivatedByGoal', 'to')]\n",
      "entities {'coffee', 'made_from_coffee_beans', 'food', 'plant', 'popular_drink', 'mug', 'n', 'internet_cafe', 'served_hot', 'caffeine', 'coffee_shop', 'sugar', 'stimulant'}\n",
      "triples [('coffee', 'IsA', 'stimulant'), ('coffee', 'HasA', 'caffeine'), ('coffee', 'HasProperty', 'served_hot'), ('food', 'Synonym', 'food'), ('coffee', 'AtLocation', 'coffee_shop'), ('sugar', 'AtLocation', 'coffee'), ('mug', 'UsedFor', 'coffee'), ('coffee', 'ReceivesAction', 'made_from_coffee_beans'), ('sugar', 'RelatedTo', 'coffee'), ('food', 'Synonym', 'food'), ('plant', 'Synonym', 'plant'), ('plant', 'Synonym', 'plant'), ('n', 'RelatedTo', 'coffee'), ('n', 'Synonym', 'coffee'), ('food', 'Synonym', 'food'), ('plant', 'Synonym', 'plant'), ('coffee', 'IsA', 'popular_drink'), ('coffee', 'AtLocation', 'internet_cafe'), ('food', 'Synonym', 'food'), ('food', 'Synonym', 'food')]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "generate_response_format() missing 3 required positional arguments: 'post', 'entities', and 'triples'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-214-51dda23a3336>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# Example usage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"What is related to coffee?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-214-51dda23a3336>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(question)\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mall_triples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtriples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m     \u001b[0mformatted_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_response_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_entities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_triples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: generate_response_format() missing 3 required positional arguments: 'post', 'entities', and 'triples'"
     ]
    }
   ],
   "source": [
    "question = ''\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', questions)\n",
    "\n",
    "def query_conceptnet(keyword):\n",
    "    url = f\"http://api.conceptnet.io/c/en/{keyword}?limit=100\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "def extract_data(concept_data):\n",
    "    entities = set()\n",
    "    triples = []\n",
    "    for edge in concept_data['edges']:\n",
    "        start = edge['start']['@id'].split('/')[-1]\n",
    "        end = edge['end']['@id'].split('/')[-1]\n",
    "        rel = edge['rel']['@id'].split('/')[-1]\n",
    "        triples.append((start, rel, end))\n",
    "        entities.update([start, end])\n",
    "    return entities, triples\n",
    "\n",
    "\n",
    "# if res not in csk_entities:\n",
    "#     print('Come again? Could you ask again a little bit detail?')\n",
    "\n",
    "    \n",
    "def generate_response_format(dict_csk_entities, dict_csk_triples, post, entities, triples):\n",
    "    res = re.findall( r'\\w+|[^\\s\\w]+', post)\n",
    "\n",
    "    # Simulate entity and triple index mapping\n",
    "    entity2id = {entity: idx for idx, entity in enumerate(entities)}\n",
    "    triple_indices = 0  # Assuming each triple gets a unique index\n",
    "    post_triples = np.zeros(len(res))\n",
    "    \n",
    "#     for idx, split in enumerate(res):\n",
    "#         if split in dict_csk_entities:\n",
    "            \n",
    "        \n",
    "# #         else:\n",
    "# #             continue\n",
    "        \n",
    "            \n",
    "    \n",
    "    data_format = {\n",
    "        \"all_entities_one_hop\": list(entity2id.values()),\n",
    "        \"post_triples\": list(triple_indices),  # Example: every triple is used\n",
    "        \"post\": res,\n",
    "        \"response\": [\"This\", \"is\", \"an\", \"example\", \"response\", \".\"],\n",
    "        \"match_response_index_one_hop\": [-1] * len(entity2id),  # Simplified example\n",
    "        \"only_two\": list(entity2id.values())[::2],  # Every second entity for example\n",
    "        \"match_response_index_only_two\": [-1] * len(entity2id),  # Simplified example\n",
    "        \"all_triples_one_hop\": list(triple_indices)  # Simplified example\n",
    "    }\n",
    "    return data_format\n",
    "\n",
    "def main(question):\n",
    "    keywords = question.split()  # Simple keyword extraction from the question\n",
    "    all_entities = set()\n",
    "    all_triples = []\n",
    "\n",
    "    for keyword in keywords:\n",
    "        concept_data = query_conceptnet(keyword)\n",
    "#         print('concept_data',concept_data)\n",
    "        entities, triples = extract_data(concept_data)\n",
    "        print('entities', entities)\n",
    "        print('triples', triples)\n",
    "        all_entities.update(entities)\n",
    "        all_triples.extend(triples)\n",
    "\n",
    "    formatted_data = generate_response_format(all_entities, all_triples)\n",
    "    print(formatted_data)\n",
    "\n",
    "# Example usage\n",
    "main(\"What is related to coffee?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = \"I wear glasses and at the age of 17 I would have been mortified to be seen in a photo wearing them.\"\n",
    "post2 = \"As the comet streaked across the Chicago sky, I felt as though time had looped back to the Wild West, where cowboys would shoot at such celestial wonders with their six-shooters.\"\n",
    "post3 = \"this fucking shit pisses me off to no end, when these fucking liberal hypocrites imply the only group of people capable of being racist are the whites.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_all_entities_one_hop & create_all_triples_one_hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post_entities ['fucking', 'shit', 'end', 'fucking', 'liberal', 'hypocrites', 'imply', 'group', 'capable', 'racist', 'whites']\n",
      "125\n",
      "[16196, 20496, 6055, 7523, 12453, 17645, 10558, 7770, 18128, 20994, 7740, 11761, 21894, 4786, 1185, 12549, 13720, 13719, 6491, 7572, 7362, 10411, 12220, 11105, 18812, 6678, 7887, 938, 15041, 5825, 4154, 12978, 12237, 19142, 9075, 2826, 21601, 21737, 1545, 16853, 21892, 14247, 3736, 12652, 1637, 21217, 14599, 2541, 20911, 9618, 4802, 13804, 5399, 20353, 19107, 15904, 19685, 6122, 942, 3571, 1606, 3487, 17142, 8545, 5192, 19351, 1974, 18311, 1802, 5562, 11938, 14895, 12380, 10576, 8695, 20626, 10083, 17116, 21079, 10252, 11450, 362, 5173, 9548, 2024, 4640, 6821, 21852, 19031, 659, 8360, 18713, 2846, 15433, 2889, 8841, 3078, 14950, 9310, 16988, 3164, 3159, 15063, 14042, 18869, 2002, 8488, 13565, 12866, 12811, 10057, 4518, 16111, 6384, 17607, 9474, 12896, 7829, 7766, 14614, 12509, 5800, 12441, 16320, 20026]\n",
      "297\n"
     ]
    }
   ],
   "source": [
    "csk_triples = csk_triples\n",
    "dict_csk_entities = dict_csk_entities\n",
    "csk_entities = csk_entities\n",
    "res = re.findall( r'\\w+|[^\\s\\w]+', post3)\n",
    "post_entities = []\n",
    "\n",
    "tmp_e = ''\n",
    "tmp_o = ''\n",
    "\n",
    "for i in res:\n",
    "    if i in csk_entities:\n",
    "        post_entities.append(i)\n",
    "    else:\n",
    "        continue\n",
    "print('post_entities',post_entities)\n",
    "\n",
    "\n",
    "# find one_hop_triples from post entities\n",
    "all_triples_one_hop = []\n",
    "\n",
    "# find one_hop_entities from post entities\n",
    "tmp_id_one_hop = []\n",
    "\n",
    "threshold = 0.24\n",
    "for e_o in csk_triples:\n",
    "    e_o_split = e_o.split(',')\n",
    "    for entity in post_entities:\n",
    "        tmp_e = e_o_split[0]\n",
    "        tmp_o = e_o_split[2].lstrip()\n",
    "        tmp_pair = [tmp_e, tmp_o]\n",
    "        if entity in tmp_pair:\n",
    "            if entity == tmp_pair[0] and tmp_pair[1] not in post_entities:\n",
    "                if entity in w2v_model.key_to_index and tmp_pair[1] in w2v_model.key_to_index: # check both of them in w2vec vocab\n",
    "                    sim_score = w2v_model.similarity(entity,tmp_pair[1])\n",
    "                    all_triples_one_hop.append(dict_csk_triples[e_o])\n",
    "                    if tmp_pair[1] in dict_csk_entities and sim_score>threshold:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair[1]])                    \n",
    "                else:\n",
    "                    all_triples_one_hop.append(dict_csk_triples[e_o])\n",
    "                    if tmp_pair[1] in dict_csk_entities:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair[1]])\n",
    "                        \n",
    "            elif entity == tmp_pair[1] and tmp_pair[0] not in post_entities:\n",
    "                if entity in w2v_model.key_to_index and tmp_pair[0] in w2v_model.key_to_index: # check both of them in w2vec vocab\n",
    "                    sim_score = w2v_model.similarity(entity,tmp_pair[0])\n",
    "                    if tmp_pair[0] in dict_csk_entities and sim_score>threshold:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "                        all_triples_one_hop.append(dict_csk_triples[e_o])\n",
    "                else:\n",
    "                    if tmp_pair[0] in dict_csk_entities:\n",
    "                        all_triples_one_hop.append(dict_csk_triples[e_o])\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair[0]])\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        tmp_e = []\n",
    "        tmp_o = []\n",
    "        tmp_pair = []\n",
    "\n",
    "id_one_hop = [i for n, i in enumerate(tmp_id_one_hop) if i not in tmp_id_one_hop[:n]]\n",
    "\n",
    "# print(id_one_hop)\n",
    "print(len(id_one_hop))\n",
    "print(id_one_hop)\n",
    "print(len(all_triples_one_hop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "[109, 18698, 1955, 17681, 17989, 589, 17252, 2231, 18661, 6617, 2581, 3097, 8527, 9411, 15859, 9619, 7784, 18870, 17092, 10552, 996, 1499, 14741, 9244, 19574, 15724, 17692, 849, 21573, 20739, 696, 10376, 5272, 1039, 12364, 16268, 20748, 20987, 16744, 16781, 16740, 14295, 17836, 16042, 7154, 20896, 5031, 10653, 13169, 4075, 12181, 7432, 8050, 15146, 20213, 5694, 720, 4272, 12060, 20754, 13236, 14496, 20717, 15662, 3983, 6654, 4606, 9448, 6881, 19949, 875, 13343, 21012, 12391, 20534, 334, 7738, 4889, 3041, 282, 18823, 11880, 11601, 14723, 6391, 15805, 9718, 15799, 2629, 17685, 6968, 6676, 14582, 15744, 18399, 13163, 17442, 14297, 943, 19511, 13866, 1483, 914, 9810, 21830, 12534, 13364, 7746, 2653, 13654, 13124, 10276, 1737, 15732, 1572, 7452, 9138, 19325, 21903, 17046, 536, 17723, 13903, 13927, 19294, 20039, 8947, 20237, 1771, 3047, 521, 8090, 17108, 1988, 20528, 12647, 9010, 18099, 6531, 17633, 18960, 17023, 17746, 5428, 14674, 18405, 21358, 13277, 3305, 8856, 22518, 8616, 1774, 5145, 3209, 2717, 12417, 6693, 1722, 16918, 9774, 9416, 18881, 7201, 269, 18655, 16090, 8410, 6665, 276, 21750, 3757, 9728, 13713, 13160, 18338]\n"
     ]
    }
   ],
   "source": [
    "tmp_e2 = ''\n",
    "tmp_o2 = ''\n",
    "tmp_pair2= []\n",
    "# find two hope from post entities\n",
    "lsk = ['loop, RelatedTo, chicago', 'shoot, RelatedTo, comet', 'dumb, RelatedTo, deaf', 'walk, RelatedTo, door', 'pier, RelatedTo, boat']\n",
    "# id2word = dict()\n",
    "# for key in word2id.keys():\n",
    "#     id2word[word2id[key]] = key\n",
    "swapped_dict = {value: key for key, value in dict_csk_entities.items()}\n",
    "entities_one_hop = []\n",
    "for i in id_one_hop:\n",
    "    entities_one_hop.append(swapped_dict[i])\n",
    "entities_one_hop2 = entities_one_hop[:5]\n",
    "threshold2 = 0.01\n",
    "tmp_entities_two_hop = []\n",
    "for e_o2 in csk_triples:\n",
    "    e_o2 = e_o2.split(',')\n",
    "    for entity2 in entities_one_hop:\n",
    "        tmp_e2 = e_o2[0]\n",
    "        tmp_o2 = e_o2[2].lstrip()\n",
    "        tmp_pair2 = [tmp_e2, tmp_o2]\n",
    "        if entity2 in tmp_pair2:\n",
    "            if entity2 == tmp_pair2[0] and tmp_pair2[1] not in post_entities and tmp_pair2[1] not in entities_one_hop:\n",
    "                if entity2 in w2v_model.key_to_index and tmp_pair2[1] in w2v_model.key_to_index: # check both of them in w2vec vocab\n",
    "                    sim_score2 = w2v_model.similarity(entity2,tmp_pair2[1])\n",
    "                    if tmp_pair2[1] in dict_csk_entities and sim_score2 > threshold2:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair2[1]])                  \n",
    "                    else:\n",
    "                        if tmp_pair2[1] in dict_csk_entities:\n",
    "                            tmp_entities_two_hop.append(dict_csk_entities[tmp_pair2[1]])\n",
    "            elif entity2 == tmp_pair2[1] and tmp_pair2[0] not in post_entities and tmp_pair2[0] not in entities_one_hop:\n",
    "                if entity2 in w2v_model.key_to_index and tmp_pair2[0] in w2v_model.key_to_index: # check both of them in w2vec vocab\n",
    "                    sim_score2 = w2v_model.similarity(entity2,tmp_pair2[0])\n",
    "                    if tmp_pair2[0] in dict_csk_entities and sim_score2>threshold2:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair2[0]])                  \n",
    "                    else: \n",
    "                        if tmp_pair2[0] in dict_csk_entities:\n",
    "                            tmp_entities_two_hop.append(dict_csk_entities[tmp_pair2[0]])\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        tmp_e2 = []\n",
    "        tmp_o2 = []\n",
    "        tmp_pair2 = []\n",
    "\n",
    "# print(len(tmp_entities_two_hop))\n",
    "id_two_hop = [i for n, i in enumerate(tmp_entities_two_hop) if i not in tmp_entities_two_hop[:n]]\n",
    "print(len(id_two_hop))\n",
    "print(id_two_hop)\n",
    "# print(entities_two_hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  362   659   938   942  1185  1545  1606  1637  1802  1974  2002  2024\n",
      "  2541  2826  2846  2889  3078  3159  3164  3487  3571  3736  4154  4518\n",
      "  4640  4786  4802  5173  5192  5399  5562  5800  5825  6055  6122  6384\n",
      "  6491  6678  6821  7362  7523  7572  7740  7766  7770  7829  7887  8360\n",
      "  8488  8545  8695  8841  9075  9310  9474  9548  9618 10057 10083 10252\n",
      " 10411 10558 10576 11105 11450 11761 11938 12220 12237 12380 12441 12453\n",
      " 12509 12549 12652 12811 12866 12896 12978 13565 13719 13720 13804 14042\n",
      " 14247 14599 14614 14895 14950 15041 15063 15433 15904 16111 16196 16320\n",
      " 16853 16988 17116 17142 17607 17645 18128 18311 18713 18812 18869 19031\n",
      " 19107 19142 19351 19685 20026 20353 20496 20626 20911 20994 21079 21217\n",
      " 21601 21737 21852 21892 21894]\n",
      "[  109   269   276   282   334   521   536   589   696   720   849   875\n",
      "   914   943   996  1039  1483  1499  1572  1722  1737  1771  1774  1955\n",
      "  1988  2231  2581  2629  2653  2717  3041  3047  3097  3209  3305  3757\n",
      "  3983  4075  4272  4606  4889  5031  5145  5272  5428  5694  6391  6531\n",
      "  6617  6654  6665  6676  6693  6881  6968  7154  7201  7432  7452  7738\n",
      "  7746  7784  8050  8090  8410  8527  8616  8856  8947  9010  9138  9244\n",
      "  9411  9416  9448  9619  9718  9728  9774  9810 10276 10376 10552 10653\n",
      " 11601 11880 12060 12181 12364 12391 12417 12534 12647 13124 13160 13163\n",
      " 13169 13236 13277 13343 13364 13654 13713 13866 13903 13927 14295 14297\n",
      " 14496 14582 14674 14723 14741 15146 15662 15724 15732 15744 15799 15805\n",
      " 15859 16042 16090 16268 16740 16744 16781 16918 17023 17046 17092 17108\n",
      " 17252 17442 17633 17681 17685 17692 17723 17746 17836 17989 18099 18338\n",
      " 18399 18405 18655 18661 18698 18823 18870 18881 18960 19294 19325 19511\n",
      " 19574 19949 20039 20213 20237 20528 20534 20717 20739 20748 20754 20896\n",
      " 20987 21012 21358 21573 21750 21830 21903 22518]\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(id_one_hop))\n",
    "print(np.sort(id_two_hop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "same = [i for i, j in zip(id_one_hop, id_two_hop) if i == j]\n",
    "print(same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_post_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the post_triples list\n",
    "def create_post_triples(post, csk_entities):\n",
    "    csk_entities = csk_entities\n",
    "    post_csk_entities = {}\n",
    "    count = 1\n",
    "    res = re.findall( r'\\w+|[^\\s\\w]+', post)\n",
    "    print(res)\n",
    "    print(len(res))\n",
    "    for i in res:\n",
    "#         print(i)\n",
    "        if i in post_csk_entities.keys():\n",
    "            continue\n",
    "\n",
    "        elif i in csk_entities:\n",
    "            post_csk_entities[i] = count\n",
    "            count = count+1\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    post_triples = []\n",
    "    for word in res:\n",
    "        if word in post_csk_entities:\n",
    "            post_triples.append(post_csk_entities[word])\n",
    "        else:\n",
    "            post_triples.append(0)  # Use 0 for words that are not in dict_csk_entities\n",
    "    return post_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'fucking', 'shit', 'pisses', 'me', 'off', 'to', 'no', 'end', ',', 'when', 'these', 'fucking', 'liberal', 'hypocrites', 'imply', 'the', 'only', 'group', 'of', 'people', 'capable', 'of', 'being', 'racist', 'are', 'the', 'whites', '.']\n",
      "29\n",
      "Post Triples: [0, 1, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 4, 5, 6, 0, 0, 7, 0, 0, 8, 0, 0, 9, 0, 0, 10, 0]\n"
     ]
    }
   ],
   "source": [
    "post_triples = create_post_triples(post3, csk_entities)\n",
    "\n",
    "print(\"Post Triples:\", post_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one_two_triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loop, RelatedTo, chicago', 'shoot, RelatedTo, comet', 'dumb, RelatedTo, deaf', 'walk, RelatedTo, door', 'pier, RelatedTo, boat', 'muscular, RelatedTo, strength', 'glass, RelatedTo, ceramic', 'shad, RelatedTo, cool', 'remind, RelatedTo, memory', 'honorable, Synonym, mention']\n"
     ]
    }
   ],
   "source": [
    "ack = csk_triples[:10]\n",
    "print(ack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true2\n"
     ]
    }
   ],
   "source": [
    "t = 1\n",
    "b = 'a'\n",
    "if ['a', 1] == [t,b]:\n",
    "    print('true1')\n",
    "elif ['a',1] == [b,t]:\n",
    "    print('true2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "[109, 18698, 1955, 17681, 17989, 589, 17252, 2231, 18661, 6617, 2581, 3097, 8527, 9411, 15859, 9619, 7784, 18870, 17092, 10552, 996, 1499, 14741, 9244, 19574, 15724, 17692, 849, 21573, 20739, 696, 10376, 5272, 1039, 12364, 16268, 20748, 20987, 16744, 16781, 16740, 14295, 17836, 16042, 7154, 20896, 5031, 10653, 13169, 4075, 12181, 7432, 8050, 15146, 20213, 5694, 720, 4272, 12060, 20754, 13236, 14496, 20717, 15662, 3983, 6654, 4606, 9448, 6881, 19949, 875, 13343, 21012, 12391, 20534, 334, 7738, 4889, 3041, 282, 18823, 11880, 11601, 14723, 6391, 15805, 9718, 15799, 2629, 17685, 6968, 6676, 14582, 15744, 18399, 13163, 17442, 14297, 943, 19511, 13866, 1483, 914, 9810, 21830, 12534, 13364, 7746, 2653, 13654, 13124, 10276, 1737, 15732, 1572, 7452, 9138, 19325, 21903, 17046, 536, 17723, 13903, 13927, 19294, 20039, 8947, 20237, 1771, 3047, 521, 8090, 17108, 1988, 20528, 12647, 9010, 18099, 6531, 17633, 18960, 17023, 17746, 5428, 14674, 18405, 21358, 13277, 3305, 8856, 22518, 8616, 1774, 5145, 3209, 2717, 12417, 6693, 1722, 16918, 9774, 9416, 18881, 7201, 269, 18655, 16090, 8410, 6665, 276, 21750, 3757, 9728, 13713, 13160, 18338]\n"
     ]
    }
   ],
   "source": [
    "tmp_e2 = ''\n",
    "tmp_o2 = ''\n",
    "tmp_pair2= []\n",
    "# find two hope from post entities\n",
    "lsk = ['loop, RelatedTo, chicago', 'shoot, RelatedTo, comet', 'dumb, RelatedTo, deaf', 'walk, RelatedTo, door', 'pier, RelatedTo, boat']\n",
    "# id2word = dict()\n",
    "# for key in word2id.keys():\n",
    "#     id2word[word2id[key]] = key\n",
    "swapped_dict = {value: key for key, value in dict_csk_entities.items()}\n",
    "entities_one_hop = []\n",
    "for i in id_one_hop:\n",
    "    entities_one_hop.append(swapped_dict[i])\n",
    "threshold2 = 0.01\n",
    "tmp_entities_two_hop = []\n",
    "\n",
    "# ☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆☆\n",
    "one_two_triples = []\n",
    "tmp_one_two_triples = []\n",
    "# tmp_one_two_triples.append(dict_csk_triples[e_o])\n",
    "# one_two_triples.append(tmp_one_two_triples)\n",
    "for e_o2 in csk_triples:\n",
    "    e_o2_split = e_o2.split(',')\n",
    "    for entity2 in entities_one_hop:\n",
    "        tmp_e2 = e_o2_split[0]\n",
    "        tmp_o2 = e_o2_split[2].lstrip()\n",
    "        tmp_pair2 = [tmp_e2, tmp_o2]\n",
    "        if entity2 in tmp_pair2:\n",
    "            if entity2 == tmp_pair2[0] and tmp_pair2[1] not in post_entities and tmp_pair2[1] not in entities_one_hop:\n",
    "                if entity2 in w2v_model.key_to_index and tmp_pair2[1] in w2v_model.key_to_index: # check both of them in w2vec vocab\n",
    "                    sim_score2 = w2v_model.similarity(entity2,tmp_pair2[1])\n",
    "                    if tmp_pair2[1] in dict_csk_entities and sim_score2 > threshold2:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair2[1]])\n",
    "                    else:\n",
    "                        if tmp_pair2[1] in dict_csk_entities:\n",
    "                            tmp_entities_two_hop.append(dict_csk_entities[tmp_pair2[1]])\n",
    "            elif entity2 == tmp_pair2[1] and tmp_pair2[0] not in post_entities and tmp_pair2[0] not in entities_one_hop:\n",
    "                if entity2 in w2v_model.key_to_index and tmp_pair2[0] in w2v_model.key_to_index: # check both of them in w2vec vocab\n",
    "                    sim_score2 = w2v_model.similarity(entity2,tmp_pair2[0])\n",
    "                    if tmp_pair2[0] in dict_csk_entities and sim_score2>threshold2:\n",
    "                        tmp_id_one_hop.append(dict_csk_entities[tmp_pair2[0]])\n",
    "                    else: \n",
    "                        if tmp_pair2[0] in dict_csk_entities:\n",
    "                            tmp_entities_two_hop.append(dict_csk_entities[tmp_pair2[0]])\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        tmp_e2 = []\n",
    "        tmp_o2 = []\n",
    "        tmp_pair2 = []\n",
    "# print(len(tmp_entities_two_hop))\n",
    "id_two_hop = [i for n, i in enumerate(tmp_entities_two_hop) if i not in tmp_entities_two_hop[:n]]\n",
    "print(len(id_two_hop))\n",
    "print(id_two_hop)\n",
    "# print(entities_two_hop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "[42515, 94866]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "[76846]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "[94514, 79026, 43932]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "[1032, 31295, 41884, 87201, 48031, 84792, 94719, 63152, 89554]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "[68428, 87949, 86554]\n",
      "10th o_h_t_i\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "[3020, 9154, 69296, 70224, 13227, 21855, 24652, 25118, 26571, 27447, 32807, 35600, 41179, 43574, 51781, 78604, 53580, 115446, 4402, 110179, 95568, 86744, 64040, 91460, 22265, 66760, 95199, 9531, 98582, 102581, 104114, 100701, 105904, 57967, 99367]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]\n",
      "[32588, 104497, 71709, 38934]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n",
      "[]\n",
      "10th o_h_t_i\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-142-24f234f022f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mt_h_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswapped_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt_h_t_i\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0me_o3\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsk_triples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0me_o3_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me_o3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[0mtmp_e3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me_o3_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mtmp_o3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me_o3_split\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "one_two_triples = []\n",
    "tmp_one_two_triples = []\n",
    "count = 0\n",
    "for o_h_t_i in id_one_hop:\n",
    "    o_h_e = swapped_dict[o_h_t_i]\n",
    "    tmp_one_two_triples = []\n",
    "    for t_h_t_i in id_two_hop:\n",
    "        t_h_e = swapped_dict[t_h_t_i]\n",
    "        for e_o3 in csk_triples:\n",
    "            e_o3_split = e_o3.split(',')\n",
    "            tmp_e3 = e_o3_split[0]\n",
    "            tmp_o3 = e_o3_split[2].lstrip()\n",
    "            if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]:\n",
    "                print('yes if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]')\n",
    "                tmp_one_two_triples.append(dict_csk_triples[e_o3])\n",
    "            elif [o_h_e, t_h_e] == [tmp_o3, tmp_e3]:\n",
    "                print('yes2 if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]')\n",
    "                tmp_one_two_triples.append(dict_csk_triples[e_o3])\n",
    "            else:\n",
    "                continue\n",
    "    print(tmp_one_two_triples)\n",
    "    one_two_triples.append(tmp_one_two_triples)\n",
    "    if count // 10 == 0:\n",
    "        print('10th o_h_t_i')\n",
    "            \n",
    "print('len_one_two_triples',len(one_two_triples))\n",
    "print(one_two_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [lmfao-16196, 20496, 6055, 7523, 12453]\n",
    "# [adapt-109, 18698, 1955, 17681, 17989]\n",
    "d_csk_triples = {\"lmfao, RelatedTo, adapt\": 0, \"lmfao, RelatedTo, gorge\":1, \"lmfao, RelatedTo, assembly\":2, \"goon, RelatedTo, malt\":3, \"frigging, RelatedTo, boat\":4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16196, 20496, 6055, 7523, 12453]\n",
      "[109, 18698, 1955, 17681, 17989]\n",
      "['loop, RelatedTo, chicago', 'shoot, RelatedTo, comet', 'dumb, RelatedTo, deaf', 'walk, RelatedTo, door', 'pier, RelatedTo, boat']\n"
     ]
    }
   ],
   "source": [
    "id_one_hop_ex = id_one_hop[:5]\n",
    "id_two_hop_ex = id_two_hop[:5]\n",
    "cst = csk_triples[:5]\n",
    "ccc_triples = ['lmfao, RelatedTo, adapt', 'gorge, RelatedTo, lmfao', \"lmfao, RelatedTo, assembly\", \"goon, RelatedTo, malt\", \"torture, RelatedTo, finish\"]\n",
    "d_csk_triples = {\"lmfao, RelatedTo, adapt\": 0, \"gorge, RelatedTo, lmfao\":1, \"lmfao, RelatedTo, assembly\":2, \"goon, RelatedTo, malt\":3, \"torture, RelatedTo, finish\":4}\n",
    "\n",
    "# 1955 -assembly\n",
    "print(id_one_hop_ex)\n",
    "print(id_two_hop_ex)\n",
    "print(cst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_one_two_triples 5\n",
      "[[0, 1, 2], [3], [], [4], []]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "one_two_triples = []\n",
    "count = 0\n",
    "for o_h_t_i in id_one_hop_ex:\n",
    "    o_h_e = swapped_dict[o_h_t_i]\n",
    "    tmp_one_two_triples = []\n",
    "    for t_h_t_i in id_two_hop_ex:\n",
    "        t_h_e = swapped_dict[t_h_t_i]\n",
    "        for e_o3 in ccc_triples:\n",
    "            e_o3_split = e_o3.split(',')\n",
    "            tmp_e3 = e_o3_split[0]\n",
    "            tmp_o3 = e_o3_split[2].lstrip()\n",
    "#             print('o_h_e:', o_h_e)\n",
    "#             print('t_h_e', t_h_e)\n",
    "            if [o_h_e, t_h_e] == [tmp_e3, tmp_o3]:\n",
    "                tmp_one_two_triples.append(d_csk_triples[e_o3])\n",
    "            elif [o_h_e, t_h_e] == [tmp_o3, tmp_e3]:\n",
    "                tmp_one_two_triples.append(d_csk_triples[e_o3])                \n",
    "            else:\n",
    "                continue\n",
    "    one_two_triples.append(tmp_one_two_triples)\n",
    " \n",
    "            \n",
    "print('len_one_two_triples',len(one_two_triples))\n",
    "print(one_two_triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_60L7hXzJA9"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713337068693,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "UYav9uIszKYC"
   },
   "outputs": [],
   "source": [
    "def padding(sent, l):\n",
    "    return sent + ['_EOS'] + ['_PAD'] * (l-len(sent)-1)\n",
    "\n",
    "def padding_triple_id(entity2id, triple, num, l):\n",
    "    newtriple = []\n",
    "    for i in range(len(triple)):\n",
    "        for j in range(len(triple[i])):\n",
    "            for k in range(len(triple[i][j])):\n",
    "                if triple[i][j][k] in entity2id:\n",
    "                    triple[i][j][k] = entity2id[triple[i][j][k]]\n",
    "                else:\n",
    "                    triple[i][j][k] = entity2id['_NONE']\n",
    "\n",
    "    for tri in triple:\n",
    "        newtriple.append(tri + [[entity2id['_PAD_H'], entity2id['_PAD_R'], entity2id['_PAD_T']]] * (l - len(tri)))\n",
    "    pad_triple = [[entity2id['_PAD_H'], entity2id['_PAD_R'], entity2id['_PAD_T']]] * l\n",
    "    return newtriple + [pad_triple] * (num - len(newtriple))\n",
    "\n",
    "def build_kb_adj_mat(kb_adj_mats, fact_dropout):\n",
    "    \"\"\"Create sparse matrix representation for batched data\"\"\"\n",
    "    mats0_batch = np.array([], dtype=int)\n",
    "    mats0_0 = np.array([], dtype=int)\n",
    "    mats0_1 = np.array([], dtype=int)\n",
    "    vals0 = np.array([], dtype=float)\n",
    "\n",
    "    mats1_batch = np.array([], dtype=int)\n",
    "    mats1_0 = np.array([], dtype=int)\n",
    "    mats1_1 = np.array([], dtype=int)\n",
    "    vals1 = np.array([], dtype=float)\n",
    "\n",
    "    for i in range(kb_adj_mats.shape[0]):\n",
    "        (mat0_0, mat0_1, val0), (mat1_0, mat1_1, val1) = kb_adj_mats[i]\n",
    "        assert len(val0) == len(val1)\n",
    "        num_fact = len(val0)\n",
    "        num_keep_fact = int(np.floor(num_fact * (1 - fact_dropout)))\n",
    "        mask_index = np.random.permutation(num_fact)[ : num_keep_fact]\n",
    "        # mat0\n",
    "        mats0_batch = np.append(mats0_batch, np.full(len(mask_index), i, dtype=int))\n",
    "        mats0_0 = np.append(mats0_0, mat0_0[mask_index])\n",
    "        mats0_1 = np.append(mats0_1, mat0_1[mask_index])\n",
    "        vals0 = np.append(vals0, val0[mask_index])\n",
    "        # mat1\n",
    "        mats1_batch = np.append(mats1_batch, np.full(len(mask_index), i, dtype=int))\n",
    "        mats1_0 = np.append(mats1_0, mat1_0[mask_index])\n",
    "        mats1_1 = np.append(mats1_1, mat1_1[mask_index])\n",
    "        vals1 = np.append(vals1, val1[mask_index])\n",
    "\n",
    "    return (mats0_batch, mats0_0, mats0_1, vals0), (mats1_batch, mats1_0, mats1_1, vals1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RE6yA-kXrJhr"
   },
   "source": [
    "# Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1713337075255,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "LH3SVLXRhgi7"
   },
   "outputs": [],
   "source": [
    "csk_triples, csk_entities, kb_dict = [], [], []\n",
    "dict_csk_entities, dict_csk_triples = {}, {}\n",
    "class Config():\n",
    "  def __init__(self, path):\n",
    "    self.config_path = path\n",
    "    self._get_config()\n",
    "\n",
    "  def _get_config(self):\n",
    "    with open(self.config_path, \"r\") as setting:\n",
    "      config = yaml.load(setting,Loader=yaml.FullLoader)\n",
    "    self.is_train = config['is_train']\n",
    "    self.test_model_path = config['test_model_path']\n",
    "    self.embed_units = config['embed_units']\n",
    "    self.symbols = config['symbols']\n",
    "    self.units = config['units']\n",
    "    self.layers = config['layers']\n",
    "    self.batch_size = config['batch_size']\n",
    "    self.data_dir = config['data_dir']\n",
    "    self.num_epoch = config['num_epoch']\n",
    "    self.lr_rate = config['lr_rate']\n",
    "    self.lstm_dropout = config['lstm_dropout']\n",
    "    self.linear_dropout = config['linear_dropout']\n",
    "    self.max_gradient_norm = config['max_gradient_norm']\n",
    "    self.trans_units = config['trans_units']\n",
    "    self.gnn_layers = config['gnn_layers']\n",
    "    self.fact_dropout = config['fact_dropout']\n",
    "    self.fact_scale = config['fact_scale']\n",
    "    self.pagerank_lambda = config['pagerank_lambda']\n",
    "    self.result_dir_name = config['result_dir_name']\n",
    "    self.generated_path = config['generated_path']\n",
    "\n",
    "  def list_all_member(self):\n",
    "    for name, value in vars(self).items():\n",
    "        print('%s = %s' % (name, value))\n",
    "\n",
    "\n",
    "def run(model, data_train, config, word2id, entity2id):\n",
    "    batched_data = gen_batched_data(data_train, config, word2id, entity2id)\n",
    "    if model.is_inference == True:\n",
    "        word_index, selector = model(batched_data)\n",
    "        return word_index, selector\n",
    "    else:\n",
    "        decoder_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, word_neg_num, local_neg_num, only_two_neg_num = model(batched_data)\n",
    "        return decoder_loss, sentence_ppx, sentence_ppx_word, sentence_ppx_local, sentence_ppx_only_two, word_neg_num, local_neg_num, only_two_neg_num\n",
    "\n",
    "\n",
    "def generate(model, data_test, config, word2id, entity2id, epoch = 0, model_path = None):\n",
    "  if model_path != None:\n",
    "      model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))\n",
    "\n",
    "  count = 0\n",
    "  model.is_inference = True\n",
    "  id2word = dict()\n",
    "  for key in word2id.keys():\n",
    "      id2word[word2id[key]] = key\n",
    "\n",
    "  def write_batch_res_text(word_index, id2word, selector = None):\n",
    "      w = open(config.generated_path + '/generated_res_Scr.txt', 'a')\n",
    "      batch_size = len(word_index)\n",
    "      decoder_len = len(word_index[0])\n",
    "      text = []\n",
    "      if selector != None:\n",
    "          for i in range(batch_size):\n",
    "              tmp_dict = dict()\n",
    "              tmp = []\n",
    "              for j in range(decoder_len):\n",
    "                  if word_index[i][j] == 2:\n",
    "                      break\n",
    "                  tmp.append(id2word[word_index[i][j]])\n",
    "              # print(tmp)\n",
    "              tmp_dict['res_text'] = tmp\n",
    "              local_tmp = []\n",
    "              only_two_tmp = []\n",
    "              for j in range(len(tmp)):\n",
    "                  if selector[i][j] == 1:\n",
    "                      local_tmp.append(tmp[j])\n",
    "                  if selector[i][j] == 2:\n",
    "                      only_two_tmp.append(tmp[j])\n",
    "              tmp_dict['local'] = local_tmp\n",
    "              tmp_dict['only_two'] = only_two_tmp\n",
    "              text.append(tmp_dict)\n",
    "\n",
    "      w.write(json.dumps(model_path+ '\\n'))\n",
    "      for line in text:\n",
    "          print(line)\n",
    "          w.write(json.dumps(line) + '\\n')\n",
    "      w.close()\n",
    "\n",
    "  for iteration in range(len(data_test)):\n",
    "    word_index, selector = run(model, data_test[(iteration * config.batch_size):(iteration * \\\n",
    "          config.batch_size + config.batch_size)], config, word2id, entity2id)\n",
    "\n",
    "    if count % 50 == 0:\n",
    "        print (\"generate:\", iteration)\n",
    "    count += 1\n",
    "    write_batch_res_text(word_index, id2word, selector=selector)\n",
    "\n",
    "\n",
    "def main():\n",
    "  config = Config('/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/config.yml')\n",
    "  config.list_all_member()\n",
    "  raw_vocab, _, data_test = prepare_data(config)\n",
    "  word2id, entity2id, vocab, embed, entity_vocab, entity_embed, relation_vocab, relation_embed, entity_relation_embed = build_vocab(config.data_dir, raw_vocab, config = config)\n",
    "  model = use_cuda(ConceptFlow(config, embed, entity_relation_embed))\n",
    "\n",
    "  model_optimizer = torch.optim.Adam(model.parameters(), lr = config.lr_rate)\n",
    "\n",
    "  if not os.path.exists(config.generated_path):\n",
    "      os.mkdir(config.generated_path)\n",
    "\n",
    "  generate(model, data_test, config, word2id, entity2id, model_path=config.test_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 175,
     "status": "ok",
     "timestamp": 1713335677369,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "Xt6MESz2w0tg",
    "outputId": "3d7f6eef-a51c-4b58-e21d-431854cb43b9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/config.yml'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.realpath('/content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 54867,
     "status": "error",
     "timestamp": 1713337133052,
     "user": {
      "displayName": "Yoonhyuck Woo",
      "userId": "02824016300247564870"
     },
     "user_tz": 240
    },
    "id": "GH850HdUhgp_",
    "outputId": "017076ab-ac85-494e-8173-0fc96ba79baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_path = /content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/config.yml\n",
      "is_train = False\n",
      "test_model_path = /content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/training_output/_epoch_4.pkl\n",
      "embed_units = 300\n",
      "symbols = 30000\n",
      "units = 512\n",
      "layers = 2\n",
      "batch_size = 30\n",
      "data_dir = /content/drive/MyDrive/Colab Notebooks/ConceptFlow(ECCF)_data\n",
      "num_epoch = 5\n",
      "lr_rate = 0.0001\n",
      "lstm_dropout = 0.3\n",
      "linear_dropout = 0.2\n",
      "max_gradient_norm = 5\n",
      "trans_units = 100\n",
      "gnn_layers = 3\n",
      "fact_dropout = 0.0\n",
      "fact_scale = 1\n",
      "pagerank_lambda = 0.8\n",
      "result_dir_name = /content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/training_output\n",
      "generated_path = /content/drive/MyDrive/Commit_colab/PURDUE-2024-SPRING/CS592 HAI/Project/inference_output\n",
      "Creating word vocabulary...\n",
      "Creating entity vocabulary...\n",
      "Creating relation vocabulary...\n",
      "Loading word vectors...\n",
      "    processing line 0\n",
      "    processing line 100000\n",
      "    processing line 200000\n",
      "    processing line 300000\n",
      "    processing line 400000\n",
      "    processing line 500000\n",
      "    processing line 600000\n",
      "    processing line 700000\n",
      "    processing line 800000\n",
      "    processing line 900000\n",
      "    processing line 1000000\n",
      "    processing line 1100000\n",
      "    processing line 1200000\n",
      "    processing line 1300000\n",
      "    processing line 1400000\n",
      "    processing line 1500000\n",
      "    processing line 1600000\n",
      "Loading entity vectors...\n",
      "Loading relation vectors...\n",
      "generate: 0\n",
      "{'res_text': ['i', 'do', \"n't\", 'know', 'if', 'i', \"'m\", 'wearing', 'this', '.'], 'local': ['wearing'], 'only_two': []}\n",
      "{'res_text': ['i', \"'ll\", 'shake', 'my', 'ass'], 'local': ['shake', 'ass'], 'only_two': []}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-86ed81c010d4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerated_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m   \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-86ed81c010d4>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(model, data_test, config, word2id, entity2id, epoch, model_path)\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;31m# for iteration in range(len(data_test) // config.batch_size):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m       word_index, selector = run(model, data_test[(iteration * config.batch_size):(iteration * \\\n\u001b[0m\u001b[1;32m     94\u001b[0m           config.batch_size + config.batch_size)], config, word2id, entity2id)\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-86ed81c010d4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, data_train, config, word2id, entity2id)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mbatched_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_batched_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_inference\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7835ad0550cd>\u001b[0m in \u001b[0;36mgen_batched_data\u001b[0;34m(data, config, word2id, entity2id)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mcsk_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsk_triples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkb_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_csk_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_csk_triples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mencoder_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mdecoder_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'response'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxoVGUzyvJRD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNEdTDQKImWv3G/elEQrVZp",
   "gpuType": "L4",
   "machine_shape": "hm",
   "mount_file_id": "1Psy_6BF-POj9vN3vC27yL_XoZNDrch1Z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
