{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu8QNsWRb_xp"
   },
   "source": [
    "# Assignment 4: Detecting and Mitigating Bias \n",
    "\n",
    "The goal of this tutorial is to introduce the basic functionality of AI Fairness 360 for detecting and mitigating bias. As before, we will work with the German Credit dataset. There are many metrics one can use to detect the presence of bias. Likewise, there are many different bias mitigation algorithms one can employ. AI Fairness 360 provides some of them most common metrics and algorithms.\n",
    "\n",
    "\n",
    "### Bias mitigation techniques\n",
    "\n",
    "We learnt about the different bias mitigation techniques in class called _pre-processing_, _in-processing_, and _post-processing_.\n",
    "\n",
    "\n",
    "We will use AI Fairness 360 (`aif360`) to detect and mitigate bias. We will look for bias in the creation of a machine learning model that predicts whether an applicant should be given credit based on various features from a typical credit application. The protected attribute will be \"Age\", with \"1\" (older than or equal to 25) and \"0\" (younger than 25) being the values for the _privileged_ and _unprivileged_ groups, respectively.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Install and import packages and modules\n",
    "2. Load dataset, split between train and test, and compute fairness metrics on original training dataset\n",
    "3. Mitigate bias using a pre-processing algorithm (reweighing)\n",
    "4. Mitigate bias using an in-processing algorithm (adversarial debiasing)\n",
    "5. Mitigate bias using a post-processing algorithm (equalized odds post processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSvZzh_-ntSU"
   },
   "source": [
    "\n",
    "## 1. Import Statements\n",
    "\n",
    "First, we install the necessary packages. Then we import several components from the `aif360` package. We are relying on aif360 for this assignment, so please start early to make sure that the dependencies are resolved and that the pacakges load correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-macos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to re-install if you already did so in Assignment 2\n",
    "!pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3511,
     "status": "ok",
     "timestamp": 1643739518204,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "FJxDwiuVb_xt",
    "outputId": "d87328c1-9c0c-44e2-ee4b-a103556d2bf6"
   },
   "outputs": [],
   "source": [
    "# import all necessary packages\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "from aif360.datasets import GermanDataset, BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, DatasetMetric\n",
    "\n",
    "from aif360.algorithms.preprocessing import Reweighing, LFR, DisparateImpactRemover\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "\n",
    "from aif360.explainers import MetricTextExplainer, MetricJSONExplainer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQj5VqOSb_xt"
   },
   "source": [
    "## 2. Load Data, Specify Protected Attribute, and Split Data\n",
    "\n",
    "We will use the German Credit data, set the protected attribute to be age, create two variables to represent the privileged and unprivileged groups, and split the original dataset into training and test data subsets. Finally, we will build a typical machine learning workflow that involves training a machine learning model on the training dataset and use a test dataset to assess the model's efficacy (e.g., accuracy, fairness). For this dataset, we have a binary classification problem that predicts individuals as being a good or a bad credit risk.\n",
    "\n",
    "In this dataset, we consider older applicants (age >= 25) as the privileged group and younger applicants (age < 25) as the unprivileged group. \n",
    "\n",
    "We will use the preprocessed GermanDataset with one-hot encoded data provided by the aif360 package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1643739518204,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "JYB3PkStb_xv"
   },
   "outputs": [],
   "source": [
    "# note that we drop sex, which may also be a protected attribute\n",
    "dataset_orig = GermanDataset(protected_attribute_names=['age'],\n",
    "                             privileged_classes=[lambda x: x >= 25],\n",
    "                             features_to_drop=['personal_status', 'sex'])\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1643739518205,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "MsFiFKReb_xw",
    "outputId": "ab4ceeb1-3ede-4ad3-d21f-7aa2816a3a29"
   },
   "outputs": [],
   "source": [
    "print(\"Original data shape: \",dataset_orig.features.shape)\n",
    "print(\"Train dataset shape: \", dataset_orig_train.features.shape)\n",
    "print(\"Test dataset shape: \", dataset_orig_test.features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object ```dataset_orig``` is an aif360 dataset, which has some useful methods and attributes that you can explore. More documentation is available at https://aif360.readthedocs.io/en/latest/modules/datasets.html. \n",
    "For now, we'll just transform the data into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1643739518206,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "W3Aonmt3b_xw"
   },
   "outputs": [],
   "source": [
    "df, dict_df = dataset_orig.convert_to_dataframe()\n",
    "print(\"Shape: \", df.shape)\n",
    "# print(df.columns)\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kITY3AtDb_xz"
   },
   "source": [
    "## 3. Compute Fairness Metrics on Original Training Data\n",
    "Now that we have identified the protected attribute \"age\" and defined privileged and unprivileged values, we can use aif360 to detect bias in the dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZLVzqlmwOXI"
   },
   "source": [
    "### Mean Outcomes\n",
    "\n",
    "Compare the base rates (i.e., percentage of favorable results) for the privileged and unprivileged groups and report the difference (unprivileged base rate - privileged base rate). This is implemented in the ```mean_difference``` method on the BinaryLabelDatasetMetric class, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1643739519124,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "bVQWXC8Ub_x0",
    "outputId": "ae13c1db-2647-4764-ddf9-e2274a93dcbf"
   },
   "outputs": [],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(\n",
    "     dataset_orig_train, \n",
    "     unprivileged_groups=unprivileged_groups,\n",
    "     privileged_groups=privileged_groups\n",
    "  )\n",
    "print(\"Original training dataset\")\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZufOXh9b_x0"
   },
   "source": [
    "### Disparate Impact\n",
    "We can calculate the ratio of (predicted) favorable outcomes for the unprivileged group compared to the privileged group as implemented in the ```disparate_impact``` method on the BinaryLabelDatasetMetric class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1643739519126,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "1iw7g6Hjb_x0",
    "outputId": "d5efcc0a-5d2c-4120-94ab-2e8dbe658fc2"
   },
   "outputs": [],
   "source": [
    "print(\"Original training dataset\")\n",
    "print(\"Disparate Impact = %f\" % metric_orig_train.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The fairness metrics above will vary depending upon the train-test split. If the magnitude of mean difference is less than 10%, try another split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcTiGnSob_x1"
   },
   "source": [
    "### Built-In Explainers\n",
    "\n",
    "```aif360``` has some useful explainers for the fairness metrics which can be used to interpret the fairness metric values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643739519127,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "ERMXmj2pb_x1"
   },
   "outputs": [],
   "source": [
    "json_expl = MetricJSONExplainer(metric_orig_train)\n",
    "def format_json(json_str):\n",
    "    return json.dumps(json.loads(json_str, object_pairs_hook=OrderedDict),\n",
    "                      indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWuosH5pxVHe"
   },
   "source": [
    "Let's print the mean difference explainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643739519128,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "7cc0pIUPxdFl",
    "outputId": "86acf8f0-f04d-4c19-ec4a-3ba6d0d6fd9e"
   },
   "outputs": [],
   "source": [
    "print(format_json(json_expl.mean_difference()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vl-r2vpjxPFo"
   },
   "source": [
    "We can also print the disparate impact explainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1643739519129,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "y-FoLzMTxSGS",
    "outputId": "8c66b2d1-0262-48c6-a17d-12c871f04c85"
   },
   "outputs": [],
   "source": [
    "print(format_json(json_expl.disparate_impact()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcDleSrDJr_B"
   },
   "source": [
    "**Q1:** Using the explainers above, tnterpret the difference in means and disparate impact in the German Credit data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HV2t23yJ19P"
   },
   "source": [
    "**Write your interpretation here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model on the training data\n",
    "\n",
    "Let's build a logistic regression model on this training data, predict credit risk for test data and compute the same fairness metrics over the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "\n",
    "df_test, dict_df_test = dataset_orig_test.convert_to_dataframe()\n",
    "df_train, dict_df_train = dataset_orig_train.convert_to_dataframe()\n",
    "\n",
    "# Fit the model to the training data\n",
    "x_train = df_train.drop(['credit'], axis=1)\n",
    "y_train = df_train['credit']\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "x_test = df_test.drop(['credit'], axis=1)\n",
    "y_test = df_test['credit']\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "dataset_pred_test = dataset_orig_test.copy()\n",
    "dataset_pred_test.labels = y_pred.copy()\n",
    "\n",
    "metric_dataset_test = BinaryLabelDatasetMetric(\n",
    "    dataset_pred_test, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here to compute fairness metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Using the fairness metric functions as before, report the bias observed in the model's predictions over test data. What do these values indicate? Are the model's predictions more biased or less biased compared to the bias observed in the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bias Mitigation Techniques\n",
    "\n",
    "We learnt in class that there are several bias mitigation techniques namely, pre-processing, in-processing, and post-processing algorithms.\n",
    "\n",
    "_Pre-processing_ bias mitigation is performed at the data end, before the creation of the model. In other words, we transform the data such that a model learned on the transformed data produces less biased decisions.\n",
    "\n",
    "_In-processing_ bias mitigation methods focus on the model training stage, as compared to pre-processing which focuses on transforming the data prior to model training. This suite of methods includes incorporating a fairness constraint during model training, tweaking the model's objective function, and adversarial learning.\n",
    "\n",
    "_Post-processing_ bias mitigation focus on the model predictions after the model has been trained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQIwiWcKb_x3"
   },
   "source": [
    "### 4.1 Bias Mitigation via Pre-Processing\n",
    "\n",
    "AI Fairness 360 implements several pre-processing mitigation algorithms. We will use the **reweighing algorithm**, which is implemented in the `Reweighing` class in the `aif360.algorithms.preprocessing` package. As discussed in class, this algorithm will transform the dataset by assigning weights to instances in each (group, label) combination to change the base rates and ensure fairness before classification. The idea is to apply appropriate weights to different tuples in the training data to reduce discrimination with respect to the protected attributes.\n",
    "\n",
    "You can find documentation for reweighting here:\n",
    "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html \n",
    "\n",
    "Call the fit and transform methods to perform the transformation, producing a newly transformed training dataset (```dataset_transf_train```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1643739523669,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "CqBmaYXab_x3"
   },
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdkCX8Fd0zgN"
   },
   "source": [
    "We can print the weights. Each observation in the data should have a weight. For brevity, let's look at the weights for the first 10 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1643739523669,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "l5ISRhOwb_x3",
    "outputId": "032e2735-51a6-42c7-9e07-07d669cbcd5e"
   },
   "outputs": [],
   "source": [
    "len(dataset_transf_train.instance_weights)\n",
    "dataset_transf_train.instance_weights[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k6wrI5Ab_x4"
   },
   "source": [
    "### Compute Fairness Metrics in Transformed Data\n",
    "\n",
    "We can check how effective the transformed data was in removing bias by calculating the metrics used for the original training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_rw_train = BinaryLabelDatasetMetric(\n",
    "    dataset_transf_train, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFZh_-La6Eq8"
   },
   "source": [
    "Print the difference in mean outcomes and disparate impact in the transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1643739523848,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "gim6DapUb_x4"
   },
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1NdOcvCFmC8"
   },
   "source": [
    "**Q3:** How do these values compare to the difference in mean outcomes and disparate impact in the original data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aG_Glh7Frbc"
   },
   "source": [
    "**Write your answer in this text cell:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Fairness Metrics on Model Trained on Transformed Data\n",
    "\n",
    "In the following, we will train a model on the transformed data and compute the metrics over predictions made on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:**  How do you expect the fairness metrics would be over a model trained on the transformed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer in this text cell:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the instances now have weights, we will use a classifier that can incorporate instance weights. In this case, we will use a Naive Bayes classifier (more details here: https://scikit-learn.org/stable/modules/naive_bayes.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_rw, dict_df_train_rw = dataset_transf_train.convert_to_dataframe()\n",
    "\n",
    "# Fit the model to the transformed training data\n",
    "x_train_rw = df_train_rw.drop(['credit'], axis=1)\n",
    "y_train_rw = df_train_rw['credit']\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(x_train_rw, y_train_rw)\n",
    "\n",
    "# Use the model to make predictions on the test data\n",
    "y_pred_rw = model.predict(x_test)\n",
    "\n",
    "dataset_pred_test_rw = dataset_orig_test.copy()\n",
    "dataset_pred_test_rw.labels = y_pred_rw.copy()\n",
    "\n",
    "# Construct the BinaryLabelDatasetMetric object over the test predictions\n",
    "metric_dataset_test_rw = BinaryLabelDatasetMetric(\n",
    "    dataset_pred_test_rw, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )\n",
    "\n",
    "# Print fairness metrics computed over test predictions\n",
    "# write code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Are your observations in line with what you expected in Q4 above? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer in this text cell:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Instead of reweighing, one could also apply techniques such as suppression, i.e. removing sensitive attributes. Write code below to train a model that does not use any information on the sensitive attribute, use this model to make predictions over the test data, and then compute the fairness metrics over the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here to implement suppression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Interpret your results. How does the preprocessing technique in Q5 compare to the suppression technique? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer in this text cell:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riLalr-WwDsG"
   },
   "source": [
    "### 4.2. Bias Mitigation via In-Processing\n",
    "\n",
    "In-processing methods focus on the model training stage, as compared to pre-processing which focuses on transforming the data prior to model training. Broadly speaking, contemporary in-processing methods are stronger than pre-processing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUZKKFAo1R1v"
   },
   "source": [
    "### Adversarial Debiasing\n",
    "\n",
    "In this part of the notebook, we will use an in-processing algorithm, called _Adversarial Debiasing_, that we briefly discussed in class. From the aif360 documentation (https://aif360.readthedocs.io/en/v0.2.3/modules/inprocessing.html):\n",
    "\n",
    "> Adversarial debiasing is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary’s ability to determine the protected attribute from the predictions. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit.\n",
    "\n",
    "For intuition, you can think of adversarial debiasing as a model with two supervised learning tasks. The first task is to predict an outcome using the training data input. The second task, i.e. the adversary, is to predict a protected feature using these predictions and non-protected features in the training data input. The aim is to maximize the model's ability to carry out the first task (i.e. predict outcomes) while minimizing its ability to carry out the second task (i.e. predict protected features).\n",
    "\n",
    "We implement adversarial debiasing below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4551,
     "status": "ok",
     "timestamp": 1643739523668,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "M_rWLmHvwBFF",
    "outputId": "79b78b63-dff7-4c8a-a9a4-3ca9448f8d89"
   },
   "outputs": [],
   "source": [
    "# reset tensorflow graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# start tensorflow session\n",
    "sess = tf.compat.v1.Session()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# create AdversarialDebiasing model\n",
    "debiased_model = AdversarialDebiasing(\n",
    "    privileged_groups = privileged_groups,\n",
    "    unprivileged_groups = unprivileged_groups,\n",
    "    scope_name = 'debiased_classifier',\n",
    "    debias = True,\n",
    "    sess = sess)\n",
    "\n",
    "# fit the model to training data\n",
    "debiased_model.fit(dataset_orig_train)\n",
    "\n",
    "# make predictions on training and test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)\n",
    "\n",
    "# metrics\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(\n",
    "    dataset_debiasing_test, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )\n",
    "\n",
    "# Close session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWJUD_1-1XOe"
   },
   "source": [
    "### Fairness Metrics under Adversarial Debiasing\n",
    "\n",
    "The adversarial debiasing algorithm has built-in methods for the difference in mean outcomes (called ```.mean_difference()```) and disparate impact (called ```.disparate_impact()```). Print these below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1643739523668,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "ro-8nuS62NZx",
    "outputId": "d0cab299-c5af-472b-cbae-5c7e77cc1707"
   },
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIQcc23c7SNA"
   },
   "source": [
    "**Q8:** Interpret the difference in means and disparate impact for the predicted outcomes under adversarial debiasing. How do these compare to the metrics calculated in Q2 and Q5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3MyIpCk7hnp"
   },
   "source": [
    "**Write your interpretation and comparison in this text cell:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Bias Mitigation via Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last section, we will use one of the post-processing algorithms in AI Fairness 360 called as **equalized odds postprocessing**, which is implemented in the `EqOddsPostprocessing` class in the `aif360.algorithms.postprocessing` package. This technique solves a linear program to find probabilities with which to change output labels to optimize equalized odds.\n",
    "\n",
    "You can find documentation for reweighting here:\n",
    "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.EqOddsPostprocessing.html \n",
    "\n",
    "Call the fit and transform methods to perform the transformation, producing a newly transformed training dataset (```dataset_post_train```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test, dict_df_test = dataset_orig_test.convert_to_dataframe()\n",
    "df_train, dict_df_train = dataset_orig_train.convert_to_dataframe()\n",
    "\n",
    "# Fit the model to the training data and predict for test data\n",
    "# write code here\n",
    "# dataset_pred_test -- dataset with predictions stored in labels\n",
    "\n",
    "# create Equalized Odds Post processing object\n",
    "eo_post = EqOddsPostprocessing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "# fit the object to training data\n",
    "eo_post.fit(dataset_orig_test, dataset_pred_test)\n",
    "\n",
    "# make predictions on test data\n",
    "# write code here\n",
    "\n",
    "# construct metrics object\n",
    "# write code here\n",
    "\n",
    "# compute fairnesss metrics \n",
    "# write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9:** Interpret the difference in fairness metrics for the predicted outcomes under this post-processing technique. How do these compare to the metrics calculated in Q2, Q5 and Q8?\n",
    "\n",
    "**Write your interpretation and comparison in this text cell:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting this Assignment Notebook\n",
    "\n",
    "Once complete, please submit your assignment notebook as an attachment under \\\"Assignments > Assignment 4\\\" on Brightspace. You can download a copy of your notebook using ```File > Download .ipynb```. Please ensure you submit the `.ipynb` file (and not a `.py` file).\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab_2",
   "provenance": [
    {
     "file_id": "1hSONDCewgk6NlUWjqiT6Qf4I0GoUKTrW",
     "timestamp": 1612381728169
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
