{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wu8QNsWRb_xp"
   },
   "source": [
    "# Yoonhyuck WOO / Purdue University_Computer and Information Technology\n",
    "# Assignment 4: Detecting and Mitigating Bias \n",
    "# Professor: Dr. Pradhan\n",
    "\n",
    "## Date: 3.4 - 5:00pm 3/22/2024 (EST)\n",
    "\n",
    "#### references\n",
    "- class lectures\n",
    "\n",
    "\n",
    "The goal of this tutorial is to introduce the basic functionality of AI Fairness 360 for detecting and mitigating bias. As before, we will work with the German Credit dataset. There are many metrics one can use to detect the presence of bias. Likewise, there are many different bias mitigation algorithms one can employ. AI Fairness 360 provides some of them most common metrics and algorithms.\n",
    "\n",
    "\n",
    "### Bias mitigation techniques\n",
    "\n",
    "We learnt about the different bias mitigation techniques in class called _pre-processing_, _in-processing_, and _post-processing_.\n",
    "\n",
    "\n",
    "We will use AI Fairness 360 (`aif360`) to detect and mitigate bias. We will look for bias in the creation of a machine learning model that predicts whether an applicant should be given credit based on various features from a typical credit application. The protected attribute will be \"Age\", with \"1\" (older than or equal to 25) and \"0\" (younger than 25) being the values for the _privileged_ and _unprivileged_ groups, respectively.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Install and import packages and modules\n",
    "2. Load dataset, split between train and test, and compute fairness metrics on original training dataset\n",
    "3. Mitigate bias using a pre-processing algorithm (reweighing)\n",
    "4. Mitigate bias using an in-processing algorithm (adversarial debiasing)\n",
    "5. Mitigate bias using a post-processing algorithm (equalized odds post processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSvZzh_-ntSU"
   },
   "source": [
    "\n",
    "## 1. Import Statements\n",
    "\n",
    "First, we install the necessary packages. Then we import several components from the `aif360` package. We are relying on aif360 for this assignment, so please start early to make sure that the dependencies are resolved and that the pacakges load correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba==0.48"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\LG\\\\AppData\\\\Roaming\\\\Python\\\\Python38\\\\site-packages\\\\~lvmlite\\\\binding\\\\llvmlite.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached numba-0.48.0-1-cp38-cp38-win_amd64.whl (2.1 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lg\\anaconda3\\lib\\site-packages (from numba==0.48) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from numba==0.48) (1.24.4)\n",
      "Collecting llvmlite<0.32.0,>=0.31.0dev0\n",
      "  Using cached llvmlite-0.31.0-cp38-cp38-win_amd64.whl (13.6 MB)\n",
      "Installing collected packages: llvmlite, numba\n",
      "  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.41.1\n",
      "    Uninstalling llvmlite-0.41.1:\n",
      "      Successfully uninstalled llvmlite-0.41.1\n"
     ]
    }
   ],
   "source": [
    "!pip install numba==0.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-macos\n",
      "ERROR: No matching distribution found for tensorflow-macos\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-macos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.2\n"
     ]
    }
   ],
   "source": [
    "print(aif360.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\LG\\AppData\\Roaming\\Python\\Python38\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "umap-learn 0.5.5 requires numba>=0.51.2, but you have numba 0.48.0 which is incompatible.\n",
      "torchvision 0.10.1 requires torch==1.9.1, but you have torch 2.2.1 which is incompatible.\n",
      "torchtext 0.10.1 requires torch==1.9.1, but you have torch 2.2.1 which is incompatible.\n",
      "pynndescent 0.5.11 requires numba>=0.51.2, but you have numba 0.48.0 which is incompatible.\n",
      "infairness 0.2.3 requires numpy>=1.21.6, but you have numpy 1.20.3 which is incompatible.\n",
      "fairlearn 0.10.0 requires numpy>=1.24.4, but you have numpy 1.20.3 which is incompatible.\n",
      "allennlp 2.9.2 requires torch<1.12.0,>=1.6.0, but you have torch 2.2.1 which is incompatible.\n",
      "allennlp 2.9.2 requires transformers<4.18,>=4.1, but you have transformers 4.18.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Using cached numpy-1.20.3-cp38-cp38-win_amd64.whl (13.7 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.20.3 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.20.3\n"
     ]
    }
   ],
   "source": [
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aif360==0.2.2\n",
      "  Using cached aif360-0.2.2-py2.py3-none-any.whl (56.4 MB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lg\\anaconda3\\lib\\site-packages (from aif360==0.2.2) (1.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\lg\\anaconda3\\lib\\site-packages (from aif360==0.2.2) (1.10.1)\n",
      "Requirement already satisfied: pandas>=0.23.3 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from aif360==0.2.2) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\lg\\appdata\\roaming\\python\\python38\\site-packages (from aif360==0.2.2) (1.20.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from pandas>=0.23.3->aif360==0.2.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from pandas>=0.23.3->aif360==0.2.2) (2021.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from pandas>=0.23.3->aif360==0.2.2) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.23.3->aif360==0.2.2) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from scikit-learn->aif360==0.2.2) (2.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from scikit-learn->aif360==0.2.2) (1.3.2)\n",
      "Installing collected packages: aif360\n",
      "  Attempting uninstall: aif360\n",
      "    Found existing installation: aif360 0.6.0\n",
      "    Uninstalling aif360-0.6.0:\n",
      "      Successfully uninstalled aif360-0.6.0\n",
      "Successfully installed aif360-0.2.2\n"
     ]
    }
   ],
   "source": [
    "# No need to re-install if you already did so in Assignment 2\n",
    "!pip install aif360==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aif360==0.6.0\n",
      "  Using cached aif360-0.6.0-py3-none-any.whl (229 kB)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from aif360==0.6.0) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from aif360==0.6.0) (1.24.4)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from aif360==0.6.0) (2.0.3)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lg\\anaconda3\\lib\\site-packages (from aif360==0.6.0) (3.7.5)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from aif360==0.6.0) (1.10.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->aif360==0.6.0) (2024.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->aif360==0.6.0) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->aif360==0.6.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.24.0->aif360==0.6.0) (1.15.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0->aif360==0.6.0) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0->aif360==0.6.0) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from matplotlib->aif360==0.6.0) (23.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from matplotlib->aif360==0.6.0) (5.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from matplotlib->aif360==0.6.0) (2.4.7)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from matplotlib->aif360==0.6.0) (1.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from matplotlib->aif360==0.6.0) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from matplotlib->aif360==0.6.0) (4.50.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from matplotlib->aif360==0.6.0) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from matplotlib->aif360==0.6.0) (8.2.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->aif360==0.6.0) (3.4.1)\n",
      "Installing collected packages: aif360\n",
      "  Attempting uninstall: aif360\n",
      "    Found existing installation: aif360 0.2.2\n",
      "    Uninstalling aif360-0.2.2:\n",
      "      Successfully uninstalled aif360-0.2.2\n",
      "Successfully installed aif360-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install aif360==0.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my laptop, when I used the aif360=0.2.2 version, it didn't work, like 'gd2 = GermanDataset()'. So, I upgraded it to v.0.6.0, refreshed the kernel, and ran the code again, but under this version, it says error. So I ran again '!pip install aif360==0.2.2' and did not refresh the kernel, and the code worked well, including' gd2 = GermanDataset()'. I am still finding the reason, but I undertake this assignment by following the above process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3511,
     "status": "ok",
     "timestamp": 1643739518204,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "FJxDwiuVb_xt",
    "outputId": "d87328c1-9c0c-44e2-ee4b-a103556d2bf6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\anaconda3\\lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n",
      "C:\\Users\\LG\\anaconda3\\lib\\site-packages\\torch_geometric\\typing.py:101: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: [WinError 127] The specified procedure could not be found\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "# import all necessary packages\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "from aif360.datasets import GermanDataset, BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, DatasetMetric\n",
    "\n",
    "from aif360.algorithms.preprocessing import Reweighing, LFR, DisparateImpactRemover\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "\n",
    "from aif360.explainers import MetricTextExplainer, MetricJSONExplainer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import json\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQj5VqOSb_xt"
   },
   "source": [
    "## 2. Load Data, Specify Protected Attribute, and Split Data\n",
    "\n",
    "We will use the German Credit data, set the protected attribute to be age, create two variables to represent the privileged and unprivileged groups, and split the original dataset into training and test data subsets. Finally, we will build a typical machine learning workflow that involves training a machine learning model on the training dataset and use a test dataset to assess the model's efficacy (e.g., accuracy, fairness). For this dataset, we have a binary classification problem that predicts individuals as being a good or a bad credit risk.\n",
    "\n",
    "In this dataset, we consider older applicants (`age >= 25`) as the privileged group and younger applicants (`age < 25`) as the unprivileged group. \n",
    "\n",
    "We will use the preprocessed GermanDataset with one-hot encoded data provided by the aif360 package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we drop sex, which may also be a protected attribute\n",
    "dataset_orig = GermanDataset(protected_attribute_names=['age'],\n",
    "                             privileged_classes=[lambda x: x >= 25],\n",
    "                             features_to_drop=['personal_status', 'sex'])\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1643739518205,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "MsFiFKReb_xw",
    "outputId": "ab4ceeb1-3ede-4ad3-d21f-7aa2816a3a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape:  (1000, 57)\n",
      "Train dataset shape:  (700, 57)\n",
      "Test dataset shape:  (300, 57)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original data shape: \",dataset_orig.features.shape)\n",
    "print(\"Train dataset shape: \", dataset_orig_train.features.shape)\n",
    "print(\"Test dataset shape: \", dataset_orig_test.features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object ```dataset_orig``` is an aif360 dataset, which has some useful methods and attributes that you can explore. More documentation is available at https://aif360.readthedocs.io/en/latest/modules/datasets.html. \n",
    "For now, we'll just transform the data into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1643739518206,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "W3Aonmt3b_xw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (1000, 58)\n"
     ]
    }
   ],
   "source": [
    "df, dict_df = dataset_orig.convert_to_dataframe()\n",
    "print(\"Shape: \", df.shape)\n",
    "# print(df.columns)\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>investment_as_income_percentage</th>\n",
       "      <th>residence_since</th>\n",
       "      <th>age</th>\n",
       "      <th>number_of_credits</th>\n",
       "      <th>people_liable_for</th>\n",
       "      <th>status=A11</th>\n",
       "      <th>status=A12</th>\n",
       "      <th>status=A13</th>\n",
       "      <th>...</th>\n",
       "      <th>housing=A153</th>\n",
       "      <th>skill_level=A171</th>\n",
       "      <th>skill_level=A172</th>\n",
       "      <th>skill_level=A173</th>\n",
       "      <th>skill_level=A174</th>\n",
       "      <th>telephone=A191</th>\n",
       "      <th>telephone=A192</th>\n",
       "      <th>foreign_worker=A201</th>\n",
       "      <th>foreign_worker=A202</th>\n",
       "      <th>credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1169.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.0</td>\n",
       "      <td>5951.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2096.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.0</td>\n",
       "      <td>7882.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>4870.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  credit_amount  investment_as_income_percentage  residence_since  \\\n",
       "0    6.0         1169.0                              4.0              4.0   \n",
       "1   48.0         5951.0                              2.0              2.0   \n",
       "2   12.0         2096.0                              2.0              3.0   \n",
       "3   42.0         7882.0                              2.0              4.0   \n",
       "4   24.0         4870.0                              3.0              4.0   \n",
       "\n",
       "   age  number_of_credits  people_liable_for  status=A11  status=A12  \\\n",
       "0  1.0                2.0                1.0         1.0         0.0   \n",
       "1  0.0                1.0                1.0         0.0         1.0   \n",
       "2  1.0                1.0                2.0         0.0         0.0   \n",
       "3  1.0                1.0                2.0         1.0         0.0   \n",
       "4  1.0                2.0                2.0         1.0         0.0   \n",
       "\n",
       "   status=A13  ...  housing=A153  skill_level=A171  skill_level=A172  \\\n",
       "0         0.0  ...           0.0               0.0               0.0   \n",
       "1         0.0  ...           0.0               0.0               0.0   \n",
       "2         0.0  ...           0.0               0.0               1.0   \n",
       "3         0.0  ...           1.0               0.0               0.0   \n",
       "4         0.0  ...           1.0               0.0               0.0   \n",
       "\n",
       "   skill_level=A173  skill_level=A174  telephone=A191  telephone=A192  \\\n",
       "0               1.0               0.0             0.0             1.0   \n",
       "1               1.0               0.0             1.0             0.0   \n",
       "2               0.0               0.0             1.0             0.0   \n",
       "3               1.0               0.0             1.0             0.0   \n",
       "4               1.0               0.0             1.0             0.0   \n",
       "\n",
       "   foreign_worker=A201  foreign_worker=A202  credit  \n",
       "0                  1.0                  0.0     1.0  \n",
       "1                  1.0                  0.0     2.0  \n",
       "2                  1.0                  0.0     1.0  \n",
       "3                  1.0                  0.0     1.0  \n",
       "4                  1.0                  0.0     2.0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kITY3AtDb_xz"
   },
   "source": [
    "## 3. Compute Fairness Metrics on Original Training Data\n",
    "Now that we have identified the protected attribute \"age\" and defined privileged and unprivileged values, we can use aif360 to detect bias in the dataset.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZLVzqlmwOXI"
   },
   "source": [
    "### Mean Outcomes\n",
    "\n",
    "Compare the base rates (i.e., percentage of favorable results) for the privileged and unprivileged groups and report the difference (unprivileged base rate - privileged base rate). This is implemented in the ```mean_difference``` method on the BinaryLabelDatasetMetric class, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1643739519124,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "bVQWXC8Ub_x0",
    "outputId": "ae13c1db-2647-4764-ddf9-e2274a93dcbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training dataset\n",
      "Difference in mean outcomes between unprivileged and privileged groups = -0.169905\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(\n",
    "     dataset_orig_train, \n",
    "     unprivileged_groups=unprivileged_groups,\n",
    "     privileged_groups=privileged_groups\n",
    "  )\n",
    "print(\"Original training dataset\")\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZufOXh9b_x0"
   },
   "source": [
    "### Disparate Impact\n",
    "We can calculate the ratio of (predicted) favorable outcomes for the unprivileged group compared to the privileged group as implemented in the ```disparate_impact``` method on the BinaryLabelDatasetMetric class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1643739519126,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "1iw7g6Hjb_x0",
    "outputId": "d5efcc0a-5d2c-4120-94ab-2e8dbe658fc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training dataset\n",
      "Disparate Impact = 0.766430\n"
     ]
    }
   ],
   "source": [
    "print(\"Original training dataset\")\n",
    "print(\"Disparate Impact = %f\" % metric_orig_train.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The fairness metrics above will vary depending upon the train-test split. If the magnitude of mean difference is less than 10%, try another split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcTiGnSob_x1"
   },
   "source": [
    "### Built-In Explainers\n",
    "\n",
    "```aif360``` has some useful explainers for the fairness metrics which can be used to interpret the fairness metric values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643739519127,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "ERMXmj2pb_x1"
   },
   "outputs": [],
   "source": [
    "json_expl = MetricJSONExplainer(metric_orig_train)\n",
    "def format_json(json_str):\n",
    "    return json.dumps(json.loads(json_str, object_pairs_hook=OrderedDict),\n",
    "                      indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWuosH5pxVHe"
   },
   "source": [
    "Let's print the mean difference explainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1643739519128,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "7cc0pIUPxdFl",
    "outputId": "86acf8f0-f04d-4c19-ec4a-3ba6d0d6fd9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metric\": \"Mean Difference\",\n",
      "  \"message\": \"Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.1699054740619017\",\n",
      "  \"numPositivesUnprivileged\": 63.0,\n",
      "  \"numInstancesUnprivileged\": 113.0,\n",
      "  \"numPositivesPrivileged\": 427.0,\n",
      "  \"numInstancesPrivileged\": 587.0,\n",
      "  \"description\": \"Computed as the difference of the rate of favorable outcomes received by the unprivileged group to the privileged group.\",\n",
      "  \"ideal\": \"The ideal value of this metric is 0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(format_json(json_expl.mean_difference()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vl-r2vpjxPFo"
   },
   "source": [
    "We can also print the disparate impact explainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1643739519129,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "y-FoLzMTxSGS",
    "outputId": "8c66b2d1-0262-48c6-a17d-12c871f04c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metric\": \"Disparate Impact\",\n",
      "  \"message\": \"Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.7664297113013201\",\n",
      "  \"numPositivePredictionsUnprivileged\": 63.0,\n",
      "  \"numUnprivileged\": 113.0,\n",
      "  \"numPositivePredictionsPrivileged\": 427.0,\n",
      "  \"numPrivileged\": 587.0,\n",
      "  \"description\": \"Computed as the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group.\",\n",
      "  \"ideal\": \"The ideal value of this metric is 1.0 A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(format_json(json_expl.disparate_impact()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcDleSrDJr_B"
   },
   "source": [
    "**Q1:** Using the explainers above, interpret the difference in means and disparate impact in the German Credit data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HV2t23yJ19P"
   },
   "source": [
    "**Q1_Write your interpretation here**\n",
    "\n",
    "- statistical parity: If the values are less than 0, the privileged group has a higher proportion of predicted positive outcomes than the unprivileged group \n",
    "\n",
    "- Disparate Imapct: If the values are less than 1, the same is true when statistical parity values are less than 0, which means positive bias.\n",
    "\n",
    "Thus, both metrics have similar meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model on the training data\n",
    "\n",
    "Let's build a logistic regression model on this training data, predict credit risk for test data and compute the same fairness metrics over the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "\n",
    "df_test, dict_df_test = dataset_orig_test.convert_to_dataframe()\n",
    "df_train, dict_df_train = dataset_orig_train.convert_to_dataframe()\n",
    "\n",
    "# Fit the model to the training data\n",
    "x_train = df_train.drop(['credit'], axis=1)\n",
    "y_train = df_train['credit']\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "x_test = df_test.drop(['credit'], axis=1)\n",
    "y_test = df_test['credit']\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "dataset_pred_test = dataset_orig_test.copy()\n",
    "dataset_pred_test.labels = y_pred.copy()\n",
    "\n",
    "metric_dataset_test = BinaryLabelDatasetMetric(\n",
    "    dataset_pred_test, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here to compute fairness metrics\n",
    "json_expl_q2 = MetricJSONExplainer(metric_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metric\": \"Mean Difference\",\n",
      "  \"message\": \"Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.30303030303030304\",\n",
      "  \"numPositivesUnprivileged\": 12.0,\n",
      "  \"numInstancesUnprivileged\": 36.0,\n",
      "  \"numPositivesPrivileged\": 168.0,\n",
      "  \"numInstancesPrivileged\": 264.0,\n",
      "  \"description\": \"Computed as the difference of the rate of favorable outcomes received by the unprivileged group to the privileged group.\",\n",
      "  \"ideal\": \"The ideal value of this metric is 0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(format_json(json_expl_q2.mean_difference()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metric\": \"Disparate Impact\",\n",
      "  \"message\": \"Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.5238095238095238\",\n",
      "  \"numPositivePredictionsUnprivileged\": 12.0,\n",
      "  \"numUnprivileged\": 36.0,\n",
      "  \"numPositivePredictionsPrivileged\": 168.0,\n",
      "  \"numPrivileged\": 264.0,\n",
      "  \"description\": \"Computed as the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group.\",\n",
      "  \"ideal\": \"The ideal value of this metric is 1.0 A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(format_json(json_expl_q2.disparate_impact()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** Using the fairness metric functions as before, report the bias observed in the model's predictions over test data. What do these values indicate? Are the model's predictions more biased or less biased compared to the bias observed in the training data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2_Write your answer here**\n",
    "\n",
    "As before, both metrics show that the privileged group has a higher proportion of predicted positive outcomes than the unprivileged group, so there is a positive bias.\n",
    "\n",
    "|      | Original data | Model's Predictions over test data   |\n",
    "| :---        |    :----:   |          ---: |\n",
    "| **Mean difference**  | -0.1699       | -0.3030   |\n",
    "| **Disparate Impact**   | 0.7644       | 0.5238      |\n",
    "\n",
    "According to the above crosstab, both values of original data are closer to the ideal value, so the model predicts less bias in original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bias Mitigation Techniques\n",
    "\n",
    "We learnt in class that there are several bias mitigation techniques namely, pre-processing, in-processing, and post-processing algorithms.\n",
    "\n",
    "_Pre-processing_ bias mitigation is performed at the data end, before the creation of the model. In other words, we transform the data such that a model learned on the transformed data produces less biased decisions.\n",
    "\n",
    "_In-processing_ bias mitigation methods focus on the model training stage, as compared to pre-processing which focuses on transforming the data prior to model training. This suite of methods includes incorporating a fairness constraint during model training, tweaking the model's objective function, and adversarial learning.\n",
    "\n",
    "_Post-processing_ bias mitigation focus on the model predictions after the model has been trained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQIwiWcKb_x3"
   },
   "source": [
    "### 4.1 Bias Mitigation via Pre-Processing\n",
    "\n",
    "AI Fairness 360 implements several pre-processing mitigation algorithms. We will use the **reweighing algorithm**, which is implemented in the `Reweighing` class in the `aif360.algorithms.preprocessing` package. As discussed in class, this algorithm will transform the dataset by assigning weights to instances in each (group, label) combination to change the base rates and ensure fairness before classification. The idea is to apply appropriate weights to different tuples in the training data to reduce discrimination with respect to the protected attributes.\n",
    "\n",
    "You can find documentation for reweighting here:\n",
    "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.Reweighing.html \n",
    "\n",
    "Call the fit and transform methods to perform the transformation, producing a newly transformed training dataset (```dataset_transf_train```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1643739523669,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "CqBmaYXab_x3"
   },
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdkCX8Fd0zgN"
   },
   "source": [
    "We can print the weights. Each observation in the data should have a weight. For brevity, let's look at the weights for the first 10 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1643739523669,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "l5ISRhOwb_x3",
    "outputId": "032e2735-51a6-42c7-9e07-07d669cbcd5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96229508, 0.96229508, 0.96229508, 0.96229508, 0.96229508,\n",
       "       0.96229508, 0.96229508, 0.96229508, 1.25555556, 0.678     ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset_transf_train.instance_weights)\n",
    "dataset_transf_train.instance_weights[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k6wrI5Ab_x4"
   },
   "source": [
    "### Compute Fairness Metrics in Transformed Data\n",
    "\n",
    "We can check how effective the transformed data was in removing bias by calculating the metrics used for the original training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_rw_train = BinaryLabelDatasetMetric(\n",
    "    dataset_transf_train, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFZh_-La6Eq8"
   },
   "source": [
    "Print the difference in mean outcomes and disparate impact in the transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1643739523848,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "gim6DapUb_x4"
   },
   "outputs": [],
   "source": [
    "# write your code here\n",
    "# write code here to compute fairness metrics\n",
    "json_expl_q3 = MetricJSONExplainer(metric_rw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metric\": \"Disparate Impact\",\n",
      "  \"message\": \"Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 1.0000000000000004\",\n",
      "  \"numPositivePredictionsUnprivileged\": 79.10000000000002,\n",
      "  \"numUnprivileged\": 113.00000000000003,\n",
      "  \"numPositivePredictionsPrivileged\": 410.89999999999986,\n",
      "  \"numPrivileged\": 587.0,\n",
      "  \"description\": \"Computed as the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group.\",\n",
      "  \"ideal\": \"The ideal value of this metric is 1.0 A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(format_json(json_expl_q3.disparate_impact()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metric\": \"Mean Difference\",\n",
      "  \"message\": \"Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): 3.3306690738754696e-16\",\n",
      "  \"numPositivesUnprivileged\": 79.10000000000002,\n",
      "  \"numInstancesUnprivileged\": 113.00000000000003,\n",
      "  \"numPositivesPrivileged\": 410.89999999999986,\n",
      "  \"numInstancesPrivileged\": 587.0,\n",
      "  \"description\": \"Computed as the difference of the rate of favorable outcomes received by the unprivileged group to the privileged group.\",\n",
      "  \"ideal\": \"The ideal value of this metric is 0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(format_json(json_expl_q3.mean_difference()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1NdOcvCFmC8"
   },
   "source": [
    "**Q3:** How do these values compare to the difference in mean outcomes and disparate impact in the original data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aG_Glh7Frbc"
   },
   "source": [
    "**Q3_Write your answer in this text cell:**\n",
    "\n",
    "|      | Original Data | Reweighing Data   | Model's Predictions over test data   |\n",
    "| :---        |    :----:   |          ---: |         ---: |\n",
    "| **Mean difference**  | -0.1699       | 3.3306   | -0.3030   |\n",
    "| **Disparate Impact**   | 0.7664       | 1      | 0.5238      |\n",
    "\n",
    "It is fascinating. While the disparate impact of reweighing data is almost ideal, the mean difference is more biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Fairness Metrics on Model Trained on Transformed Data\n",
    "\n",
    "In the following, we will train a model on the transformed data and compute the metrics over predictions made on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:**  How do you expect the fairness metrics would be over a model trained on the transformed data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4_Write your answer in this text cell:**\n",
    "\n",
    "Even if the fairness metrics over reweighing data show differently, I expect the model's predictions over transformed data will show similarly as before because, according to Q2, fairness metrics on the model's predictions show farther from the ideal value than the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the instances now have weights, we will use a classifier that can incorporate instance weights. In this case, we will use a Naive Bayes classifier (more details here: https://scikit-learn.org/stable/modules/naive_bayes.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_rw, dict_df_train_rw = dataset_transf_train.convert_to_dataframe()\n",
    "\n",
    "# Fit the model to the transformed training data\n",
    "x_train_rw = df_train_rw.drop(['credit'], axis=1)\n",
    "y_train_rw = df_train_rw['credit']\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model__gnb = GaussianNB()\n",
    "model__gnb.fit(x_train_rw, y_train_rw)\n",
    "\n",
    "# Use the model to make predictions on the test data\n",
    "y_pred_rw = model__gnb.predict(x_test)\n",
    "\n",
    "dataset_pred_test_rw = dataset_orig_test.copy()\n",
    "dataset_pred_test_rw.labels = y_pred_rw.copy()\n",
    "\n",
    "# Construct the BinaryLabelDatasetMetric object over the test predictions\n",
    "metric_dataset_test_rw = BinaryLabelDatasetMetric(\n",
    "    dataset_pred_test_rw, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print fairness metrics computed over test predictions\n",
    "# write code here\n",
    "json_expl_q5 = MetricJSONExplainer(metric_dataset_test_rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metric\": \"Disparate Impact\",\n",
      "  \"message\": \"Disparate impact (probability of favorable outcome for unprivileged instances / probability of favorable outcome for privileged instances): 0.810077519379845\",\n",
      "  \"numPositivePredictionsUnprivileged\": 19.0,\n",
      "  \"numUnprivileged\": 36.0,\n",
      "  \"numPositivePredictionsPrivileged\": 172.0,\n",
      "  \"numPrivileged\": 264.0,\n",
      "  \"description\": \"Computed as the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group.\",\n",
      "  \"ideal\": \"The ideal value of this metric is 1.0 A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(format_json(json_expl_q5.disparate_impact()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metric\": \"Mean Difference\",\n",
      "  \"message\": \"Mean difference (mean label value on unprivileged instances - mean label value on privileged instances): -0.1237373737373737\",\n",
      "  \"numPositivesUnprivileged\": 19.0,\n",
      "  \"numInstancesUnprivileged\": 36.0,\n",
      "  \"numPositivesPrivileged\": 172.0,\n",
      "  \"numInstancesPrivileged\": 264.0,\n",
      "  \"description\": \"Computed as the difference of the rate of favorable outcomes received by the unprivileged group to the privileged group.\",\n",
      "  \"ideal\": \"The ideal value of this metric is 0.0\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(format_json(json_expl_q5.mean_difference()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Are your observations in line with what you expected in Q4 above? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5_Write your answer in this text cell:**\n",
    "\n",
    "|      | Original Data | Reweighing Data   | Model's Predictions over test data   |Model's Predictions over RW data   |\n",
    "| :---        |    :----:   |          ---: |         ---: |        ---: |\n",
    "| **Mean difference**  | -0.1699       | 3.3306   | -0.3030   | -0.1237   |\n",
    "| **Disparate Impact**   | 0.7664       | 1      | 0.5238      | 0.81    |\n",
    "\n",
    "While the disparate impact shows what I expected, the mean difference does not. Instead, the mean difference value shows the most ideal among the others. In addition, even if the model's prediction over reweighing data is lower than the fairness of the original reweighing data, when I compare each model's predictions over each data, the prediction over reweighing data shows better fairness than the other in both values.\n",
    "\n",
    "The observation does not align with what I expected from the data and the model because we manipulated the data and used another model, so these differences are enough to show different values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** Instead of reweighing, one could also apply techniques such as suppression, i.e. removing sensitive attributes. Write code below to train a model that does not use any information on the sensitive attribute, use this model to make predictions over the test data, and then compute the fairness metrics over the predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Reference: https://www.oreilly.com/library/view/practical-fairness/9781492075721/ch04.html#callout_fairness_pre_processing_CO1-1\n",
    "\n",
    "Suppression part function 'def build_logit_model_suppression' part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1643739518204,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "JYB3PkStb_xv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_map = {1.0: 'Good Credit', 0.0: 'Bad Credit'}\n",
    "protected_attribute_maps = [{1.0: 'Male', 0.0: 'Female'}]\n",
    "gd = GermanDataset(protected_attribute_names=['sex'],\n",
    "                   privileged_classes=[['male']], \n",
    "                   metadata={'label_map': label_map, 'protected_attribute_maps': protected_attribute_maps})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.000e+00 1.169e+03 4.000e+00 ... 1.000e+00 1.000e+00 0.000e+00]\n",
      " [4.800e+01 5.951e+03 2.000e+00 ... 0.000e+00 1.000e+00 0.000e+00]\n",
      " [1.200e+01 2.096e+03 2.000e+00 ... 0.000e+00 1.000e+00 0.000e+00]\n",
      " ...\n",
      " [1.200e+01 8.040e+02 4.000e+00 ... 0.000e+00 1.000e+00 0.000e+00]\n",
      " [4.500e+01 1.845e+03 4.000e+00 ... 1.000e+00 1.000e+00 0.000e+00]\n",
      " [4.500e+01 4.576e+03 3.000e+00 ... 0.000e+00 1.000e+00 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "aa=dataset_orig_sup.features[:,]\n",
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 1. 1.]\n",
      "[1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(aa[:,4][:4])\n",
    "print(aa[:,7][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.000e+00 1.169e+03 4.000e+00 ... 1.000e+00 1.000e+00 0.000e+00]\n",
      " [4.800e+01 5.951e+03 2.000e+00 ... 0.000e+00 1.000e+00 0.000e+00]\n",
      " [1.200e+01 2.096e+03 2.000e+00 ... 0.000e+00 1.000e+00 0.000e+00]\n",
      " ...\n",
      " [1.200e+01 8.040e+02 4.000e+00 ... 0.000e+00 1.000e+00 0.000e+00]\n",
      " [4.500e+01 1.845e+03 4.000e+00 ... 1.000e+00 1.000e+00 0.000e+00]\n",
      " [4.500e+01 4.576e+03 3.000e+00 ... 0.000e+00 1.000e+00 0.000e+00]]\n",
      "[2. 1. 1. 1.]\n",
      "[1. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "aa2 = np.delete(aa, 4&7, 1)\n",
    "print(aa2)\n",
    "print(aa2[:,4][:4])\n",
    "print(aa2[:,7][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 2. 2. 1. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 2. 1. 1. 2. 2. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 1. 2. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1.\n",
      " 2. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.\n",
      " 1. 2. 2. 1. 2. 1. 2. 2. 1. 1. 1. 1. 2. 2. 2. 1. 2. 1. 2. 1. 2. 1. 2. 2.\n",
      " 2. 1. 2. 2. 1. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 1. 2. 1. 1. 1. 1. 2. 2. 2. 1. 1.\n",
      " 2. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 1.\n",
      " 1. 2. 1. 2. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2. 2.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2.\n",
      " 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 1. 1. 2. 1. 1. 2. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1.\n",
      " 1. 2. 1. 1. 2. 1. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 2.\n",
      " 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 2. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1.\n",
      " 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 2. 1. 2. 2. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 2.\n",
      " 2. 2. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1.\n",
      " 2. 1. 1. 2. 1. 1. 1. 2. 1. 1. 2. 1. 2. 1. 2. 2. 1. 2. 1. 1. 2. 1. 1. 1.\n",
      " 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 1. 2. 1. 1. 2. 1. 1. 2. 2. 1. 1. 1. 1. 1.\n",
      " 1. 1. 2. 1. 2. 1. 1. 2. 1. 2. 1. 1. 2. 2. 1. 1. 1. 2. 2. 2. 2. 2. 2. 1.\n",
      " 1. 2. 2. 2. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 2. 1. 1. 2. 2. 1.\n",
      " 2. 1. 1. 2. 1. 1. 1. 2. 1. 2. 2. 1. 1. 1. 1. 2. 2. 1. 2. 1. 1. 2. 1. 2.\n",
      " 2. 2. 1. 2. 2. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1.\n",
      " 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 2. 2. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1.\n",
      " 2. 2. 2. 1. 2. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 2. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 1. 2. 1.\n",
      " 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 2. 2. 2. 1.\n",
      " 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 2. 1. 1. 2. 2. 2. 1.\n",
      " 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 2. 2. 2. 1. 1. 2. 2. 1. 2. 2. 1. 1. 1. 1.\n",
      " 2. 1. 2. 1. 1. 1. 2. 1. 1. 2. 2. 1. 1. 2. 1. 1. 1. 1. 2. 1. 1. 2. 2. 1.\n",
      " 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.\n",
      " 1. 1. 2. 2. 1. 2. 2. 2. 1. 1. 2. 1. 2. 2. 1. 2. 1. 1. 1. 2. 1. 1. 1. 2.\n",
      " 2. 1. 2. 1. 1. 1. 1. 1. 1. 1. 2. 1. 2. 2. 1. 2. 2. 2. 1. 1. 1. 1. 2. 1.\n",
      " 1. 1. 1. 2. 1. 1. 2. 1. 1. 1. 1. 1. 2. 2. 1. 1. 1. 1. 2. 2. 2. 2. 1. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1.]\n"
     ]
    }
   ],
   "source": [
    "bb=dataset_orig_sup.labels.ravel()\n",
    "print(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "cc=dataset_orig_sup.instance_weights.ravel()\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_sup = GermanDataset(protected_attribute_names=['sex', 'age'],\n",
    "                 privileged_classes=[['male'], lambda x: x > 25],\n",
    "    features_to_drop=['personal_status'])\n",
    "dataset_orig_train_sup, dataset_orig_test_sup = dataset_orig_sup.split([0.7], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LogisticRegression(solver='liblinear', class_weight='balanced')\n",
    "\n",
    "df_test_sup, dict_df_test_sup = dataset_orig_test_sup.convert_to_dataframe()\n",
    "df_train_sup, dict_df_train_sup = dataset_orig_train_sup.convert_to_dataframe()\n",
    "\n",
    "# Fit the model to the training data\n",
    "x_train_sup = df_train_sup.drop(['credit','sex','age'], axis=1)\n",
    "y_train_sup = df_train_sup['credit']\n",
    "model2.fit(x_train_sup, y_train_sup)\n",
    "\n",
    "x_test_sup = df_test_sup.drop(['credit','sex','age'], axis=1)\n",
    "y_test_sup = df_test['credit']\n",
    "\n",
    "y_pred_sup = model2.predict(x_test_sup)\n",
    "\n",
    "dataset_pred_test_sup = dataset_orig_test_sup.copy()\n",
    "dataset_pred_test_sup.labels = y_pred_sup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = [{'sex': 1, 'age': 1}, {'sex': 0}]\n",
    "p = [{'sex': 1, 'age': 0}]\n",
    "metric_dataset_test_sup = BinaryLabelDatasetMetric(\n",
    "    dataset_pred_test_sup,\n",
    "privileged_groups=p,\n",
    "unprivileged_groups=up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suppression\n",
      "Difference in mean outcomes between unprivileged and privileged groups = 0.074949\n",
      "Disparate Impact = 1.145214\n"
     ]
    }
   ],
   "source": [
    "print(\"Suppression\")\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_test_sup.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_dataset_test_sup.disparate_impact())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (1000, 59)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>credit_amount</th>\n",
       "      <th>investment_as_income_percentage</th>\n",
       "      <th>residence_since</th>\n",
       "      <th>age</th>\n",
       "      <th>number_of_credits</th>\n",
       "      <th>people_liable_for</th>\n",
       "      <th>sex</th>\n",
       "      <th>status=A11</th>\n",
       "      <th>status=A12</th>\n",
       "      <th>...</th>\n",
       "      <th>housing=A153</th>\n",
       "      <th>skill_level=A171</th>\n",
       "      <th>skill_level=A172</th>\n",
       "      <th>skill_level=A173</th>\n",
       "      <th>skill_level=A174</th>\n",
       "      <th>telephone=A191</th>\n",
       "      <th>telephone=A192</th>\n",
       "      <th>foreign_worker=A201</th>\n",
       "      <th>foreign_worker=A202</th>\n",
       "      <th>credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>1169.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.0</td>\n",
       "      <td>5951.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>2096.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42.0</td>\n",
       "      <td>7882.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.0</td>\n",
       "      <td>4870.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  credit_amount  investment_as_income_percentage  residence_since  \\\n",
       "0    6.0         1169.0                              4.0              4.0   \n",
       "1   48.0         5951.0                              2.0              2.0   \n",
       "2   12.0         2096.0                              2.0              3.0   \n",
       "3   42.0         7882.0                              2.0              4.0   \n",
       "4   24.0         4870.0                              3.0              4.0   \n",
       "\n",
       "   age  number_of_credits  people_liable_for  sex  status=A11  status=A12  \\\n",
       "0  1.0                2.0                1.0  1.0         1.0         0.0   \n",
       "1  0.0                1.0                1.0  0.0         0.0         1.0   \n",
       "2  1.0                1.0                2.0  1.0         0.0         0.0   \n",
       "3  1.0                1.0                2.0  1.0         1.0         0.0   \n",
       "4  1.0                2.0                2.0  1.0         1.0         0.0   \n",
       "\n",
       "   ...  housing=A153  skill_level=A171  skill_level=A172  skill_level=A173  \\\n",
       "0  ...           0.0               0.0               0.0               1.0   \n",
       "1  ...           0.0               0.0               0.0               1.0   \n",
       "2  ...           0.0               0.0               1.0               0.0   \n",
       "3  ...           1.0               0.0               0.0               1.0   \n",
       "4  ...           1.0               0.0               0.0               1.0   \n",
       "\n",
       "   skill_level=A174  telephone=A191  telephone=A192  foreign_worker=A201  \\\n",
       "0               0.0             0.0             1.0                  1.0   \n",
       "1               0.0             1.0             0.0                  1.0   \n",
       "2               0.0             1.0             0.0                  1.0   \n",
       "3               0.0             1.0             0.0                  1.0   \n",
       "4               0.0             1.0             0.0                  1.0   \n",
       "\n",
       "   foreign_worker=A202  credit  \n",
       "0                  0.0     1.0  \n",
       "1                  0.0     2.0  \n",
       "2                  0.0     1.0  \n",
       "3                  0.0     1.0  \n",
       "4                  0.0     2.0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sup, dict_df_sup = dataset_orig_sup.convert_to_dataframe()\n",
    "print(\"Shape: \", df_sup.shape)\n",
    "# print(df.columns)\n",
    "df_sup.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7:** Interpret your results. How does the preprocessing technique in Q5 compare to the suppression technique? \n",
    "\n",
    "|      | Original Data | Reweighing Data   | Model's Predictions over test data   |Model's Predictions over RW data   |Suppression   |\n",
    "| :---        |    :----:   |          ---: |         ---: |        ---: |        ---: |\n",
    "| **Mean difference**  | -0.1699       | 3.3306   | -0.3030   | -0.1237   | 0.074949   |\n",
    "| **Disparate Impact**   | 0.7664       | 1      | 0.5238      | 0.81    |1.1452    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer in this text cell:**\n",
    "Overall, the suppression shows better fairness than the Model's prediction over RW data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riLalr-WwDsG"
   },
   "source": [
    "### 4.2. Bias Mitigation via In-Processing\n",
    "\n",
    "In-processing methods focus on the model training stage, as compared to pre-processing which focuses on transforming the data prior to model training. Broadly speaking, contemporary in-processing methods are stronger than pre-processing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUZKKFAo1R1v"
   },
   "source": [
    "### Adversarial Debiasing\n",
    "\n",
    "In this part of the notebook, we will use an in-processing algorithm, called _Adversarial Debiasing_, that we briefly discussed in class. From the aif360 documentation (https://aif360.readthedocs.io/en/v0.2.3/modules/inprocessing.html):\n",
    "\n",
    "> Adversarial debiasing is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversaryâ€™s ability to determine the protected attribute from the predictions. This approach leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit.\n",
    "\n",
    "For intuition, you can think of adversarial debiasing as a model with two supervised learning tasks. The first task is to predict an outcome using the training data input. The second task, i.e. the adversary, is to predict a protected feature using these predictions and non-protected features in the training data input. The aim is to maximize the model's ability to carry out the first task (i.e. predict outcomes) while minimizing its ability to carry out the second task (i.e. predict protected features).\n",
    "\n",
    "We implement adversarial debiasing below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4551,
     "status": "ok",
     "timestamp": 1643739523668,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "M_rWLmHvwBFF",
    "outputId": "79b78b63-dff7-4c8a-a9a4-3ca9448f8d89",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LG\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\LG\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0; iter: 0; batch classifier loss: 71.666153; batch adversarial loss: 0.660049\n",
      "epoch 1; iter: 0; batch classifier loss: 50.631836; batch adversarial loss: 0.655333\n",
      "epoch 2; iter: 0; batch classifier loss: 43.741997; batch adversarial loss: 0.666746\n",
      "epoch 3; iter: 0; batch classifier loss: 52.799423; batch adversarial loss: 0.654768\n",
      "epoch 4; iter: 0; batch classifier loss: 39.849537; batch adversarial loss: 0.650470\n",
      "epoch 5; iter: 0; batch classifier loss: 60.623253; batch adversarial loss: 0.640732\n",
      "epoch 6; iter: 0; batch classifier loss: 40.378685; batch adversarial loss: 0.635663\n",
      "epoch 7; iter: 0; batch classifier loss: 56.493797; batch adversarial loss: 0.643048\n",
      "epoch 8; iter: 0; batch classifier loss: 56.042507; batch adversarial loss: 0.640165\n",
      "epoch 9; iter: 0; batch classifier loss: 48.317097; batch adversarial loss: 0.647052\n",
      "epoch 10; iter: 0; batch classifier loss: 54.358109; batch adversarial loss: 0.622438\n",
      "epoch 11; iter: 0; batch classifier loss: 61.225632; batch adversarial loss: 0.610394\n",
      "epoch 12; iter: 0; batch classifier loss: 60.859924; batch adversarial loss: 0.639261\n",
      "epoch 13; iter: 0; batch classifier loss: 38.076008; batch adversarial loss: 0.642897\n",
      "epoch 14; iter: 0; batch classifier loss: 53.698582; batch adversarial loss: 0.621165\n",
      "epoch 15; iter: 0; batch classifier loss: 50.671394; batch adversarial loss: 0.631271\n",
      "epoch 16; iter: 0; batch classifier loss: 30.610872; batch adversarial loss: 0.625734\n",
      "epoch 17; iter: 0; batch classifier loss: 40.552078; batch adversarial loss: 0.594846\n",
      "epoch 18; iter: 0; batch classifier loss: 48.892139; batch adversarial loss: 0.603099\n",
      "epoch 19; iter: 0; batch classifier loss: 35.735352; batch adversarial loss: 0.581148\n",
      "epoch 20; iter: 0; batch classifier loss: 40.534889; batch adversarial loss: 0.607976\n",
      "epoch 21; iter: 0; batch classifier loss: 30.250309; batch adversarial loss: 0.603879\n",
      "epoch 22; iter: 0; batch classifier loss: 27.540771; batch adversarial loss: 0.598538\n",
      "epoch 23; iter: 0; batch classifier loss: 25.764259; batch adversarial loss: 0.591175\n",
      "epoch 24; iter: 0; batch classifier loss: 21.476700; batch adversarial loss: 0.589810\n",
      "epoch 25; iter: 0; batch classifier loss: 29.765810; batch adversarial loss: 0.591520\n",
      "epoch 26; iter: 0; batch classifier loss: 39.698669; batch adversarial loss: 0.582718\n",
      "epoch 27; iter: 0; batch classifier loss: 26.134941; batch adversarial loss: 0.571435\n",
      "epoch 28; iter: 0; batch classifier loss: 26.337933; batch adversarial loss: 0.563923\n",
      "epoch 29; iter: 0; batch classifier loss: 32.902405; batch adversarial loss: 0.569845\n",
      "epoch 30; iter: 0; batch classifier loss: 21.798214; batch adversarial loss: 0.592783\n",
      "epoch 31; iter: 0; batch classifier loss: 30.809292; batch adversarial loss: 0.595219\n",
      "epoch 32; iter: 0; batch classifier loss: 19.859165; batch adversarial loss: 0.572196\n",
      "epoch 33; iter: 0; batch classifier loss: 9.036346; batch adversarial loss: 0.558460\n",
      "epoch 34; iter: 0; batch classifier loss: 29.243681; batch adversarial loss: 0.584549\n",
      "epoch 35; iter: 0; batch classifier loss: 21.417049; batch adversarial loss: 0.590921\n",
      "epoch 36; iter: 0; batch classifier loss: 20.659492; batch adversarial loss: 0.579028\n",
      "epoch 37; iter: 0; batch classifier loss: 23.849796; batch adversarial loss: 0.580055\n",
      "epoch 38; iter: 0; batch classifier loss: 16.608458; batch adversarial loss: 0.577598\n",
      "epoch 39; iter: 0; batch classifier loss: 30.916473; batch adversarial loss: 0.547313\n",
      "epoch 40; iter: 0; batch classifier loss: 15.294442; batch adversarial loss: 0.587794\n",
      "epoch 41; iter: 0; batch classifier loss: 15.515129; batch adversarial loss: 0.550671\n",
      "epoch 42; iter: 0; batch classifier loss: 14.443226; batch adversarial loss: 0.549175\n",
      "epoch 43; iter: 0; batch classifier loss: 13.593607; batch adversarial loss: 0.559396\n",
      "epoch 44; iter: 0; batch classifier loss: 14.306405; batch adversarial loss: 0.563229\n",
      "epoch 45; iter: 0; batch classifier loss: 9.310801; batch adversarial loss: 0.545849\n",
      "epoch 46; iter: 0; batch classifier loss: 12.140771; batch adversarial loss: 0.550980\n",
      "epoch 47; iter: 0; batch classifier loss: 16.915203; batch adversarial loss: 0.539670\n",
      "epoch 48; iter: 0; batch classifier loss: 13.083958; batch adversarial loss: 0.544079\n",
      "epoch 49; iter: 0; batch classifier loss: 14.172578; batch adversarial loss: 0.578915\n"
     ]
    }
   ],
   "source": [
    "# reset tensorflow graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# start tensorflow session\n",
    "sess = tf.compat.v1.Session()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# create AdversarialDebiasing model\n",
    "debiased_model = AdversarialDebiasing(\n",
    "    privileged_groups = privileged_groups,\n",
    "    unprivileged_groups = unprivileged_groups,\n",
    "    scope_name = 'debiased_classifier',\n",
    "    debias = True,\n",
    "    sess = sess)\n",
    "\n",
    "# fit the model to training data\n",
    "debiased_model.fit(dataset_orig_train)\n",
    "\n",
    "# make predictions on training and test data\n",
    "dataset_debiasing_train = debiased_model.predict(dataset_orig_train)\n",
    "dataset_debiasing_test = debiased_model.predict(dataset_orig_test)\n",
    "\n",
    "# metrics\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(\n",
    "    dataset_debiasing_test, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )\n",
    "\n",
    "# Close session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWJUD_1-1XOe"
   },
   "source": [
    "### Fairness Metrics under Adversarial Debiasing\n",
    "\n",
    "The adversarial debiasing algorithm has built-in methods for the difference in mean outcomes (called ```.mean_difference()```) and disparate impact (called ```.disparate_impact()```). Print these below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1643739523668,
     "user": {
      "displayName": "Aniruddha Chauhan",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16209082779696786371"
     },
     "user_tz": 300
    },
    "id": "ro-8nuS62NZx",
    "outputId": "d0cab299-c5af-472b-cbae-5c7e77cc1707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adversarial Debiasing\n",
      "Difference in mean outcomes between unprivileged and privileged groups = 0.000000\n",
      "Disparate Impact = 1.000000\n"
     ]
    }
   ],
   "source": [
    "# write your code here\n",
    "print(\"Adversarial Debiasing\")\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_dataset_debiasing_test.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIQcc23c7SNA"
   },
   "source": [
    "**Q8:** Interpret the difference in means and disparate impact for the predicted outcomes under adversarial debiasing. How do these compare to the metrics calculated in Q2 and Q5?\n",
    "\n",
    "|      | Model's Predictions over test data   |Model's Predictions over RW data   | Adversarial Debiasing|\n",
    "| :---        |        ---: |        ---: |        ---: |\n",
    "| **Mean difference**  | -0.3030   | -0.1237   | 0   |\n",
    "| **Disparate Impact**   | 0.5238      | 0.81    | 1   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3MyIpCk7hnp"
   },
   "source": [
    "**Write your interpretation and comparison in this text cell:**\n",
    "\n",
    "Under adversarial debiasing, the difference in means and disparate impact for the predicted outcomes show an ideal value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Bias Mitigation via Post-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last section, we will use one of the post-processing algorithms in AI Fairness 360 called as **equalized odds postprocessing**, which is implemented in the `EqOddsPostprocessing` class in the `aif360.algorithms.postprocessing` package. This technique solves a linear program to find probabilities with which to change output labels to optimize equalized odds.\n",
    "\n",
    "You can find documentation for reweighting here:\n",
    "https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.postprocessing.EqOddsPostprocessing.html \n",
    "\n",
    "Call the fit and transform methods to perform the transformation, producing a newly transformed training dataset (```dataset_post_train```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equalized odds postprocessing\n",
      "Difference in mean outcomes between unprivileged and privileged groups = 0.345960\n",
      "Disparate Impact = 1.992754\n"
     ]
    }
   ],
   "source": [
    "df_test, dict_df_test = dataset_orig_test.convert_to_dataframe()\n",
    "df_train, dict_df_train = dataset_orig_train.convert_to_dataframe()\n",
    "\n",
    "# Fit the model to the training data and predict for test data\n",
    "# write code here\n",
    "# dataset_pred_test -- dataset with predictions stored in labels\n",
    "\n",
    "# create Equalized Odds Post processing object\n",
    "eo_post = EqOddsPostprocessing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "\n",
    "# fit the object to training data\n",
    "eo_post.fit(dataset_orig_test, dataset_pred_test)\n",
    "\n",
    "# make predictions on test data\n",
    "# write code here\n",
    "dataset_eop_train = eo_post.predict(dataset_orig_train)\n",
    "dataset_eop_test = eo_post.predict(dataset_orig_test)\n",
    "\n",
    "# construct metrics object\n",
    "# write code here\n",
    "metric_dataset_eop_test = BinaryLabelDatasetMetric(\n",
    "    dataset_eop_test, \n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    "  )\n",
    "\n",
    "# compute fairnesss metrics \n",
    "# write code here\n",
    "print(\"Equalized odds postprocessing\")\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_eop_test.mean_difference())\n",
    "print(\"Disparate Impact = %f\" % metric_dataset_eop_test.disparate_impact())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9:** Interpret the difference in fairness metrics for the predicted outcomes under this post-processing technique. How do these compare to the metrics calculated in Q2, Q5 and Q8?\n",
    "\n",
    "|      | Model's Predictions over test data   |Model's Predictions over RW data   | Adversarial Debiasing| Equalized Odds Postprocessing\n",
    "| :---        |        ---: |        ---: |        ---: |        ---: |\n",
    "| **Mean difference**  | -0.3030   | -0.1237   | 0   | 0.3459   |\n",
    "| **Disparate Impact**   | 0.5238      | 0.81    | 1   | 1.9927   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your interpretation and comparison in this text cell:**\n",
    "While both values in EOP are skewed toward positive values, they are not close to each ideal value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting this Assignment Notebook\n",
    "\n",
    "Once complete, please submit your assignment notebook as an attachment under \\\"Assignments > Assignment 4\\\" on Brightspace. You can download a copy of your notebook using ```File > Download .ipynb```. Please ensure you submit the `.ipynb` file (and not a `.py` file).\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab_2",
   "provenance": [
    {
     "file_id": "1hSONDCewgk6NlUWjqiT6Qf4I0GoUKTrW",
     "timestamp": 1612381728169
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
