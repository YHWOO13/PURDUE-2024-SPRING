{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "355LmyCRd_sZ"
   },
   "source": [
    "# Assignment 2: Exploring fairness during model training\n",
    "\n",
    "We discussed a number of fairness metrics in the context of binary classification. Building up on the previous assignment, we will explore aspects of fairness in model predictions while training our model.\n",
    "\n",
    "In this lab, we will detect bias that may be introduced while training classifiers.\n",
    "\n",
    "This notebook has four stages in which we will: \n",
    "1. Import the data\n",
    "2. Implement a few pre-processing steps, and inspect the data (compute base rates on original data)\n",
    "3. Train a classifier to predict credit using the original data, and measure bias using fairness metrics including statistical parity and equalized odds. \n",
    "4. Train a classifier to predict credit using the original data without sensitive features, and measure bias using fairness metrics including statistical parity and equalized odds. \n",
    "5. (Extra credit) Use IBM's AIF360 toolkit to compute fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(6)\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJDFA9XSwdMi"
   },
   "source": [
    "### Step 1: Load data\n",
    "\n",
    "For this assignment, we will work with the German Credit dataset which has been provided with this notebook (the dataset can also be downloaded from the UCI ML repository). The dataset has demographic and financial information for about 1,000 individuals, and the task for this dataset is to predict whether an individual is a good credit risk or a bad credit risk. More information about the dataset can be found here: https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data.\n",
    "\n",
    "Load the dataset and check the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['status', 'duration', 'credit_hist', 'purpose', 'credit_amt', 'savings', 'employment',\\\n",
    "            'install_rate', 'personal_status', 'debtors', 'residence', 'property', 'age', 'install_plans',\\\n",
    "            'housing', 'num_credits', 'job', 'num_liable', 'telephone', 'foreign_worker', 'credit']\n",
    "data_df = pd.read_table('german.data', names=cols, sep=\" \", index_col=False)\n",
    "y = data_df['credit']\n",
    "\n",
    "print(\"Shape: \", data_df.shape)\n",
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data preprocessing and data analysis\n",
    "\n",
    "#### 2.1. Adding two new columns\n",
    "Before training a model, repeat the data processing steps from assignment 1 here to create the \"sex\" and \"age\" columns.\n",
    "<ol>\n",
    "    <li> Write code to create the \"sex\" column and append to the dataframe. Consider sex=`male' as privileged. </li> \n",
    "    <li> Write code to replace the \"age\" column with discrete values. Consider age â‰¥ 25 as privileged. </li>\n",
    "</ol>\n",
    "\n",
    "As in assignment 1, we will train a machine learning model to predict good/bad credit. You are free to use any model but you may use the data preprocessing steps from assignment 1 to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code for data preparation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Sensitive attributes\n",
    "\n",
    "In the class, we learnt about sensitive attributes for computing model fairness. In this assignment, we will compute different fairness metrics in two scenarios:\n",
    "\n",
    "<ol>\n",
    "    <li> considering only \"age\" as the sensitive attribute </li>\n",
    "    <li> considering only \"sex\" as the sensitive attribute </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Compute base rates on original data\n",
    "Before we consider model fairness, let's first compare the base rates for the privileged and unprivileged groups with that of the entire data. Base rate indicates the fraction of data (or a subset of the data) that has positive outcomes. Base rates indicate class imbalance in the dataset.\n",
    "\n",
    "In the following code block, compute the base rates for the entire dataset, and for the privileged and unprivileged groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to compute base rates with respect to \"age\" as the sensitive attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Write in words if the value computed above indicates favoring the privileged group or the unprivileged group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to compute base rates with respect to \"sex\" as the sensitive attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Write in words if the value computed above indicates favoring the privileged group or the unprivileged group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Model training\n",
    "Let's now prepare the setup for model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Identify the training features and target variable\n",
    "\n",
    "We will evaluate the learned model on the training data. Therefore, we will not split the data into train and test sets. We are also not concerned with optimizing the learned model for performance and, therefore, we will not need to create a validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the data for training\n",
    "x_train = data_df.drop(\"credit\", axis=1)\n",
    "y_train = data_df.credit.replace({2:0}) #1 = Good, 2= Bad credit risk\n",
    "print(\"Training Outcomes: \\n\", y_train.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Model training\n",
    "\n",
    "In this part, we will set up our machine learning model and fit the model using the training data. Below, we have learnt a logistic regression model, but **you are free to choose any model**. Since we are not concerned with optimal model performance here, there is no need to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=0.5, penalty=\"l1\", solver='liblinear')\n",
    "    \n",
    "# Fit the model using the training data\n",
    "model = model.fit(x_train, y_train, sample_weight=None)\n",
    "\n",
    "# Observe the class labels\n",
    "print(model.classes_)\n",
    "\n",
    "# Calculate predicted values on training data\n",
    "y_pred = model.predict(x_train)\n",
    "\n",
    "# You can also compute the probabilties for the two clases using the predict_proba function\n",
    "y_pred_probs = model.predict_proba(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Model evaluation\n",
    "\n",
    "Compute the model accuracy using sklearn's `accuracy_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code to compute model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Evaluate model fairness \n",
    "In the class, we went over a number of fairness metrics. In this part of the assignment, you will write functions to compute statistical parity and equalized odds. Recall that both of these metrics use the model predictions $\\hat{Y}$ (computed as $y\\_pred$ above).\n",
    "\n",
    "Let $S=0$ indicate the unprivileged group and $S=1$ indicate the privileged group. FPR and FNR represent the false positive rate and false negative rate respectively. Then, we can compute:\n",
    "\n",
    "**Statistical parity** = $P(\\hat{Y} = 1 | S = 0) - P(\\hat{Y} = 1 | S = 1)$ \n",
    "\n",
    "**Equalized odds** $\\dfrac{(FPR_{S=0} - FPR_{S=1}) + (FNR_{S=0} - FNR_{S=1})}{2}$\n",
    "\n",
    "where $FPR = P(\\hat{Y}=1 | Y=0)$ and $FNR = P(\\hat{Y}=0 | Y=1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write function to compute statistical parity over model predictions with given sensitive attribute\n",
    "\n",
    "# Write function to compute equalized odds with given sensitive attribute\n",
    "\n",
    "# Report statistical parity and equalized odds using \"sex\" as the sensitive attribute\n",
    "\n",
    "# Report statistical parity and equalized odds using \"age\" as the sensitive attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** What can you say about the model being more or less biased than the original credit scores?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Model training without using the sensitive feature\n",
    "\n",
    "We discussed in the class how removing a sensitive attribute is not enough to generate models that result in fair predictions. In this step, we will see if that is indeed true in action.\n",
    "\n",
    "4.1. In the following, we will build models trained without the sensitive attribute (once for the \"sex\" attribute and one for the \"age\" attribute). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the sensitive attribute from training data\n",
    "x_train_no_age = x_train.drop('age', axis=1)\n",
    "x_train_no_sex = x_train.drop('sex', axis=1)\n",
    "\n",
    "model = LogisticRegression(C=0.5, penalty=\"l1\", solver='liblinear')\n",
    "    \n",
    "# Fit the model using the training data\n",
    "model_no_age = model.fit(x_train_no_age, y_train, sample_weight=None)\n",
    "model_no_sex = model.fit(x_train_no_sex, y_train, sample_weight=None)\n",
    "\n",
    "# Observe the class labels\n",
    "print(model.classes_)\n",
    "\n",
    "# Calculate the predicted values and probabilities on the updated training data\n",
    "y_pred_no_age = \n",
    "y_pred_no_sex = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Evaluating updated model\n",
    "\n",
    "Now, let's evaluate the predictions obtained from the updated model using the functions that your wrote in Step 3.4. Although the sensitive attribute has not been used in training the model, the group memberships will be used to compute the fairness metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report statistical parity and equalized odds using \"sex\" as the sensitive attribute\n",
    "\n",
    "# Report statistical parity and equalized odds using \"age\" as the sensitive attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Compare the metrics obtained here with those obtained in Step 3.4. What can you say about whether or not using the sensitive attribute have an effect on bias in model predictions for this example?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (**Extra Credit**) Step 5: Using AIF360 to compute fairness metrics\n",
    "\n",
    "In this step, we will use the \"aif360\" package to detect bias in the dataset (AIF360 is IBM's AI Fairness toolkit) to evaluate fairness of the learned model. This package requires the data to be in a certain format, which will be clear as we walk through the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should need to run this package installation only once. After the first time, you can comment it out\n",
    "# !pip install aif360==0.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.datasets import GermanDataset, StandardDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.postprocessing import EqOddsPostprocessing\n",
    "\n",
    "from aif360.explainers import MetricTextExplainer, MetricJSONExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let us use \"age\" as the sensitive attribute and defined privileged and unprivileged values. We use the GermanDataset in aif360 to do our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store definitions of privileged and unprivileged groups\n",
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]\n",
    "\n",
    "dataset_orig = GermanDataset(protected_attribute_names=['age'],           \n",
    "                             privileged_classes=[lambda x: x >= 25], \n",
    "                             features_to_drop=['personal_status', 'sex'])      # age >=25 is considered privileged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We will convert the dataset to a Pandas dataframe, extract the input features (X) and target variable (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_df = dataset_orig.convert_to_dataframe()[0]\n",
    "x_train = dataset_orig_df.drop(\"credit\", axis=1)\n",
    "y_train = dataset_orig_df.credit.replace({2:0}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use our model to calculate predicted values for the data and attach them as a new column in the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the dataset\n",
    "preds_df = dataset_orig_df.copy()\n",
    "\n",
    "model = LogisticRegression(C=0.5, penalty=\"l1\", solver='liblinear')\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Calculate predicted values\n",
    "preds_df['credit'] = model.predict(x_train)\n",
    "\n",
    "# Recode the predictions so that they match the format that the dataset was originally provided in \n",
    "# (1 = good credit, 2 = bad credit)\n",
    "preds_df['credit'] = preds_df.credit.replace({0:2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will create an object of the aif360 StandardDataset class. You can read more about this in the documentation:\n",
    "https://aif360.readthedocs.io/en/latest/modules/standard_datasets.html#aif360.datasets.StandardDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_aif360 = StandardDataset(dataset_orig_df, label_name='credit', protected_attribute_names=['age'], \n",
    "                privileged_classes=[[1]], favorable_classes=[1])\n",
    "\n",
    "preds_aif360 = StandardDataset(preds_df, label_name='credit', protected_attribute_names=['age'], \n",
    "                privileged_classes=[[1]], favorable_classes=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate some fairness metrics on the training data using the `ClassificationMetric` function in aif360. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_vs_preds_metrics = ClassificationMetric(orig_aif360, preds_aif360,\n",
    "                                                   unprivileged_groups=unprivileged_groups,\n",
    "                                                   privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, we can compute statistical parity difference as below:\n",
    "print(\"Statistical parity difference: \", orig_vs_preds_metrics.statistical_parity_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Compare the value obtained above with those obtained in steps 4.2 and 3.4 above. You might observe a difference in the performance due to the way the dataset has been created within aif360."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to explore some of the other fairness metrics that we discussed in class. You can find a list of fairness metrics currently supported by aif360 here: https://aif360.readthedocs.io/en/latest/modules/generated/aif360.metrics.ClassificationMetric.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06n4G0ShMTWY"
   },
   "source": [
    "# Submitting this Assignment Notebook\n",
    "\n",
    "Once complete, please submit your assignment notebook as an attachment under \"Assignments > Assignment 2\" on Brightspace. You can download a copy of your notebook using ```File > Download .ipynb```. Please ensure you submit the `.ipynb` file (and not a `.py` file)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "355LmyCRd_sZ",
    "JI0slRqXFyXZ",
    "06n4G0ShMTWY"
   ],
   "name": "lab_1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
